\def\newstep#1{\smallskip \noindent {\bf #1}}
\def\newitem#1{\vskip 0.05in \noindent [p #1]}
\def\subsec#1{\vskip 0.08in \noindent {\bf #1}}
\def\sec#1 {\vfil\break \centerline{\tt #1} \vskip 0.2in}
\def\CC{\hbox{C++}}

\topglue 0.5in
\centerline{Notes on Knuth Chapter 7.1: Zeroes and Ones}
\vskip 0.3in
\centerline{\tt Section 7.1.1: Boolean Basics}
\vskip 0.2in

\noindent
{\bf Basic Identities}

\noindent [p 50] Eq (9) states $\left( x \oplus y \right)
\oplus x = y$.  Why?  \hfil\break
Well, $\oplus$ is not distributive, even with itself,
but it is commutative and associative. The above is therefore
equivalent to $y \oplus \left( x \oplus x \right) = y \oplus 0 = y$.
The same argument shows that $\left( x \oplus y \right) \oplus y = x$.

\newitem{51} Why are Eq (13)-(15) true? \hfil\break
I don't see how to derive (13), or (14) besides just writing out the truth table, but (15) is:
$x \oplus y = \left(x \vee y\right) \wedge \overline x \wedge y$, which
is true by inspection (either x or y has to be true, but not both).
These can be expanded using (12) and (1) to give
$x \oplus y = \left(x \vee y\right) \wedge \left(\overline{x} \vee \overline{y}
\right) = \left(x \wedge \left(\overline{x} \vee \overline{y}\right)\right)
\vee \left(y \wedge \left(\overline{x} \vee \overline{y}\right)\right)$.
Applying (1) to each of these terms again gives things like
$\left(\overline{x} \wedge x\right) \vee \left(\overline{y} \vee x\right) =
\overline{y} \vee x$, which, when applied to both terms gives
$x \oplus y = \left(x \wedge \overline{y}\right) \vee \left(\overline{x}
\wedge y\right)$, which is the claimed identity.

\subsec{Functions of n variables}

\noindent [p51] Why does the functional decomposition
of eq (16) and (17) work?\hfil\break
First, note that $h$ is 0 if
$f\left(x_1,\ldots,x_{n-1},0\right) = f\left( x_1,\ldots,x_{n-1},1\right)$, so
in this case we have,
 $g\left(x_1,\ldots,x_{n-1}\right) \oplus 0 = f\left(x_1,\ldots,x_{n-1},0\right)$.
 Since we already said that $f\left(x_1,\ldots,x_{n-1},0\right) = f\left( x_1,\ldots,x_{n-1},1\right)$, this must be true.  
 
If $f\left(x_1,\ldots,x_{n-1},0\right) \neq f\left( x_1,\ldots,x_{n-1},1\right)$,
then $h\left(x_1,\ldots,x_{n-1}\right)=1$, so we have $1\, \wedge\, x_n
= x_n$.  Thus, the rhs of (16) becomes $f\left(x_1,\ldots,x_{n-1},0\right) 
\, \oplus \, x_n$.  Now note that if $x_n = 1$ this is
$f\left(x_1,\ldots,x_{n-1},0\right) \, \oplus 1 = 
\overline{f\left(x_1,\ldots,x_{n-1},0\right)}$.  But we already said that
$f\left(x_1,\ldots,x_{n-1},0\right) \neq f\left( x_1,\ldots,x_{n-1},1\right)$,
so this is just $f\left(x_1,\ldots,x_{n-1},1\right)$, which is what we want.
Similarly, for $x_n = 0$, we just get
$f\left(x_1,\ldots,x_{n-1},0\right) \oplus 0 = f\left(x_1,\ldots,x_{n-1},0\right)$.

\newitem{52} That demonstrates why (16) works -- why does (18), 
the ``law of development'', work?\hfil\break
First, let's say that $x_n = 0$.  Only the first term applies, giving
$f\left(x_1,\ldots,x_{n-1},0\right) \wedge 1 = f\left(x_1,\ldots,x_{n-1},0\right)$,
which is indeed the value of $f\left( x_1,\ldots,x_n \right)$ when $x_n = 0$.
It works the same way for $x_n = 1$.

\newitem{55} {\bf Theorem Q}\hfil\break 
A prime implicant corresponds to a 1 in the
truth table of the function.  Since the function is monotone, the implicant
can't become false (0) when any of it's values go from 0 to 1.  If one
of the terms in the prime implicant were, say, $\overline{x_i}$, then
when that changed from 0 to 1, the implicant would become false.  This
is not allowed.

\subsec{Satisfiability}

\noindent [p56] 3SAT\hfil\break
The text uses problem 39 to explore a fairly general form where the 
operators between variables
in the function can be any of the 16 boolean 2-ary functions, but it's
interesting enough to explore the more restrictive reduction from SAT --
which is how to turn a general problem in conjunctive normal form
into one with only three literals per clause.  After all, we know that
any boolean function can be written that way, so while it may be
possible to make a more efficient 3SAT instance using problem 39,
just studying how to do this with CNF form is enough to imply that
all boolean functions can be turned into 3SAT.

Clearly it's enough to figure out how to turn any clause $t_1 \vee \ldots
\vee t_n$ into a conjunction of 3-literal clauses.  To do this, one introduces
``nuisance'' variables $x_i$ for $2 \leq i \leq n - 2$ and uses them to sandwich
each literal.
$$
t_1 \vee \ldots \vee t_n = \left(t_1 \vee t_2 \vee x_2\right)
\land \left(\bar x_2 \vee t_3 \vee x_3\right)
\land \left(\bar x_3 \vee t_4 \vee x_4\right)
\land \ldots \land \left(\bar x_{n-2} \vee t_{n-1} \vee t_n\right) .
$$
Actually, they aren't {\it identical}, they are {\it equisatisfiable}.

So, why does this work?  Well, imagine this clause is satisfiable.
Then at least one of the $t_i$ must be 1. 
The point is that we can force all of the other 3-literal clauses
to be true by a judicious choice of the $x_i$.   Say that the
literal is $t_j$.  We then set $x_k$ for $k \le j$ to 1,
and $x_k$ for $k \geq j$ to 0 and the clause is satisfiable.

Conversely, say that the clause is not satisfiable, so all the $t$
are zero.  Then to satisfy the new form we need $x_2 = 1$ for
the first clause, but then that forces $x_3 = 1$ in the second,
etc., until we get to the last clause where we need $x_{n-2} = 0$
but it has been forced to be 1 by the previous clause.  So therefore
if the original clause is satisfiable we can satisfy the new one, and
if it isn't then we can't.

\subsec{Simple Special Cases}

\noindent [p57] {\bf Theorem H} \hfil\break
The first step in the formula is to use the 
distributive law (2): $\left(x \wedge y\right) \vee z = \left(x \vee z\right) 
\wedge \left(y \vee z\right)$.
The second step makes use of the fact that $\vee$ is commutative to move all the $y$s 
rightward in the left term.  We then have $\left(x_1 \vee \overline{x}_2
\vee \ldots \vee \overline{x}_k\right) \vee \overline{y}_1 \vee \ldots \vee 
\overline{y}_k
\geq \left(x_1 \vee \overline{x}_2 \vee \ldots \vee \overline{x}_k\right)$,
which is clearly true since the extra terms can either leave the truth value 
unchanged or increase it from 0 to 1.  The same logic applies to the second term.

\newitem{58} In the example of Horn clauses following theorem H, where 
do the propositions come from? \hfil\break  
Well, take the first one: $\bf xE \Rightarrow xT$ states
that it is allowable for an expression to start with {\bf x} if and only if
it is allowable for a term to start with {\bf x}; this is clear from the first
line of the specification.  

How do these convert into Horn clauses?  Well, recall that $x \Rightarrow y$
is the same as $\overline{x} \wedge y$ (Table 1).  The Horn clause expression is
just all of those $\wedge$ed together.

\subsec{Medians}

\newitem{63} Self duality: in the discussion after (46), why does a formula
need to be indifferent to swapping $\wedge$ and $\vee$?  Recall De Morgan's
laws (11 and 12): $\bar{x \wedge y} = \bar x \vee \bar y$ and $\bar{x \vee y} =
\bar x \wedge \bar y$.  So, using (46), when we negate the entire function,
we get something like $\bar{\left(x_i \wedge \ldots \wedge x_j\right) \vee
\left(x_k \wedge \ldots \wedge x_l\right) \vee \ldots} =
\left(\bar x_i \vee \ldots \vee \bar x_j\right) \wedge
\left(\bar x_k \vee \ldots \vee \bar x_l\right) \wedge \ldots$;
if this is to be equal to the original with the variables negated, then
indeed we must be able to interchange $\vee$ and $\wedge$.

\subsec{Threshold functions}

\noindent [p75] Negative weights: to get rid of negative weights,
the replacements $x_j \gets \bar x_j$ and $w_j \gets -w_j$ seem
pretty obvious, but what about the $t \gets t + \left|w_j\right|$?
Well, consider some variable $x_j$.  the old contribution was
$w_j x_j$, the new one is $-w_j \bar x_j$.  The first evaluates to
$0, w_j$ for $x_j = 0, 1$, the second to $-w_j, 0$.  These aren't
equal, so the right hand side ($t$) also needs to be adjusted.
Let's say the old threshold was 2, and $w_j = -2$.  Then with
the negative weight the rest of the terms need to add up to 2 or 4
for $x_j = 0, 1$.  For the new form, if we didn't adjust $t$, they
would have to add up to $0, 2$, so the formula are no longer
equivalent.  If we add $\left|-w_j\right| = 2$ so that $t \gets 4$,
then they need to be $2, 4$, as before.

\newitem{77} {\bf Theorem T} \hfil\break
The proof works because the number
of vectors where we must have $f\left(x^{\left(m\right)}\right) = 
g\left(x^{\left(m\right)}\right) = 1$ is $N\left(f\right) - k = N\left(g\right) - k$.
That means there must also be $k$ vectors where $f\left(y^{\left(k\right)}\right) = 1$
and $g\left(y^{\left(k\right)}\right) = 0$.

In the next step, note that $w \cdot x^{\left(j\right)} \geq t$ for each of the $k$
such values. 

\sec{Section 7.1.2: Boolean Evaluation}

\noindent [p 97] Different boolean chains with the same diagram: the point
of the discussion in the paragraph following the one with (4) is that the same
steps are just being done in a different order.  As long as they don't depend
on each other, that's fine -- which is equivalent to a different topological sorting.

\newitem{98} Note the convention for the truth tables here:
the columns are just $\left(x_1 x_2 x_3 x_4\right)_2$ counted up from 0.

\newitem{99} The difference between $L\left(f\right)$
and $C\left(f\right)$ is that $C$ is allowed to re-use intermediate results,
$L$ is not.  So, the reuse of $x_6$ in (7) is fine for $C$, but not for $L$.

\vskip 0.1in \noindent {\bf Optimum chains for n=4}

\noindent [p 99] Each boolean function has the same depth, length,
and cost as it's negation because the latter computed by using the same
chain except for the last step, where the operator $\circ$ is replaced
by the 2-ary binary function with the negated truth table.  And it can't
be shallower/shorter/cheaper because if we had such a chain for the
negation we could negate it in the same fashion to get a shallower/shorter/cheaper
chain for the original function.

\newitem{100} {\it Normalized chains:} The idea behind (10),
of course, is that it makes a non-normal function normal.  However, the claim
that a Boolean chain is normal if and only if each of its binary operators is
normal seems to be wrong -- at least if a chain being normal means what
I think it does.  It's not actually defined, but I'm assuming it means that the
function it evaluates is normal.  

Now, the if direction is obvious -- any chain composed of normal operators
will be normal.  And from the discussion at the top of the page, you can convert
any normal chain that doesn't use normal operators into one that does.
But I can construct normal functions using non-normal operators.
For example, take the simple 3-ary function with truth table
$0\,1\,1\,1\,1\,1\,1\,1$.  This can be computed using normal
operators via $x_4 = x_1 \lor x_2$, $x_3 \lor x_4$.  But you get
the same truth table with $x_4 = x_1 \bar \lor x_2$, $x_5 = x_3 \subset x_4$,
and both $\bar \lor$ and $\subset$ are decidedly non-normal operators.

The number of normalized 4-ary functions simply comes from their being
only 15 free settings for the truth table, since the all 0s one is fixed.

Don responds: by normalized boolean chain he simply meant one
that only uses normalized operators, not one that computes a normalized
function.

\vskip 0.05in \noindent [p 100] {\bf Algorithm L} ({\it Find normal lengths}).\hfil\break
The idea is pretty obvious -- make a vector for all possible truth tables.
Then set all the trivial 1-element functions (which are just the $x_j$ --
recall that since we are considering normal functions we don't have
a matching truth table for $\bar x_j$.  Then just keep forming all the
combinations of previously computed truth tables with the allowed operators,
keeping the shortest one -- which is easy since the algorithm always goes
through them in length order.

\subsec{Multiple Outputs}

\noindent [p 107] It is simply stated that the cost of the 3-ary median
operator is 4, but this hasn't been proven.  On the other hand, (23)
demonstrates the chain: $x_4 = x_1 \oplus x_2$, $x_5 = x_3 \land x_4$,
$x_6 = x_1 \land x_2$, $x_7 = x_5 \lor x_6$.  Note that the discussion
on p99 therefore implies there is a formula of length 4 as well, although
the formula 7.1.1.43 has 5 operators.  Let's see -- does just writing
the above out work?  Sure:
$
\left< x_1 x_2 x_3 \right> = \left(\left(x_1 \oplus x_2\right) \land x_3\right)
\lor \left(x_1 \land x_2\right).
$

\sec{Section 7.1.3: Bitwise Tricks and Techniques}

Most of the stuff in this chapter is covered a little more gently
in {\it Hackers~Delight} by Henry~S.~Warren,~Jr., also using an imaginary RISC machine,
but with more extensive discussion about what to do when certain
operators aren't available, as well as a bunch of other stuff not
covered here.

\subsec{Enriched arithmetic}

\noindent [p 135] Where does (18) come from? \hfil\break Combining (16) and
(17) we have $\bar x + 1 = \overline{x - 1}$.  Substitute $x-1$ for $x$
to get $\overline{x - 1} + 1 = \overline{x - 2}$, and use the same
formula to convert that to $\bar x + 2 = \overline{x - 2}$.  For any
finite $y$, just repeat $y-2$ more times to get $\overline{x - y} = \bar x + y$.

\vskip 0.08in \noindent [p 135] Left and right shift: what are the floor
relations doing in (19)?  \hfil\break They have to do with what happens when $k$
is negative, and so you can loose 1 bits off of the right.

Note that (19) and (20) are assuming infinite 2-adic numbers; on a real
computer one must be careful even with left shifts if any bits get shifted
close to the left end.  That is, logical left shift is not quite equivalent
to multiplying by powers of 2 on real computers because a) usually
it can't trigger overflow, while straight multiplication usually has
some mechanism for doing so, and b) for signed numbers you can
flip the sign if you shift a 1 into the leftmost position.  For example,
for a signed 64 bit number, $1 \ll 63$ is negative ($-2^{63}$).
Similar complications can ensue with right shifts because one has to
be careful to understand what bit gets inserted at the left.

\subsec{Working with the rightmost bits.}

\noindent [p 140] Equation (32): note that the exceptions 
are for any non-negative integer $b$ in $-2^b$: -1, -2, -4, etc.,
not just $-2^{d-1}$ (where $d$ is the word size).

\vskip 0.08in \noindent [p 140] So, given that (and the fact that
that representation doesn't work for 0), which of the formulae
(36)-(42) are affected if $x$ is one of those values? Equations (36)-(39)
all give zero for $x=0$, so can be said to work.
Equation (40)-(42) give $-1$, which doesn't really
match the description.  For $x = -2^b$, they all work as advertised.

\vskip 0.08in \noindent [p 140] Working with $x+1$\hfil\break
While Knuths expression (32) is a  cool trick for working out
these expressions, one weakness is that it doesn't make $x + 1$
particularly clean to express:
$$
\eqalign{
 x = & \, \alpha 0 1^a 1 0^b \cr
 x + 1 =& \cases{\alpha 1 0^a 0, &if $b = 0$;\cr
                         \alpha 0 1^a 1 0^{b-1} 1, &otherwise.\cr} 
}                        
$$
But there are surely ways to use that
to do interesting things:
$$
\eqalign{ 
 x\, | \left(x + 1\right) = & \cases{\alpha 1 1^a 1, & if $b=0$;
     \qquad [turn on the rightmost 0 bit]\cr
    \alpha 0 1^a 1 0^{b-1} 1,  & otherwise. \cr} \cr
 x\, \& \left(x + 1\right) = & \cases{\alpha 0^{a+2}, & if $b=0$;  
   \quad \qquad [turn off the trailing 1s]\cr
     \alpha 0 1^a 1 0^b, & otherwise. \qquad [or return $x$ if none] \cr} \cr
  x \oplus \left(x + 1\right) = & \cases{0^{\infty} 1 1^a 1, & if $b=0$;\cr
    0^{\infty} 0 0 0 0^{b-1} 1, & otherwise.\cr}\cr
 \bar x \, | \left(x + 1\right) = & \cases{1^{\infty} 1 0^a 0, & if $b=0$;
   \qquad [Create 0s at the position of any trailing 1s and 1s elsewhere]\cr
   1^{\infty} 1^a 1 1^b, & otherwise. \quad[Or all 1s if none]\cr}\cr     
 \bar x \, \& \left(x + 1\right) = & \cases{0^{\infty} 1 0^{a+1}, & if $b=0$;
   \qquad [Replace the rightmost 0 with a 1, turn all other bits off]\cr
   0^{\infty} 0^a 0 0^{b-1} 1, & otherwise.\cr}\cr
  \bar x \oplus \left(x + 1\right) = & \cases{ 1^{\infty} 0 0^a 0, & if $b=0$;\cr
    1^{\infty} 1 1^a 1 0^{b-1} 0, & otherwise.\cr}\cr
}
$$

\vskip 0.08in \noindent [p 140] Why is it called the ruler function?\hfil\break
Because if you want to draw a ruler with $k$ ticks (where $k$ is
a power of 2), then the number of trailing 0s plus 1 is the size of the
tick (with 0 a special case).  Example: make a ruler with 8
divisions. First, draw a large tick (size $m$ where $k = 2^m$).  
Then, in binary, count up:
$\rho\left(\{001, 010, 011, 100, 101, 110, 111\}\right) + 1 = 
\{1, 2, 1, 3, 1, 2, 1\}$.  If you draw this, it looks like, well,
a ruler. 

\vskip 0.08in \noindent [p 140] Why does $\rho\left(x - y\right) = 
\rho\left(x \oplus y \right)$? \hfil\break
Well, $\rho \left(x \, \oplus \, y\right)$ is counting the number of
positions (starting from the right) where $x$ and $y$ agree.
Now think about subtraction -- nothing happens in subtraction
until the first place that the two numbers disagree.  And so
all of the rightmost digits where they do agree will be zero.

\vskip 0.08in \noindent [p 141] Quick computation of $\rho$ in (46).
\hfil\break
Recall that {\tt SADD x,y,z} counts the number of places that
{\tt y} has a 1 while {\tt z} has a 0.  So {\tt SADD rho,(x-1),x}
is computing the sideways addition of (42): $\bar x \, \& \, 
\left(x - 1\right)$, which is the ruler function.

\vskip 0.08in \noindent [p 141] The $\mu_k$: the point is not 
exactly what integers the $\mu_k$ are (although Knuth being
Knuth, he gives a formula), but what the binary representation
looks like: alternating blocks of $2^k$ 0s and 1s.  They allow
you to select every other something (bit, $\ldots$, byte, $\ldots$).

\vskip 0.08in \noindent [p 141] So, how does the computation
of the ruler (number of trailing zeros) function actually work?
\hfil\break The idea
is that $x\, \& -x$ turns off all bits except the rightmost one --
so it is a power of 2.  So now we just need to figure
out what power of 2 it is.  That is, $x\, \& -x = 2^{\rho x}$.
This is basically a binary search -- see if there are any 1s
in the right half by {\tt AND}ing with $\mu_5$.  If there are not,
add 32 to the number of trailing zeros.  Then see if there are any
1s in the right half of each half, and add 16 if there are, etc.

An example -- but with 8 bits only.  The equivalent to (50)
is: {\tt NEGU y,x}, {\tt AND y,x,y}, {\tt AND q,y,m2}, {\tt ZSZ rho,q,4},
{\tt AND q,y,m1}, {\tt ADD t,rho,2}, {\tt CSZ rho,q,t},
{\tt AND q,y,m0}, {\tt ADD,t,rho,1}, {\tt CSZ rho,q,t},
where here \hbox{\tt m2 = \#0f}, {\tt m1=\#33}, and {\tt m0=\#55}.
Say that $x = 96 = \left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$,
so that $\rho\,x = 5$.  The first two instructions result in
$y \gets 96\, \& -96 = \left(0\,0\,1\,0\,0\,0\,0\,0\right)_2$.
Next $q \gets y\, \& \,\mu_2 = 0$ so {\tt ZSN rho,q,4} sets $\rho \gets 4$
(recall that {\tt ZSZ a,b,c} sets {\tt a} to {\tt c} if {\tt b} is 0, and
otherwise sets it to 0).  Next, $q \gets y\, \& \,\mu_1 = 32$, {\tt ADD t,rho,2} 
results in $t \gets 6$, and {\tt CSZ rho,q,t} leaves {\tt rho} unmodified
(recall: {\tt CSZ x,y,z} sets {\tt x} to {\tt z} if {\tt y} is 0, and otherwise
leaves it unmodified).  So, next $q \gets y\, \& \, \mu_0 = 0$,
$t \gets 5$, and the final instruction sets $\rho \gets 5$.

\subsec{Working with the leftmost bits}

\noindent [p 142] Using the same 8-bit example, the equivalent
code is {\tt SRU y,x,4}, {\tt ZSNZ lam,y,4;} {\tt ADD t,lam,2},
{\tt SRU y,x,t}, {\tt CSNZ lam,y,t;} {\tt ADD t,lam,1}, {\tt SRU y,x,t},
{\tt CSNZ lam,y,t}. So, again try $x = 96 = 
\left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$, so $\lambda\,x = 6$ (not 7!).  
Now the first instruction
does $y \gets 96 \gg 4 = 6 = \left(0\, 0\, 0\, 0\, 0\, 1\, \,1 \,0\right)_2$.
This isn't zero, so $\lambda \gets 4$ from {\tt ZSNZ} (zero or set
if not zero).  The next set of instructions does $t \gets 6$,
$y \gets x \gg 6 = 1$, $\lambda \gets 6$.  After that, $t \gets 7$,
$y \gets x \gg 7 = 0$, and on this step $\lambda$ doesn't get changed
because $y = 0$.

\subsec{Sideways addition}

\noindent [p 143] Note that {\tt MMIX} has a sideways add intrinsic
({\tt SADD}) -- which isn't actually that exotic.  x86 has such an intrinsic,
for example, in recent (SSE4) architectures, as well as the number of
leading zeros intrinsic.  Even {\tt MMIX} doesn't have that...  What
is exotic, however, is that {\tt MMIX}s {\bf SADD x,y,z}
computes $x \gets \nu \! \left(y \& \bar{z}\right)$, which is
taken advantage of in several of the formulae.

\subsec{Bit reversal}

\noindent [p 144] 
This is actually pretty easy to understand -- the
first step selects every other bit and reverses them.
The second step does that in blocks of two, etc.
Again, the same example on an 8-bit machine
with $x = 96 = \left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$:
$y \gets \left(x \gg 1\right) \, \& \, \mu_0 = 16$,
$z \gets \left(x \, \& \, \mu_0\right) \ll 1 = 128$,
$x = y | z = 144 = \left(1\,0\,0\,1\,0\,0\,0\,0\right)_2$.
So every other bit has been interchanged.

Okay -- now blocks of two.  Note we are
not starting with the original $x$, but the one
from the previous flips.
$y \gets \left(x \gg 2\right) \, \& \mu_1 = 32$,
$z \gets \left(x \, \& \, \mu_1\right) \ll 2 = 64$.
$x \gets 96$.  Yes, we are back where we started,
but it is the right thing, since 96 is what happens when
you 2-bit flip 144.  

Last step: $y \gets \left(x \gg 4\right) \, \& \mu_2 = 6$,
$z \gets \left(x \, \& \, \mu_2\right) \ll 4 = 0$,
and $x = 6 = \left(\,0\,0\,0\,0\,0\,1\,1\,0\right)_2$,
which is indeed the bit reversal of 96.

\vskip 0.1in \noindent {\bf Bit swapping}
\noindent [p 144] How does (67) work?\hfil\break
Well, $y$ gets the new j$^{th}$ elements with everything
else 0, $z$ gets the new i$^{th}$ element, again with
everything else 0.  Then $x \, \& \, m$ turns off those two
bits, and the or puts the new ones in.

\newitem{145} Now, how about (68)?\hfil\break
The first step puts $x_i \, \oplus \, x_j$ into position $j$ will all
other entries 0.  For all positions besides $i$ and $j$, the second
is $x_k \, \oplus \, 0 \, \oplus \, 0 = x_k$, so it doesn't touch those
positions.  In position $i$ we have $x_i \gets x_i \, \oplus \, 0 \, \oplus \,
\left(x_i \, \oplus \, x_j\right)$, since $y \ll \delta$ is a bit vector
with $x_i \, \oplus \, x_j$ in the i$^{th}$ position and 0 elsewhere.
$z \, \oplus \, 0 = z$, $\oplus$ is distributive, so this is
$\left(x_i \, \oplus \, x_i\right) \, \oplus \, x_j$.  And now, 
$z \, \oplus \, z = 0$, so this is $0 \, \oplus \, x_j = x_j$.
Thus, this chain sets $x_i = x_j$.  The same logic applies
to $x_j$, so this does indeed swap the bits.

\newitem{150} Notes on (84): $x^{\prime} = 
\left(x - \chi\right) \, \& \, \chi$\hfil\break
You know it must be non-obvious when Knuth actually explains
the derivation...  One note: the replacement of $\left( \bar \chi + 1\right)$
with $-\chi$ is the well-known `negate all the bits and add one is the same
as sign flipping' rule for 2s complement signed integers.  The $\& \chi$
part is because $x | \bar chi = x - \chi$ turns on lots of additional
bits not in $\chi$ -- in fact, all of them, in addition to the bits already
set in $x$.  So we have to turn those extra bits off.

This can be extended to larger sets by treating several words as
one contiguous number.  Addition and subtraction of such numbers 
is discussed in {\it Hackers Delight} -- the issue is more or less
how to detect overflow so effects can propagate across word
boundaries.

\newitem{150} Subcube generation\hfil\break
Here $a$ has a 1 at the location of each $*$, and $b$ has
a 1 at the location of each actual 1, and 0 for actual 0s or
$*$s.  So, what's going on in this formula?  All that is happening
is that the algorithm is generating all the subsets for $\chi = a$
and then adding the fixed values back in.  The derivation
is the same but starting by considering $x\, | \, \overline{a + b}$,
which now has 1s only at the positions fixed to be 0.  That is,
$a + b = a \, | \, b$, since $a \, \& \, b$ by construction.
Then we select only the wildcard bits, leaving the set bits in $b$
alone, and finally add them back in.

\subsec{Tweaking several bytes at once}

\noindent [p 151] Byte summing: equation~(87).\hfil\break
The first line ($z \gets \left(x \, \oplus \, y\right) \, \& \, h$)
selects the leading bits where either $x$ has a 1 or $y$
has a 1 -- but not both.  These are where carries might
cause problems.  The $0\,0$ case is obviously not a problem,
but neither is $1\,1$ (at least in this step) because the
addition will be modulo 256.  The second line explicitly
masks off the leading bit, then adds it back in with
$\oplus \, z$.  

So -- example: $x = 160 = \left(1\,0\,1\,0\,0\,0\,0\,0\right)_2$,
$y = 100 = \left(0\,1\,1\,0\,0\,1\,0\,0\right)_2$.  Done 
carelessly this would spill over: 
$x + y = 260 = \left(1\,0\,0\,0\,0\,0\,1\,0\,0\right)_2$,
which has 9 bits.  To correct for this problem, the first line
carries out $z \gets \left(x \, \oplus \, y\right) \, \& \, h =
\left(1\,1\,0\,0\,0\,1\,0\,0\right)_2 \, \& \, h =
\left(1\,0\,0\,0\,0\,0\,0\,0\right)$, indicating there is a carry
bit issue.  The second
line then does 
$$\eqalign{z \gets \left(\left(x \, \& \, \bar h\right) \, \oplus
\left(y \, \& \, \bar h\right)\right) \, \oplus \, z &= 
\left( \left(0\,0\,1\,0\,0\,0\,0\,0\right)_2 + \left(0\,1\,1\,0\,0\,1\,0\,0\right)_2
\right) \, \oplus \, z\cr &= \left(1\,0\,0\,0\,0\,1\,0\,0\right)_2 \, \oplus \, z \cr
&= \left(0\,0\,0\,0\,0\,1\,0\,0\right)_2}
$$
which is $260 \bmod 256$.

How about one where there should be a leading 1?
Try $y = 225$, so $x + y \bmod 256 = \left(1\,0\,0\,0\,0\,0\,0\,1\right)_2$.
Now they both have leading 1s, so the first line sets $z \gets 0$,
and the second line does the sum just throwing out the leading bit.

\newitem{151} Byte averages: equation~(88).\hfil\break
The first operation ($z \gets \left(\left( x \, \oplus \, y \right) \, \& \, \bar l
\right) \gg 1$), first finds all the places where the sum will be one,
then right shifts to divide by 2.  The $\&\,\bar l$ is to remove the
first (irrelevant) bit so that the shift doesn't move it across a byte
boundary.  The second line ($z \gets \left(x\,\&\,y\right) + z$)
deals with the positions where both $x$ and $y$ were 1,
so the sum is 2, which when divided by 2 is 1 again.

\newitem{152} Testing whether all bytes are zero: 
equations~(90) and (91). \hfil\break 
Recall from 42 that the effects of $\left(x - 1\right) \, \& \, \bar x$
is to turn all trailing 0s into 1s and zero everything else.  $l$
is used instead to make this into a bytewise operation.  The
largest this can be if the byte is non-zero is $\#7f$ -- that is, the 
first bit must be zero. So, when $\& \, h$ is applied you get zero.
Now -- what happens if the all 8 bytes are 0?  It turns out you
just get $h$ back, since $\left(0 - l\right) \, \& \bar 0$ is all 1s.

Now, on to identifying the zero byte with equation~(91).
Well, first imagine a byte that is zero.  Well,
$\left(x | \left(\left(0 | h\right) - l \right)\right) = \bar h$,
so this becomes $h \, \& \, h = h$, and each byte of $h$
is $\#80 = 128$.  Now, for non-zero bytes, the trick
is that the expression in parenthesis will have a leading 1,
and so when it is negated and $\&$ed with $h$ it will be zero.
If $x \neq h$ (and not zero) it must have a 1 bit somewhere
in the middle.  So the subtraction will leave the leading bit
added by the $|\,h$ will still be there, and we will get zero.
But if it is $h$, then we will have $h | \left(h - l\right) = \#ff$,
and hence $h \, \& \,0 = 0$.

\subsec{An application in directed graphs}

\noindent [p 159] As a reminder, Algorithm 2.3.5E is more or
less depth first search specialized for binary trees.

\newitem{159} {\bf Algorithm R} ({\it Reachability})\hfil\break
This is, or course, breath first search, very slightly modified from
the usual implementation to search from a set of starting vertices
rather than a single one.  The real point, of course, is the bitwise
operations implementation...

\newitem{160} {\bf Program R} ({\it Reachability})\hfil\break
So, how does it work?  It's not really all that tricky; almost the only
thing worth noting is that $X \subset R$, which is why $\sigma\left(R\right) -
\sigma\left(X\right)$ works -- the subtraction never causes any carry issues.
Also, ${\tt ANDN\ tt,t,tt}$, which is ${\tt tt} \gets {\tt t \, \& \, \overline{\left( t - 1 \right)}}$.
Examination of (32) and (34) shows this is equivalent to (37): it extracts the
rightmost 1, and thus is equivalent to $2^u = 2^{\rho\left(t\right)}$.

\sec{Section 7.1.4: Binary Decision Diagrams}

\noindent [p 204] Why (3) gives the truth tables of functions that
depend on $x_1$: because the first two elements of each truth table
are $f\left(0, x_2\right)$ (00, 01) and the second two are $f\left(1, x_2\right)$
(10, 11).  If the two halves are the same, then clearly $f$ doesn't
depend on $x_1$.

\newitem{205}  Correspondence between beads and BDDs.\hfil\break
Consider diagram (5): the top level is the full truth table before any
decisions are made.  Chose $x_1 = 0$ and going right restricts
the truth table to what is shown.  The critical point here is that truth
tables that are not beads don't show up, because those correspond
to variables that {\it don't matter}, at least given the previous choices.

\newitem{205} Working out the BDD for the ``more-or-less-random''
function.\hfil\break
We only skip a variable when the corresponding truth table isn't a bead.  Since
the full truth table isn't a bead, therefore the first node is on variable 1, and
since both halves are also beads in their own right, they will both be on
variable 2.  The $x_2 = 0$ branch is also a bead (11001001) so we will
get 3 nodes for that branch.  But the $x_2 = 1$ branch is not a bead
(0011), so we don't need $x_3$ and can just hook up true/false sinks, etc.

\subsec{BDD Virtues}

\noindent [p 206] The list of virtues is somewhat meaningless unless there is something
to compare it to.  So, how about just storing the truth table?  Evaluating
$f\left(x\right)$ is certainly fast -- it just amounts to memory lookup, probably
using bit-packing in the style of 7.1.3.(23).  The issue is that directly
storing the truth table can really take up a lot of space compared to a BDD.
For `random' truth tables BDDs don't have an advantage, but for the
sort of things encountered in practice it can make a very large difference,
at least once the number of variables becomes large.  

Consider (12): these are functions of 6 variables, so the truth table is $2^6$ bits
in length, which is just one octabyte.  Hence, it is massively more efficient
to just store the truth table.  Consider the maximal independent sets of the
49 `united states' as in (17); that has 780 nodes, so takes $780 \times 8 = 6240$
bytes.  Representing the truth table for 49 variables requires $2^49 / 2^3 = 2^46$
bytes, which is spectacularly larger.  The point being that BDDs can stop early
once something evaluates to true or false, the truth table can't.  If, for example,
the first five variables force a function of 32 variables to be false, the BDD
can stop there, while the truth table can't.  And sharing subtables can save
significant space as well.

The other big win is composing BDDs.  Combining binary functions with
logical operations is easier with truth tables (e.g., $h\left(x_1, \ldots, x_n\right)
= f\left(x_1, \ldots, x_n\right) \land g\left(x_1, \ldots, x_n\right)$), but
composing them (e.g., $h\left(x_1, \ldots, x_n\right) = f\left(x_1, \ldots,
g\left(x_1, \ldots, x_n\right), \ldots, x_n\right)$) is not so easy, and 
that corresponds to the way that humans, at least, like to build up complicated
functions.

\newitem{207} {\bf Algorithm C} ({\it Count solutions})\hfil\break
The powers of 2 are all about what happens when you skip unused
variables -- for example, the right branch in (6) which doesn't depend
on $x_3$ or $x_4$, or the leftmost pair of 5 boxes in the independent
sets example (12), where the leftmost one gets 2 for skipping variable 6
on the LO chain and 1 by not skipping 6 on the HI chain, and the one
next to that gets 2 from the LO chain connecting to true.

\newitem{208} Independent and maximal subset
examples: recall that the independent subsets are those such that
no edge is entirely contained in the subset (hence, two 1s in a row
for the cycle would not be independent).  And a kernel/maximal
independent subset means no element not in the subset can be
added without making it non-independent.   And if you had 3 0s in 
a row you could replace them with 010 to get a larger independent
subset.  But note these are {\it maximum} independent subsets -- 
they aren't necessarily the largest possible; for example, 001001
is maximal, but has fewer 1 elements than 010101.

\newitem{208} Generating random solutions: note that
it's not quite enough to just use the $c_k$ because we have
to account for skipping variables in the computation.  For example,
consider the path $x_1 x_2 x_3 \gets 1 0 0$ in the left hand part
of (13); to chose the next path, we can't just use $c_6$ and $c_5$
(which are both 1), but must take into account the fact that
going from 4 to 6 skips 5, and hence $c_6$ needs to be multiplied
by 2.  Unfortunately, we can't just pre-tabulate these values because
there can be multiple ways into the node.  In the example just given,
there's also a path from a 5 node for which $c_6$ shouldn't be multiplied
by 2.

\newitem{209} {\bf Algorithm B} ({\it Solutions of Maximum Weight})\hfil\break
Working through the example (problem~18) wasn't that helpful, but the
algorithm isn't that hard to understand if you just think about how to
combine optimum sub-problems.  The idea is that each node in a diagram,
or instruction in the sequential list form, corresponds to a bead.
The LO branch corresponds to setting $x_k \gets 0$ and hence
has the same weight as the LO branch, while taking the HI branch
sets $x_k \gets 1$ and has a weight equal to that of the high branch
plus $w_k$.  $m_k$ corresponds to the current weight, $t_k$ to
whether we took LO or HI, and $W_i$ provides a way to handle
the situation where we skip a few variables (like skipping 3 and 4
in diagram 6 in the rightmost branch).  So step 3 is all about moving up
the BDD choosing the best sub-branch at each step, then 4 is about
unwinding that chain back down the globally best one.

\newitem{211} Positive weights and independent subsets:
the kernel of maximum weight and an independent set of maximum weight
are the same with positive weights because for any independent set that
was not a kernel, you could always add at least one more element
and still be independent while increasing the total weight because the
weights are positive.

\subsec{Friendly functions}

\noindent [p 213] Constraint on the maximum size of the BDD for
a symmetric function: the number of nodes in the `body' is
$1 + 2 + \ldots + n = n \left(n + 1\right) / 2 = {n + 1 \choose 2}$,
and we only need one copy each of truth and falsehood, since
we share in BDDs.  So the maximum size is ${n + 1 \choose 2} + 2$.

\newitem{216} {\bf Algorithm R} ({\it Reduction to BDD})\hfil\break
This is pretty complicated as spelled out, although here doing
the example (problem~53) was somewhat helpful, if tedious.
R1 is relatively easy to understand -- it does a depth first search,
with {\tt HEAD} pointing to the first item of a linked list (through {\tt AUX})
of all fields with the same $v$ (variable).  So, for example,
if ${\tt HEAD}\left(2\right) = \sim b$ for some node with address $b$,
this means that the first element with variable 2 is node $b$,
and the negation of ${\tt AUX}\left(b\right)$ will point at the next node with the same
variable if any, and then that nodes {\tt AUX} at the next, etc., with
a $-1$ in AUX terminating the list (since negating that gives 0).
On each step, $p$ holds the current node, $s$ the next node to consider
(unless a higher priority one is found -- basically, any child of $p$ since
this is depth first).  After setting $s$, the possible children are checked,
{\tt LO} first then {\tt HI} so that {\tt HI} will be searched first (if {\tt LO}
was also present it's pushed onto the {\tt AUX} stack).  The truth and
falsehood nodes have been set to $-1$ so that the search doesn't
actually examine them directly.

Step R3 iterates through the levels of the variable $v$, starting at
the bottom.  It uses ${\tt HEAD}\left(v\right)$ to find the first element for a 
level, and then the {\tt AUX} fields to move to the next.  It has three jobs:
first, to correct the {\tt LO} and {\tt HI} links from a node that now point
to deleted nodes on lower levels, to eliminate unused nodes,
and to link together all nodes with the same {\tt LO} by their
{\tt AUX} fields.
The first part is handled by the second and third statements.  Recall
that deleted nodes are identified by having their {\tt LO} fields
made negative such that their complements point to equivalent 
non-deleted nodes.  So, first the algorithm looks at the {\tt HI}
field of the current node, and if it is pointing to a deleted node
(that is, ${\tt LO}({\tt HI}\left(p\right)right) < 0$, then {\tt HI}
is repointed at the non-deleted equivalent.  The same is done
for {\tt LO}.  

The next step is to eliminate unused nodes. If {\tt HI} and {\tt LO} for the current node
point at the same thing, then the node can be eliminated by pushing it onto
the stack.  The other two cases deal with the case where {\tt LO}
and {\tt HI} are different by linking together all nodes with the same
{\tt LO} field $x$ through their {\tt AUX} fields beginning with ${\tt AUX}\left(x\right)$.
In addition, the reduced nodes that aren't eliminated have their {\tt AUX} fields 
non-negative.  The job of R4 is to make $s$ points at the first of the 
non-eliminated nodes on this level, which are still linked together by their
(now positive) {\tt AUX} fields.

The next step is to loop through all these sets and remove the duplicates,
which is mostly the job of R7.  It does a similar trick -- set's the {\tt LO}
nodes of the duplicate nodes to the negation of the link to their equivalent
nodes and uses their {\tt HI} nodes to link them into the stack of deleted
nodes.  This may require another clean up if this caused our current node
to point at something eliminated.  Finally, check if we've reached the top.
If not, move up a level, if so, finish (after one final cleanup of the root).

\subsec{Synthesis of BDDs}

\noindent [p 218] What's the idea behind equation (37)?  It's designed
to match variables.  Only when the two nodes being merged are
for the same variable can we immediately proceed to merging their
subtrees.  If they don't, we need to merge the one with the larger
variable with both subtrees until it reaches its level.

\newitem{222} {\bf Algorithm S}
This one is too complicated to make me want to play through, so
I'll skip the lengthy explanation.  The first stage (43) is easy
to understand, but the second stage is pretty complicated, although
similar in principle to what was done in Algorithm~R.

\vskip 0.08in \noindent [p 224] {\bf Synthesis in a BDD base}

\noindent Recall that a BDD `base' is defined in the Shared BDDs section
on page 215.  But the idea is what you would expect: to share
common sub-functions between different BDDs.

\noindent [p 225] Why set all the unused variables to 0 low
and 1 high?  Because you can -- you could make any other
choice -- after all, $f$ doesn't depend on $x_1, \ldots, x_{v-1}$ --
but it is a convenient choice.  The idea behind $v$ in (52) follows --
the join doesn't depend on any variables that neither $f$ nor $g$
depend on.

\subsec{Quantifiers}

\noindent [p231] Independent sets and Kernels.  The Independent
set function of (67) is easy to understand (no two points connected
by an edge are included), but (68), the Kernel function, is a bit
more complicated.  Recall that a Kernel is a maximal independent set,
meaning adding any more vertices makes it no longer independent.
The first part just makes it independent.  The second clause amounts
to saying that for every vertex we must either have the vertex or
some vertex connected to it in the set, because if that wasn't the case 
we could add that vertex and still have an independent set.

On a related note, the statement that we must have an edge between
$u$ and $v$ if two kernels differ only in those variables.  It isn't
true in general that if two kernels differ for $u$, $v$ there must be 
an edge between them. However, if two kernels differ in {\it only} that
position there must be an edge essentially because there is no place
else to put the vertex and not have it ruin the independence property.

\subsec{Functional composition}

\noindent [p 233] The map 4 coloring relation is one relation
for each color (00, 01, 10, 11) that just says each submap
is independent, and hence colors never neighbor themselves.

\subsec{Nasty functions}

\noindent [p 234] The upper bound on the BDD size is theoretical,
and wouldn't quite be met in practice because it assumes you
can always get exactly the number of bits you need, while
on a real machine you need some multiple of the word size.
But you won't be that much worse.

\newitem{234} Average BDD size: the usual
caveats to the `average' size apply: choosing a random truth
table is a rather poor model for boolean functions that arise
in practice (as Knuth commented earlier, and as the examples
tend to reflect).

\newitem{235} Quasi-profiles: $q_k$ is the
number of distinct subtables because the QDD is reduced;
non-distinct subtables would be reduced to the same node.
Furthermore, (85)  arises just because if the two relations
are summed in (84) they are $1 + \sum b_k = 1 + B\left(f\right)$.
The other limit, (86) is because
$Q\left(f\right) = \sum q_k =
1/2 \left(\sum q_k + \sum q_k\right)
\le {1 \over 2} \sum_k \left(B\left(f\right) + 1\right) = \left(n + 1\right)
\left(B\left(f\right) + 1\right) / 2$.

\subsec{Zero Suppressed BDDs}

\noindent [p 250] Zeads: The first rule for zeads is easy to 
understand: A binary string ending in all 0s corresponds to
the function being zero whenever the current variable is 1, which 
means we can skip it in the ZDD.  The whole point of a ZDD
is to do exactly this: skip over any nodes where the HI pointer
leads to false.  However, the second rule doesn't
make any sense in the context of the truth tables we've been working
with, which never double the length of an odd number.  But Knuth
has defined it for arbitrary binary strings for reasons that aren't
clear.

\newitem{251} Exact cover problems: what is
the numbering scheme used for the chessboard?  The first
column is the upper left point, the next is the one next to it, etc.,
until the last column is the lower right.  The rows seem to be
vertical then horizontal for the cells in the same order;
so the first is the upper left domino in the leftmost diagram in (127),
the second is the upper leftmost domino in the rightmost
diagram.  Thus, the first column (upper left) is in the first two rows,
the second column (one to the right of the upper left) is
in the third and fourth, but also in the first because.


\vfil\break
\centerline{Notes on Knuth Chapter 7.2}
\vskip 0.3in
\centerline {\tt Section 7.2.1.1: Generating all $n$-tuples}
\vskip 0.2in

\noindent [p 284] Reflection: why does $g\left(k\right) = 2^n + g\left(
2^n - 1 - r\right)$ for $k = 2^n + r$?  The $2^n$ outside $g$ is
just starting the value with a 1, as per (5), and the value inside
$g$ is just working out the reflection.

\newitem{284} Inverting the binary formula (6): Note that
$a_j \oplus a_{j + 1} = b_j \oplus b_{j+1} \oplus b_{j+1} \oplus b_{j+2}
= b_j \oplus b_{j+2}$.  In other words, the xor chain cancels
out all the internal variables.  But since $b_k = 0$ for all
$k$ above some value, the upper term vanishes as well.

\newitem{284} Show $g\left(k \oplus k^{\prime}\right) = 
g\left(k\right) \oplus g\left(k^{\prime}\right)$:
$\left(k \oplus k^{\prime}\right)_j = k_j \oplus k^{\prime}_j$,
so $g_j\left(k \oplus k^{\prime}\right) = \left(k \oplus k^{\prime}\right)_j
\oplus \left(k \oplus k^{\prime}\right)_{j+1} =
k_j \oplus k^{\prime} \oplus k_{j+1} \oplus k^{\prime}_{j+1} =
\left(k_j \oplus k_{j+1}\right) \oplus \left(k^{\prime}_j \oplus k^{\prime}_{j+1}
\right) = g\left(k\right)_j \oplus g\left(k^{\prime}\right)_j$.

\newitem{284} Reversing a Gray binary code: The equation
before (12) is really just another way of stating the symmetry
relation at the top of the page.

\newitem{285} Chinese ring puzzle: note that the state
of the puzzle is the Gray's binary code number.  This makes sense --
we are removing or adding one ring at a time, which is what
a Gray's code is designed to do.

Removing or adding the ring at the end changes $a_0$ from
0 to 1 or vice versa, so by equation (8) changes $b_0$ to $b_0 \oplus 1$,
which is the same as doing $k \gets k \oplus 1$.  Removing or
adding a ring in the middle changes $\left(110^{\,j-1}\right)_2 \rightarrow
\left(010^{\,j-1}\right)_2$ or $\left(010^{\,j-1}\right)_2 \rightarrow
\left(110^{\,j-1}\right)_2$, which injects an extra 1 into all positions
$i \le j+1$, which is the same as $k \gets k \oplus \left(2^{\,j+1} - 1\right)$.

Note, however, that the ordering for the Gray code 
is the opposite of the solution to the ring puzzle
because generates the reversed Gray binary code.  That is,
we start with $\rho\left(1\right)$, then $\rho\left(2\right)$, etc.,
and so this, like all of the other Gray generating algorithms
amounts to computing successive values of $\rho$.  That is,
if we have ISA support for the ruler function, there is no need
for these algorithms, but assuming we don't we can in general
find faster ways than to get successive values of $\rho$, which
is what Algorithms~G and L are about.

\newitem{286} {\bf Algorithm G}\hfil\break
This follows immediately from the ring discussion.  
The parity bit tells us if we want to make a type (a) move (remove or add the ring
at the end), if not G4/G5 find $\rho$ and remove or add that ring.

Interestingly, in performance comparison tests (in Go), Algorithm~G is faster
than the apparently simpler method of simply using the relation
$a_j = b_j \oplus b_{j-1}$ of equation~7.  It also beats the loopless
generator of Algorithm~L.  However, in \CC\ the bit twiddling soluition
is faster.

\newitem{288} Recursion relation between $w$:
$w_k\left(x\right) = r_{\rho\left(x\right) + 1}\left(x\right) w_{k-1} \left(x\right)$.
First, some examples: $k = 4 = \left(1 0 0 \right)_2$, $a = k \oplus 
\left(k \gg 1\right) = \left(1 1 0 \right)_2$, so from (17)
$w_4 = r_2 r_3$; $k = 5 = \left(1 0 1\right)_2$, $a = \left( 1 1 1 \right)_2$,
so $w_5 = r_1 r_2 r_3$; $k = 6 = \left( 1 1 0 \right)_2$,
$a = \left(1 0 1\right)_2$, $w_6 = r_3 r_1$.  The last may seem to
violate (18), which says that $w_6 = r_2 w_5 = r_1 r_2^2 r_3$, but
note that $r_j^2 = 1$, so it is correct.

So how do we show the general relation?  Well, it's helpful to add another
bitwise formula following 7.1.3: $k = \alpha 0 1^a 1 0^b$,
$k \gg 1 = \alpha 0 1^1 1 0^{b-1}$ (with the understanding that
$1 0^{-1} = \emptyset$), so $k \oplus \left(k \gg 1\right) = 
\left(\alpha 0 \oplus \alpha\right) 1 0^1 1 0^{b-1}$.
Next, $k - 1 = \alpha 0 1^a 0 1^b$, and $\left(k - 1\right) \oplus
\left(\left(k - 1\right) \gg 1\right) = \left(\alpha 0 \oplus \alpha\right)
1 0^{a-1} 1 1 0^{b-1}$.  Thus, considering equation (17),
$w_k$ and $w_{k-1}$ differ in position $b + 1 = \rho(k) + 1$
where the latter has a 1 and the former a 0.  Thus,
$w_{k-1} = r_{\rho\left(k\right) + 1} w_k$, and if we multiply
both sides by $r_{\rho \left( k \right)+1}$ and again take advantage
of $r_j^2 = 1$, we get $w_k = r_{\rho\left(k\right) + 1} w_{k-1}$
as claimed.

\newitem{288} Why does $r_j\left(x\right)^{a \oplus b} =
r_j\left(x\right)^{a+b}$?  This goes back to 7.1.3.(89):  
$x + y = \left(x \oplus y\right) + \left(\left(x \& y \right) \ll 1\right)$,
and noting that the $\left(a \& b\right) \ll 1$ part will always be a power
of 2, and so cancels out because $r_j\left(x\right)^2 = 1$.

\newitem{290}  {\bf Algorithm L} ({\it Loopless Gray binary 
generation})\hfil\break
Note that passive somewhat confusingly means the corresponding
bit being set in $b$, while active means it is zero.  And, indeed,
considering the example given, the underlined bits in $a$ are
simply counting $0, 1, 2, 3, \ldots , 8$.  Translating items 1 and 2
to this language, which I find a bit clearer, item 1 is:
if $b_j$ is 1 and $b_{j-1}$ is 0, then $f_j$ is the smallest index
$j^{\prime} > j$ such that $b_{j^{\prime}} = 0$.  So, for example,
consider $b = 0100$.  Then the quantity of interest is $f_2$,
which is 3 -- and indeed $b_3 = 0$.  Note there can be more
than one -- for $b=1010$, we have $f_3 = 4$ and $f_1 = 2$.

If $f_0 = 0$ already -- that is, $b_0$ is not the rightmost 1
in a block of 1s -- then we simply get $j \gets 0$.  This corresponds
to odd values of $k$ where $\rho\left(k\right)$ is 0, meaning it's
time to flip the last bit.  And, of course, the last bit of $a$ gets
flipped on every other step.  But then we copy the value from $f_1$,
which means that we point to the left of the block of 1s starting at
$b_1$.

Consider what happens when we transition from $b=0011$
to $0100$.  $f_0 = 2$ is the index of the rightmost active (0) element.
All the other $f_j = j$. Now we set $j \gets 2$, and then flip it in $a$.
But how does that make $a_0$ and $a_1$ passive?  Well, remember
the relation between $b$ and $a$: $b_j = a_j \oplus a_{j+1} \oplus \ldots$
(7.2.1.1.7). Thus, by flipping $a_j$ we are flipping the signs of all $b_{\le j}$,
while leaving $b_{>j}$ untouched.

For the second part of L3, since $a_j$ and $a_{j-1}$ are both active
prior to this step, we must have $f_j = j$, $f_{j-1} = j - 1$.  So
in this step we are making $j+1$ be the rightmost element in a block of 1s,
which means $j+1$ can't be if it was, and so must have $f_{j+1} = j + 1$.

\subsec{Shift Register Sequences}

\noindent [p 302] Note that the idea in equation (53) is to take five
numbers sequentially from overlapping sub-sequences -- that is
00000, then shift the left 0 off and one in from the right to get 00001,
etc.

\sec{Section 7.2.1.2: Generating all Permutations}

\noindent [p 319] {\bf Algorithm L} The important thing to note here is that
this generates only {\it distinct} permutations -- so 1223 has only 12
permutations, not 24, because in any permutation the two 2s can be
exchanged.

It's useful to create an example of this
in action.  Consider the sequence $246$ -- so $a_1 = 2$, $a_2 = 4$,
$a_3 = 6$, and the convenience value $a_0$ is, say $a_0 = 0$.
So, step by step:\hfil\break
{\bf L1} Output 246.\hfil\break
{\bf L2} Let $j \leftarrow 2$, which does satisfy $a_2 < a_3$.\hfil\break
{\bf L3} Set $l \leftarrow 3$.  We have $a_j < a_l$.  Interchange
  $a_2$ and $a_3$, which gives us $264$.\hfil\break
{\bf L4} is a null step, since $j + 1 = 3$ is the last element.\hfil\break
{\bf L1} Output 264.\hfil\break
{\bf L2} Let $j \leftarrow 2$, but now $a_2 > a_3$, so decrease
  $j$ until $j = 1$ (since $a_1 < a_2$).\hfil\break
{\bf L3} Set $l \leftarrow 3$.  $a_1 < a_2$, so swap to form
 $462$.\hfil\break
{\bf L4} Reverse $a_2$ and $a_3$ to get 426.\hfil\break
{\bf L1} Output 426.\hfil\break
{\bf L2} $j \leftarrow 2$, since $a_2 < a_3$ ($2 < 6$).\hfil\break
{\bf L3} $n \leftarrow 3$, swap to get 462.\hfil\break
{\bf L4} Swapping is null.\hfil\break
{\bf L1} Output 462.\hfil\break
{\bf L2} $j \leftarrow 1$.\hfil\break
{\bf L3} $n \leftarrow 2$, swap to get $642$.\hfil\break
{\bf L4} reverse from 2 to get $624$.\hfil\break
{\bf L1} output 624.\hfil\break
{\bf L2} $j \leftarrow 2$.\hfil\break
{\bf L3} $l \leftarrow 3$.  Swap to get $642$.\hfil\break
{\bf L4} Reversing is null.\hfil\break
{\bf L1} Output 642.\hfil\break
{\bf L2} $j$ is 0, so terminate.\hfil\break
The output is ${246, 264, 426, 462, 624, 642}$, which is in lexicographic
order.

So, how does this algorithm work?  The discussion in the text is helpful, but
a few notes are in order.  First, because this works lexicographically, the
changes try to advance from right to left.  That's why the first step looks
for the {\it rightmost} element that is in order as the place to modify.

Next we try to increase $a_j$ by the smallest amount possible by finding
the smallest element $a_l$ such that $a_l > a_j$ by scanning from the right,
and taking advantage of the fact that the elements after $a_j$ are in
reverse sorted order by L1 (which is what makes it efficient to scan
in from the right).  Then we take advantage of that again to reverse to find the
final version.

\vskip 0.08in
\noindent [p 322] {\bf Algorithm P} ({\it Plain Changes})\hfil\break
Most of this algorithm is just how to count using a Gray code in the
mixed radix system $1 2 3 4 \ldots n$.  That part is actually pretty easy --
first you count up in the last position until you reach $n$, then you advance
the next position, then you count down in the last position, etc.  As such,
$o$ just keeps track of which direction you are going in each position of $c$.
The tricky part is in understanding which elements you should swap when
changing $c_j$ to $q$.  In most cases this isn't so strange -- you are just
moving $j$ either one element to the left or right -- so, for example, when 
$j = 4$ you are exchanging $4$ with the element to either it's left.  
So what we need to know is: given $j$ and $c_j$, where is $j$ in
$a$?  This is simple enough: starting from the right, $j$ goes into
slot $j - c_{j}$ of the {\it remaining} slots.  So, for $c=0122$
you first put $4$ into $a_2$, then 3 into the 1st remaining slot
(so $a = 34xx$), then 2 into the 1st remaining one, and 1 into
the last one to get $a=3421$.  Which is $a_{j - c_j + m}$,
where $m$ is the number of indices $k > j$ where 
$k - c_k \le j - c_j$.  The simpler form used in the algorithm
arises because we don't care about unravelling the entire
permutation, just the position for the one we want to move,
and there are rules that stop that process.  To wit, you can
only get to $j$ if all indices greater than $j$ are saturated --
either 0 or $j-1$.


\subsec{A general framework}

\noindent [p 327] Sims tables.\hfil\break
Knuth's use of commas makes this a little ambiguous, but the Sims table
does {\it not} contain permutations $\sigma^{\prime}_{kj}$ that send
$k \mapsto j$ and do {\it not} fix the value of all elements greater than $k$.

Also note that there are no elements $\sigma_{kj}$ where $j > k$
because such a permutation couldn't fix all elements $> k$.  So
we only have to worry about $\sigma{k0} \ldots \sigma{kk}$.

\newitem{328} 4-cube example.\hfil\break
Note that the numbering starts at 0, so $S_{n-1} = S_f$ here.  So $S_f$
doesn't have to fix anything, and so has exactly one permutation that sends
$f$ to each other value, and hence 16 values.  $S_e$ has to fix $f$, but
can only contain permutations that actually preserve symmetry, so has
fewer entries.

The example membership check halts when $\alpha^{\prime\prime}$
is actually in $S_e$ because we can go the rest of the way with
identity permutations which are in each $S$.

\newitem{328} Generating all elements: the point is that 
$\sigma\left(a, \cdot\right)$ fixes all elements greater than $a$,
and since we advance through the permutations at the right end
the most slowly (e.g., $c_{n-1}$ changes more slowly than $c_{n-2}$,
etc.

\subsec{Using the general framework}

\noindent [p 328] The point about $\sigma\left(k, j\right)$ not
necessarily being $\sigma_{kj}$ is just that you can store them in
the $\sigma\left(\cdot, \cdot\right)$ array in any order as long
as $\sigma\left(\cdot, 0\right)$ is the identity.

\newitem{329} The example of figure~39: the way that the
permuations is written backwards makes this moderately confusing.
The point is that the permutation $c_3 c_2 c_1$ is
$\sigma\left(1, c_1\right) \sigma\left(2, c_2\right) \sigma\left(3, c_3\right)$.
Note that we proceed through the bottom row of figure~39 from left
to right, and when we advance from $121$ we are starting from
$\sigma\left(1, 1\right) \sigma\left(2, 2\right) \sigma\left(3, 1\right)$.
Then G4 premultiplies by 
$$
 \tau\left(3, 2\right) \omega\left(2\right)^{-} = 
  \tau\left(3, 2\right) \sigma\left(2, 2\right)^{-} \sigma\left(1, 1\right)^{-} =
  \sigma\left(3, 2\right) \sigma\left(3, 1\right)^{-} \sigma\left(2, 2\right)^{-} 
  \sigma\left(1, 1\right)^{-} .
$$
and so G4 is doing 
$$
\sigma\left(3, 2\right) \sigma\left(3, 1\right)^{-} \sigma\left(2, 2\right)^{-} 
  \sigma\left(1, 1\right)^{-} \sigma\left(1, 1\right) \sigma\left(2, 2\right) \sigma\left(3, 1\right)
  = \sigma\left(3, 2\right) = \sigma\left(1, 0\right) \sigma\left(2, 0\right) \sigma\left(3, 2\right)
$$
which is $200$ since $\sigma\left(k, 0\right)$ is the identity.
 
\subsec{Faster, Faster}\hfil\break
Incidentally, an implementation of Algorithms~L and P
in golang shows that Algorithm~P is about 4 times faster
for cheap compares (and does even better relatively
if comparisons are expensive).  However, the abstraction
used prevents most of the improvements from exercises
7.1.2.1.(1) and 7.1.2.1.(16).  Algorithm~G using Heap's
approach of 7.1.2.1.(40) is about 10\% faster still.

\sec{Section 7.2.1.3: Generating all Combinations}

Implementations of Algorithms~L, T, and F in c++ show that Algorithm~T is about
25\% faster than Algorithm~L (the examples used were $14 \choose 4$ and $16 \choose 5$),
but Algorithm~R is about 20\% faster still.

\sec{Section 7.2.1.4: Generating all Partitions}

\noindent {\bf The number of partitions}

\newitem{396} Recurrence relation for the number of partitions.  We have
$$
 \prod_{m=1}^{\infty} 1 - z^m = \sum_{-\infty < n < \infty} \left(-1\right)^n z^{\left(3 n^2 + n\right) / 2}
  = \sum_n \left(-1\right)^n z^{3 n^2 / 2} \left(z^{n/2} + z^{-n/2}\right),
$$
but how do we get the complicated recurrence relation?  Well, we have 
$ \prod_m 1 / \left(1 - z^m\right) = P\left(z\right)$, so $ \prod_m \left(1 - z^m\right) = 1 / P\left(z\right),$
or $P\left(z\right) \prod_m \left(1 + z^m\right).$  So, writing out the first few terms, this is
$$
 \left(1 - z - z^2 + z^5 + z^7 - \ldots \right) \left(p\left(0\right) z + p\left(1\right) z + p\left(2\right) z^2
 + \ldots\right) = 1.
$$
Consider the coefficient of $z^n$ in this product.  It is $p\left(n\right) z^n - p\left(n - 1\right) z^{n-1} z
- p\left(n - 2\right) z^{n-2} z^2 + \ldots = 0$, from which the stated occurrence follows immediately.

\newitem{399} Partitions into at most $m$ parts.\hfil\break
Why is this equal to $\vert {n + m \over m}\vert$?  You can see this by telescoping the recurrence
formula:
$$
\eqalign{\left| {n + m \over m} \right| 
  &= \left| {n \over m} \right| + \left| {n + m - 1 \over m - 1} \right| \cr
  &= \left| {n \over m} \right| + \left| {n \over m - 1} \right| + \left| {n + m - 2 \over m - 2} \right| \cr
  &= \left| {n \over m} \right| + \left| {n \over m - 1} \right| + \left| {n \over m - 2} \right| +
                          \left| { n + m - 3 \over m - 3} \right| \cr
  &= \ldots}
$$
which is just the sum of the number of ways to split $n$ into $m, m-1, \ldots, 1$ parts -- exactly
what we want. 

\sec{Section 7.2.1.6: Generating all Trees}

\noindent [p 443] {\bf Algorithm P} \hfil\break
Try it in action for $n=3$:
\newstep {P1} We have $a = ) \vert ()()()$, $m = 5$, where the $\vert$
separates the sentinel value.
\newstep {P2} Output ()()().
\newstep {P3} First set $a \gets ) \vert ()()))$, then take the easy
case by with $a = ) \vert ()(())$ with $m \gets 4$.
\newstep {P2} Output ()(()).
\newstep {P3} Set $a \gets ) \vert ()()))$.  We can't take the easy case.
\newstep {P4} $j \gets 3$, $k \gets 5$.  Then do $a \gets ) \vert ()))()$,
$j \gets 2$, $k \gets 3$.
\newstep {P5} $a \gets ) \vert (())()$, $m \gets 5$ (easy case).
\newstep {P2} Output (())().
\newstep {P3} $a \gets (()())$ (easy case), $m \gets 4$.
\newstep {P2} Output (()()).
\newstep {P3} $a \gets ) \vert (())))$, then we take the easy branch and
 $a \gets ((()))$. $m \gets 3$.
\newstep {P2} Output ((())).
\newstep {P3} Set $a \gets ) \vert (())))$.
\newstep {P4} Cycle through, eventually getting $a \gets ) \vert ))()()$ and
 $j = 0$.
\newstep {P5} Terminate.

As usual the rule is: find the rightmost element that can be increased (so
the leading elements of the rightmost set of ('s), increase it, fill out the rest with the least possible pattern,
which is of the form ()() etc.  
Step {\bf P2} handles the case where the end of the string has the form
$)()^n$ for some $n$ by quickly converting to $())^n$ and moving the current
position one to the left.  So $)()) \rightarrow ()))$, $)())) \rightarrow ())))$, etc.
The more interesting step taken by {\bf P4} handles
cases like )(()), but may require handling more of the pattern.  It does this by
converting any run of )'s to ('s while retaining the balance. 

It is also interesting to compare this with the
bit twiddling solution of Problem~7.1.3.(23).  Which is faster?  Well, naturally
this depends on the details of the architecture in use -- is there a
population count intrinsic, for example.  But in practice converting the bit
form to a string is the expensive part.  If this is not done (so the use case
for the parens can work with the bit form efficiently), then (based on an
implementation in Go) the bit twiddling approach is $5-6\times$ faster.  But if
one really needs the string form, then Algorithm~P is $\sim 2\times$ faster --
at least in my implementation!

\end