\topglue 0.5in
\centerline{Notes on Knuth Chapter 7.1: Zeroes and Ones}
\vskip 0.3in
\centerline{\tt Section 7.1.1: Boolean Basics}
\vskip 0.2in

\noindent
{\bf Basic Identities} \hfil\break
\vskip 0.05in
\noindent [p 50] Eq (9) states $\left( x \oplus y \right)
\oplus x = y$.  Why?  \hfil\break
Well, $\oplus$ is not distributive, even with itself,
but it is commutative and associative. The above is therefore
equivalent to $y \oplus \left( x \oplus x \right) = y \oplus 0 = y$.
The same argument shows that $\left( x \oplus y \right) \oplus y = x$.

\vskip 0.05in
\noindent [p 51] Why are Eq (13)-(15) true? \hfil\break
I don't see how to derive (13), or (14) besides just writing out the truth table, but (15) is:
$x \oplus y = \left(x \vee y\right) \wedge \overline x \wedge y$, which
is true by inspection (either x or y has to be true, but not both).
These can be expanded using (12) and (1) to give
$x \oplus y = \left(x \vee y\right) \wedge \left(\overline{x} \vee \overline{y}
\right) = \left(x \wedge \left(\overline{x} \vee \overline{y}\right)\right)
\vee \left(y \wedge \left(\overline{x} \vee \overline{y}\right)\right)$.
Applying (1) to each of these terms again gives things like
$\left(\overline{x} \wedge x\right) \vee \left(\overline{y} \vee x\right) =
\overline{y} \vee x$, which, when applied to both terms gives
$x \oplus y = \left(x \wedge \overline{y}\right) \vee \left(\overline{x}
\wedge y\right)$, which is the claimed identity.

\vskip 0.1in
\noindent
{\bf Functions of n variables} \hfil\break

\noindent [p51] Why does the functional decomposition
of eq (16) and (17) work?\hfil\break
First, note that $h$ is 0 if
$f\left(x_1,\ldots,x_{n-1},0\right) = f\left( x_1,\ldots,x_{n-1},1\right)$, so
in this case we have,
 $g\left(x_1,\ldots,x_{n-1}\right) \oplus 0 = f\left(x_1,\ldots,x_{n-1},0\right)$.
 Since we already said that $f\left(x_1,\ldots,x_{n-1},0\right) = f\left( x_1,\ldots,x_{n-1},1\right)$, this must be true.  
 
If $f\left(x_1,\ldots,x_{n-1},0\right) \neq f\left( x_1,\ldots,x_{n-1},1\right)$,
then $h\left(x_1,\ldots,x_{n-1}\right)=1$, so we have $1\, \wedge\, x_n
= x_n$.  Thus, the rhs of (16) becomes $f\left(x_1,\ldots,x_{n-1},0\right) 
\, \oplus \, x_n$.  Now note that if $x_n = 1$ this is
$f\left(x_1,\ldots,x_{n-1},0\right) \, \oplus 1 = 
\overline{f\left(x_1,\ldots,x_{n-1},0\right)}$.  But we already said that
$f\left(x_1,\ldots,x_{n-1},0\right) \neq f\left( x_1,\ldots,x_{n-1},1\right)$,
so this is just $f\left(x_1,\ldots,x_{n-1},1\right)$, which is what we want.
Similarly, for $x_n = 0$, we just get
$f\left(x_1,\ldots,x_{n-1},0\right) \oplus 0 = f\left(x_1,\ldots,x_{n-1},0\right)$.

\vskip 0.05in
\noindent [p52] That demonstrates why (16) works -- why does (18), 
the ``law of development'', work?\hfil\break
First, let's say that $x_n = 0$.  Only the first term applies, giving
$f\left(x_1,\ldots,x_{n-1},0\right) \wedge 1 = f\left(x_1,\ldots,x_{n-1},0\right)$,
which is indeed the value of $f\left( x_1,\ldots,x_n \right)$ when $x_n = 0$.
It works the same way for $x_n = 1$.

\vskip 0.05in \noindent [p55] {\bf Theorem Q}\hfil\break 
A prime implicant corresponds to a 1 in the
truth table of the function.  Since the function is monotone, the implicant
can't become false (0) when any of it's values go from 0 to 1.  If one
of the terms in the prime implicant were, say, $\overline{x_i}$, then
when that changed from 0 to 1, the implicant would become false.  This
is not allowed.

\vskip0.1in \noindent {\bf Satisfiability}\hfil\break

\noindent [p56] 3SAT\hfil\break
The text uses problem 39 to explore a fairly general form where the 
operators between variables
in the function can be any of the 16 boolean 2-ary functions, but it's
interesting enough to explore the more restrictive reduction from SAT --
which is how to turn a general problem in conjunctive normal form
into one with only three literals per clause.  After all, we know that
any boolean function can be written that way, so while it may be
possible to make a more efficient 3SAT instance using problem 39,
just studying how to do this with CNF form is enough to imply that
all boolean functions can be turned into 3SAT.

Clearly it's enough to figure out how to turn any clause $t_1 \vee \ldots
\vee t_n$ into a conjunction of 3-literal clauses.  To do this, one introduces
``nuisance'' variables $x_i$ for $2 \leq i \leq n - 2$ and uses them to sandwich
each literal.
$$
t_1 \vee \ldots \vee t_n = \left(t_1 \vee t_2 \vee x_2\right)
\land \left(\bar x_2 \vee t_3 \vee x_3\right)
\land \left(\bar x_3 \vee t_4 \vee x_4\right)
\land \ldots \land \left(\bar x_{n-2} \vee t_{n-1} \vee t_n\right) .
$$
Actually, they aren't {\it identical}, they are {\it equisatisfiable}.

So, why does this work?  Well, imagine this clause is satisfiable.
Then at least one of the $t_i$ must be 1. 
The point is that we can force all of the other 3-literal clauses
to be true by a judicious choice of the $x_i$.   Say that the
literal is $t_j$.  We then set $x_k$ for $k \le j$ to 1,
and $x_k$ for $k \geq j$ to 0 and the clause is satisfiable.

Conversely, say that the clause is not satisfiable, so all the $t$
are zero.  Then to satisfy the new form we need $x_2 = 1$ for
the first clause, but then that forces $x_3 = 1$ in the second,
etc., until we get to the last clause where we need $x_{n-2} = 0$
but it has been forced to be 1 by the previous clause.  So therefore
if the original clause is satisfiable we can satisfy the new one, and
if it isn't then we can't.

\vskip0.1in \noindent {\bf Simple Special Cases}\hfil\break

\noindent [p57] {\bf Theorem H} \hfil\break
The first step in the formula is to use the 
distributive law (2): $\left(x \wedge y\right) \vee z = \left(x \vee z\right) 
\wedge \left(y \vee z\right)$.
The second step makes use of the fact that $\vee$ is commutative to move all the $y$s 
rightward in the left term.  We then have $\left(x_1 \vee \overline{x}_2
\vee \ldots \vee \overline{x}_k\right) \vee \overline{y}_1 \vee \ldots \vee 
\overline{y}_k
\geq \left(x_1 \vee \overline{x}_2 \vee \ldots \vee \overline{x}_k\right)$,
which is clearly true since the extra terms can either leave the truth value 
unchanged or increase it from 0 to 1.  The same logic applies to the second term.

\vskip0.05in \noindent [p58] In the example of Horn clauses following theorem H, where 
do the propositions come from? \hfil\break  
Well, take the first one: $\bf xE \Rightarrow xT$ states
that it is allowable for an expression to start with {\bf x} if and only if
it is allowable for a term to start with {\bf x}; this is clear from the first
line of the specification.  

How do these convert into Horn clauses?  Well, recall that $x \Rightarrow y$
is the same as $\overline{x} \wedge y$ (Table 1).  The Horn clause expression is
just all of those $\wedge$ed together.

\vskip0.1in \noindent {\bf Medians}\hfil\break

\noindent [p63] Self duality: in the discussion after (46), why does a formula
need to be indifferent to swapping $\wedge$ and $\vee$?  Recall De Morgan's
laws (11 and 12): $\bar{x \wedge y} = \bar x \vee \bar y$ and $\bar{x \vee y} =
\bar x \wedge \bar y$.  So, using (46), when we negate the entire function,
we get something like $\bar{\left(x_i \wedge \ldots \wedge x_j\right) \vee
\left(x_k \wedge \ldots \wedge x_l\right) \vee \ldots} =
\left(\bar x_i \vee \ldots \vee \bar x_j\right) \wedge
\left(\bar x_k \vee \ldots \vee \bar x_l\right) \wedge \ldots$;
if this is to be equal to the original with the variables negated, then
indeed we must be able to interchange $\vee$ and $\wedge$.

\vskip0.1in \noindent {\bf Threshold functions}\hfil\break

\noindent [p75] Negative weights: to get rid of negative weights,
the replacements $x_j \gets \bar x_j$ and $w_j \gets -w_j$ seem
pretty obvious, but what about the $t \gets t + \left|w_j\right|$?
Well, consider some variable $x_j$.  the old contribution was
$w_j x_j$, the new one is $-w_j \bar x_j$.  The first evaluates to
$0, w_j$ for $x_j = 0, 1$, the second to $-w_j, 0$.  These aren't
equal, so the right hand side ($t$) also needs to be adjusted.
Let's say the old threshold was 2, and $w_j = -2$.  Then with
the negative weight the rest of the terms need to add up to 2 or 4
for $x_j = 0, 1$.  For the new form, if we didn't adjust $t$, they
would have to add up to $0, 2$, so the formula are no longer
equivalent.  If we add $\left|-w_j\right| = 2$ so that $t \gets 4$,
then they need to be $2, 4$, as before.

\vskip 0.05in \noindent [p 77] {\bf Theorem T} \hfil\break
The proof works because the number
of vectors where we must have $f\left(x^{\left(m\right)}\right) = 
g\left(x^{\left(m\right)}\right) = 1$ is $N\left(f\right) - k = N\left(g\right) - k$.
That means there must also be $k$ vectors where $f\left(y^{\left(k\right)}\right) = 1$
and $g\left(y^{\left(k\right)}\right) = 0$.

In the next step, note that $w \cdot x^{\left(j\right)} \geq t$ for each of the $k$
such values. 

\vfil\break

\vskip 0.3in
\centerline{\tt Section 7.1.2: Boolean Evaluation}
\vskip0.2in

\noindent [p 97] Different boolean chains with the same diagram: the point
of the discussion in the paragraph following the one with (4) is that the same
steps are just being done in a different order.  As long as they don't depend
on each other, that's fine -- which is equivalent to a different topological sorting.

\vskip 0.05in \noindent [p 98] Note the convention for the truth tables here:
the columns are just $\left(x_1 x_2 x_3 x_4\right)_2$ counted up from 0.

\vskip 0.05in \noindent [p 99] The difference between $L\left(f\right)$
and $C\left(f\right)$ is that $C$ is allowed to re-use intermediate results,
$L$ is not.  So, the reuse of $x_6$ in (7) is fine for $C$, but not for $L$.

\vskip 0.1in \noindent {\bf Optimum chains for n=4}

\noindent [p 99] Each boolean function has the same depth, length,
and cost as it's negation because the latter computed by using the same
chain except for the last step, where the operator $\circ$ is replaced
by the 2-ary binary function with the negated truth table.  And it can't
be shallower/shorter/cheaper because if we had such a chain for the
negation we could negate it in the same fashion to get a shallower/shorter/cheaper
chain for the original function.

\vskip 0.05in \noindent [p 100] {\it Normalized chains:} The idea behind (10),
of course, is that it makes a non-normal function normal.  However, the claim
that a Boolean chain is normal if and only if each of its binary operators is
normal seems to be wrong -- at least if a chain being normal means what
I think it does.  It's not actually defined, but I'm assuming it means that the
function it evaluates is normal.  

Now, the if direction is obvious -- any chain composed of normal operators
will be normal.  And from the discussion at the top of the page, you can convert
any normal chain that doesn't use normal operators into one that does.
But I can construct normal functions using non-normal operators.
For example, take the simple 3-ary function with truth table
$0\,1\,1\,1\,1\,1\,1\,1$.  This can be computed using normal
operators via $x_4 = x_1 \lor x_2$, $x_3 \lor x_4$.  But you get
the same truth table with $x_4 = x_1 \bar \lor x_2$, $x_5 = x_3 \subset x_4$,
and both $\bar \lor$ and $\subset$ are decidedly non-normal operators.

The number of normalized 4-ary functions simply comes from their being
only 15 free settings for the truth table, since the all 0s one is fixed.

\vskip 0.05in \noindent [p 100] {\bf Algorithm L} ({\it Find normal lengths}).\hfil\break
The idea is pretty obvious -- make a vector for all possible truth tables.
Then set all the trivial 1-element functions (which are just the $x_j$ --
recall that since we are considering normal functions we don't have
a matching truth table for $\bar x_j$.  Then just keep forming all the
combinations of previously computed truth tables with the allowed operators,
keeping the shortest one -- which is easy since the algorithm always goes
through them in length order.

\vskip 0.1in \noindent {\bf Multiple Outputs}

\noindent [p 107] It is simply stated that the cost of the 3-ary median
operator is 4, but this hasn't been proven.  On the other hand, (23)
demonstrates the chain: $x_4 = x_1 \oplus x_2$, $x_5 = x_3 \land x_4$,
$x_6 = x_1 \land x_2$, $x_7 = x_5 \lor x_6$.  Note that the discussion
on p99 therefore implies there is a formula of length 4 as well, although
the formula 7.1.1.43 has 5 operators.  Let's see -- does just writing
the above out work?  Sure:
$
\left< x_1 x_2 x_3 \right> = \left(\left(x_1 \oplus x_2\right) \land x_3\right)
\lor \left(x_1 \land x_2\right).
$

\vfil\break

\vskip 0.3in
\centerline{\tt Section 7.1.3: Bitwise Tricks and Techniques}
\vskip0.2in

Most of the stuff in this chapter is covered a little more gently
in {\it Hackers~Delight} by Henry~S.~Warren,~Jr., also using an imaginary RISC machine,
but with more extensive discussion about what to do when certain
operators aren't available.

\vskip 0.1in \noindent {\bf Enriched arithmetic}

\noindent [p 135] Where does (18) come from?  Combining (16) and
(17) we have $\bar x + 1 = \overline{x - 1}$.  Substitute $x-1$ for $x$
to get $\overline{x - 1} + 1 = \overline{x - 2}$, and use the same
formula to convert that to $\bar x + 2 = \overline{x - 2}$.  For any
finite $y$, just repeat $y-2$ more times to get $\overline{x - y} = \bar x + y$.

\vskip 0.08in \noindent [p 135] Left and right shift: what are the floor
relations doing in (19)?  They have to do with what happens when $k$
is negative, and so you can loose 1 bits off of the right.

Note that (19) and (20) are assuming infinite 2-adic numbers; on a real
computer one must be careful even with left shifts if any bits get shifted
close to the left end.  That is, logical left shift is not quite equivalent
to multiplying by powers of 2 on real computers because a) usually
it can't trigger overflow, while straight multiplication usually has
some mechanism for doing so, and b) for signed numbers you can
flip the sign if you shift a 1 into the leftmost position.  For example,
for a signed 64 bit number, $1 \ll 63$ is negative ($-2^{63}$).
Similar complications can ensue with right shifts because one has to
be careful to understand what bit gets inserted at the left.

\vskip 0.1in \noindent {\bf Working with the rightmost bits.}

\noindent [p 140] Why is it called the ruler function?
Because if you want to draw a ruler with $k$ ticks (where $k$ is
a power of 2), then the number of trailing 0s plus 1 is the size of the
tick (with 0 a special case).  Example: make a ruler with 8
divisions. First, draw a large tick (size $m$ where $k = 2^m$).  
Then, in binary, count up:
$\rho\left(\{001, 010, 011, 100, 101, 110, 111\}\right) + 1 = 
\{1, 2, 1, 3, 1, 2, 1\}$.  If you draw this, it looks like, well,
a ruler. 

\vskip 0.08in \noindent [p 140] Why does $\rho\left(x - y\right) = 
\rho\left(x \oplus y \right)$? \hfil\break
Well, $\rho \left(x \, \oplus \, y\right)$ is counting the number of
positions (starting from the right) where $x$ and $y$ agree.
Now think about subtraction -- nothing happens in subtraction
until the first place that the two numbers disagree.  And so
all of the rightmost digits where they do agree will be zero.

\vskip 0.08in \noindent [p 141] Quick computation of $\rho$ in (46).
\hfil\break
Recall that {\tt SADD x,y,z} counts the number of places that
{\tt y} has a 1 while {\tt z} has a 0.  So {\tt SADD rho,(x-1),x}
is computing the sideways addition of (42): $\bar x \, \& \, 
\left(x 0 1\right)$, which is the ruler function.

\vskip 0.08in \noindent [p 141] The $\mu_k$: the point is not 
exactly what integers the $\mu_k$ are (although Knuth being
Knuth, he gives a formula), but what the binary representation
looks like: alternating blocks of $2^k$ 0s and 1s.  They allow
you to select every other something (bit, $\ldots$, byte, $\ldots$).

\vskip 0.08in \noindent [p 141] So, how does the computation
of the ruler (number of trailing zeros) function actually work?  The idea
is that $x\, \& -x$ turns off all bits except the rightmost one --
so it is a power of 2.  So now we just need to figure
out what power of 2 it is.  That is, $x\, \& -x = 2^{\rho x}$.
This is basically a binary search -- see if there are any 1s
in the right half by {\tt AND}ing with $\mu_5$.  If there are not,
add 32 to the number of trailing zeros.  Then see if there are any
1s in the right half of each half, and add 16 if there are, etc.

An example -- but with 8 bits only.  The equivalent to (50)
is: {\tt NEGU y,x}, {\tt AND y,x,y}, {\tt AND q,y,m2}, {\tt ZSZ rho,q,4},
{\tt AND q,y,m1}, {\tt ADD t,rho,2}, {\tt CSZ rho,q,t},
{\tt AND q,y,m0}, {\tt ADD,t,rho,1}, {\tt CSZ rho,q,t},
where here {\tt m2 = \#0f}, {\tt m1=\#33}, and {\tt m0=\#55}.
Say that $x = 96 = \left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$,
so that $\rho\,x = 5$.  The first two instructions result in
$y \gets 96\, \& -96 = \left(0\,0\,1\,0\,0\,0\,0\,0\right)_2$.
Next $q \gets y\, \& \,\mu_2 = 0$ so {\tt ZSN rho,q,4} sets $\rho \gets 4$
(recall that {\tt ZSZ a,b,c} sets {\tt a} to {\tt c} if {\tt b} is 0, and
otherwise sets it to 0).  Next, $q \gets y\, \& \,\mu_1 = 32$, {\tt ADD t,rho,2} 
results in $t \gets 6$, and {\tt CSZ rho,q,t} leaves {\tt rho} unmodified
(recall: {\tt CSZ x,y,z} sets {\tt x} to {\tt z} if {\tt y} is 0, and otherwise
leaves it unmodified).  So, next $q \gets y\, \& \, \mu_0 = 0$,
$t \gets 5$, and the final instruction sets $\rho \gets 5$.

\vskip 0.1in \noindent {\bf Working with the leftmost bits}

\noindent [p 142] Using the same 8-bit example, the equivalent
code is {\tt SRU y,x,4}, {\tt ZSNZ lam,y,4;} {\tt ADD t,lam,2},
{\tt SRU y,x,t}, {\tt CSNZ lam,y,t;} {\tt ADD t,lam,1}, {\tt SRU y,x,t},
{\tt CSNZ lam,y,t}. So, again try $x = 96 = 
\left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$, so $\lambda\,x = 6$ (not 7!).  
Now the first instruction
does $y \gets 96 \gg 4 = 6 = \left(0\, 0\, 0\, 0\, 0\, 1\, \,1 \,0\right)_2$.
This isn't zero, so $\lambda \gets 4$ from {\tt ZSNZ} (zero or set
if not zero).  The next set of instructions does $t \gets 6$,
$y \gets x \gg 6 = 1$, $\lambda \gets 6$.  After that, $t \gets 7$,
$y \gets x \gg 7 = 0$, and on this step $\lambda$ doesn't get changed
because $y = 0$.

\vskip 0.1in \noindent {\bf Sideways addition}

\noindent [p 143] Note that {\tt MMIX} has a sideways add intrinsic
({\tt SADD}) -- which isn't actually that exotic.  x86 has such an intrinsic,
for example.

\vskip 0.1in \noindent {\bf Bit reversal}

\noindent [p 144] 
This is actually pretty easy to understand -- the
first step selects every other bit and reverses them.
The second step does that in blocks of two, etc.
Again, the same example on an 8-bit machine
with $x = 96 = \left(0\,1\,1\,0\,0\,0\,0\,0\right)_2$:
$y \gets \left(x \gg 1\right) \, \& \, \mu_0 = 16$,
$z \gets \left(x \, \& \, \mu_0\right) \ll 1 = 128$,
$x = y | z = 144 = \left(1\,0\,0\,1\,0\,0\,0\,0\right)_2$.
So every other bit has been interchanged.

Okay -- now blocks of two.  Note we are
not starting with the original $x$, but the one
from the previous flips.
$y \gets \left(x \gg 2\right) \, \& \mu_1 = 32$,
$z \gets \left(x \, \& \, \mu_1\right) \ll 2 = 64$.
$x \gets 96$.  Yes, we are back where we started,
but it is the right thing, since 96 is what happens when
you 2-bit flip 144.  

Last step: $y \gets \left(x \gg 4\right) \, \& \mu_2 = 6$,
$z \gets \left(x \, \& \, \mu_2\right) \ll 4 = 0$,
and $x = 6 = \left(\,0\,0\,0\,0\,0\,1\,1\,0\right)_2$,
which is indeed the bit reversal of 96.

\vskip 0.1in \noindent {\bf Bit swapping}

\noindent [p 144] How does (67) work?\hfil\break
Well, $y$ gets the new j$^{th}$ elements with everything
else 0, $z$ gets the new i$^{th}$ element, again with
everything else 0.  Then $x \, \& \, m$ turns off those two
bits, and the or puts the new ones in.

\vskip 0.08in \noindent [p 145] Now, how about (68)?\hfil\break
The first step puts $x_i \, \oplus \, x_j$ into position $j$ will all
other entries 0.  For all positions besides $i$ and $j$, the second
is $x_k \, \oplus \, 0 \, \oplus \, 0 = x_k$, so it doesn't touch those
positions.  In position $i$ we have $x_i \gets x_i \, \oplus \, 0 \, \oplus \,
\left(x_i \, \oplus \, x_j\right)$, since $y \ll \delta$ is a bit vector
with $x_i \, \oplus \, x_j$ in the i$^{th}$ position and 0 elsewhere.
$z \, \oplus \, 0 = z$, $\oplus$ is distributive, so this is
$\left(x_i \, \oplus \, x_i\right) \, \oplus \, x_j$.  And now, 
$z \, \oplus \, z = 0$, so this is $0 \, \oplus \, x_j = x_j$.
Thus, this chain sets $x_i = x_j$.  The same logic applies
to $x_j$, so this does indeed swap the bits.

\vskip 0.08in \noindent [p 150] Notes on (84): $x^{\prime} = 
\left(x - \chi\right) \, \& \, x$\hfil\break
You know it must be non-obvious when Knuth actually explains
the derivation...  One note: the replacement of $\left( \bar \chi + 1\right)$
with $-\chi$ is the well-known `negate all the bits and add one is the same
as sign flipping' rule for 2s complement signed integers.

\vskip 0.08in \noindent [p 150] Subcube generation\hfil\break
Here $a$ has a 1 at the location of each $*$, and $b$ has
a 1 at the location of each actual 1, and 0 for actual 0s or
$*$s.  So, what's going on in this formula?  All that is happening
is that the algorithm is generating all the subsets for $\chi = a$
and then adding the fixed values back in.  The derivation
is the same but starting by considering $x\, | \, \overline{a + b}$,
which now has 1s only at the positions fixed to be 0.  That is,
$a + b = a \, | \, b$, since $a \, \& \, b$ by construction.
Then we select only the wildcard bits, leaving the set bits in $b$
alone, and finally add them back in.

\vskip 0.1in \noindent {\bf Tweaking several bytes at once}\hfil\break

\noindent [p 151] Byte summing: equation~(87).\hfil\break
The first line ($z \gets \left(x \, \oplus \, y\right) \, \& \, h$)
selects the leading bits where either $x$ has a 1 or $y$
has a 1 -- but not both.  These are where carries might
cause problems.  The $0\,0$ case is obviously not a problem,
but neither is $1\,1$ (at least in this step) because the
addition will be modulo 256.  The second line explicitly
masks off the leading bit, then adds it back in with
$\oplus \, z$.  

So -- example: $x = 160 = \left(1\,0\,1\,0\,0\,0\,0\,0\right)_2$,
$y = 100 = \left(0\,1\,1\,0\,0\,1\,0\,0\right)_2$.  Done 
carelessly this would spill over: 
$x + y = 260 = \left(1\,0\,0\,0\,0\,0\,1\,0\,0\right)_2$,
which has 9 bits.  To correct for this problem, the first line
carries out $z \gets \left(x \, \oplus \, y\right) \, \& \, h =
\left(1\,1\,0\,0\,0\,1\,0\,0\right)_2 \, \& \, h =
\left(1\,0\,0\,0\,0\,0\,0\,0\right)$, indicating there is a carry
bit issue.  The second
line then does 
$$\eqalign{z \gets \left(\left(x \, \& \, \bar h\right) \, \oplus
\left(y \, \& \, \bar h\right)\right) \, \oplus \, z &= 
\left( \left(0\,0\,1\,0\,0\,0\,0\,0\right)_2 + \left(0\,1\,1\,0\,0\,1\,0\,0\right)_2
\right) \, \oplus \, z\cr &= \left(1\,0\,0\,0\,0\,1\,0\,0\right)_2 \, \oplus \, z \cr
&= \left(0\,0\,0\,0\,0\,1\,0\,0\right)_2}
$$
which is $260 \bmod 256$.

How about one where there should be a leading 1?
Try $y = 225$, so $x + y \bmod 256 = \left(1\,0\,0\,0\,0\,0\,0\,1\right)_2$.
Now they both have leading 1s, so the first line sets $z \gets 0$,
and the second line does the sum just throwing out the leading bit.

\vfil\break
\topglue 0.5in
\centerline{Notes on Knuth Chapter 7.2}
\vskip 0.5in

\noindent
{\bf Section 7.2.1.2 Algorithm L} The important thing to note here is that
this generates only {\it distinct} permutations -- so 1223 has only 12
permutations, not 24, because in any permutation the two 2s can be
exchanged.

It's useful to create an example of this
in action.  Consider the sequence $246$ -- so $a_1 = 2$, $a_2 = 4$,
$a_3 = 6$, and the convenience value $a_0$ is, say $a_0 = 0$.
So, step by step:\hfil\break
{\bf L1} Output 246.\hfil\break
{\bf L2} Let $j \leftarrow 2$, which does satisfy $a_2 < a_3$.\hfil\break
{\bf L3} Set $l \leftarrow 3$.  We have $a_j < a_l$.  Interchange
  $a_2$ and $a_3$, which gives us $264$.\hfil\break
{\bf L4} is a null step, since $j + 1 = 3$ is the last element.\hfil\break
{\bf L1} Output 264.\hfil\break
{\bf L2} Let $j \leftarrow 2$, but now $a_2 > a_3$, so decrease
  $j$ until $j = 1$ (since $a_1 < a_2$).\hfil\break
{\bf L3} Set $l \leftarrow 3$.  $a_1 < a_2$, so swap to form
 $462$.\hfil\break
{\bf L4} Reverse $a_2$ and $a_3$ to get 426.\hfil\break
{\bf L1} Output 426.\hfil\break
{\bf L2} $j \leftarrow 2$, since $a_2 < a_3$ ($2 < 6$).\hfil\break
{\bf L3} $n \leftarrow 3$, swap to get 462.\hfil\break
{\bf L4} Swapping is null.\hfil\break
{\bf L1} Output 462.\hfil\break
{\bf L2} $j \leftarrow 1$.\hfil\break
{\bf L3} $n \leftarrow 2$, swap to get $642$.\hfil\break
{\bf L4} reverse from 2 to get $624$.\hfil\break
{\bf L1} output 624.\hfil\break
{\bf L2} $j \leftarrow 2$.\hfil\break
{\bf L3} $l \leftarrow 3$.  Swap to get $642$.\hfil\break
{\bf L4} Reversing is null.\hfil\break
{\bf L1} Output 642.\hfil\break
{\bf L2} $j$ is 0, so terminate.\hfil\break
The output is ${246, 264, 426, 462, 624, 642}$, which is in lexicographic
order.

\end