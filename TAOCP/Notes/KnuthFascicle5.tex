\def\newstep#1{\smallskip \noindent {\bf #1}}
\def\newhead#1{\vskip 0.1in \noindent {\bf #1}}
\def\subsec#1{\vskip 0.08in \noindent {\bf #1}}
\def\sec#1 {\vfil\break \centerline{\tt #1} \vskip 0.2in}

\topglue 0.5in
\centerline{Notes on Knuth Fascicle 5: MPR, Backtracking, Dancing Links}
\vskip 0.3in
\centerline{\tt Mathematical Preliminaries Redux}
\vskip 0.2in

\noindent [p 3] Forumula (12): Expectation of expectations: show that
$E\left( E\left(X | Y\right)\right) = E\left(X\right)$.  As explained in the text, $E\left(X|Y\right)$
is a random function -- that is, a map from events $\omega \in \Omega$ to real numbers.   For an 
event $\omega$ it takes the value:
$$
  E\left(X|Y\right)\left(\omega\right) =
    \sum_{\omega^{\prime} \in \Omega} X\left(\omega^{\prime}\right)
      \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
      {Pr\left(\omega^{\prime}\right) \over Pr \left(Y = Y\left(\omega\right)\right)}.
$$
where
$$
  Pr\left(Y = Y\left(\omega\right)\right) = \sum_{\omega^{prime} \in  \Omega}
    Pr \left(\omega^{\prime}\right) \left[ Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
$$
and $[]$ is the indicator function.  For example, if $\Omega = {-2, -1, 0, 1, 2}$, $Pr\left(\omega\right)
= 1/5$, and $Y\left(\omega\right) = \omega^2$, then 
$Pr \left(Y = 1\right) = Pr\left(1\right) + Pr\left(-1\right) = 2/5$.

So, continuing:
$$
\eqalign{
  E\left(E\left(X|Y\right)\right) =&\sum_{\omega \in \Omega} \sum_{\omega^{\prime} \in \Omega}
  X\left(\omega^{\prime}\right)
  \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
  {Pr\left(\omega^{\prime}\right) Pr\left(\omega\right) \over Pr \left(Y = Y\left(\omega\right)\right)} \cr
   =& \sum_{\omega^{\prime} \in \Omega} \left(
     X\left(\omega^{\prime}\right) Pr\left(\omega^{\prime}\right) 
      \sum_{\omega \in \Omega} {\left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
       Pr\left(\omega\right) \over Pr \left(Y = Y\left(\omega\right)\right)} \right).
}
$$
Now, the inner sum is just one, since it is just the definition of $Pr\left(Y = Y\left(\omega\right)\right)$ summed
over all events $\omega$.  So this is just $E\left(E\left(X|Y\right)\right) = \sum_\omega X\left(\omega\right)
Pr \left(\omega\right) = E\left(X\right)$, as claimed.

\noindent {\bf Inequalities}

\noindent [p 5] Formula (23): derivation of the second moment principle.  The
final step again takes advantage of ${\rm E}\left(X | X > 0\right) = {\rm E} \left(X | X > 0\right)
{\rm Pr}\left(X > 0\right) + {\rm E} \left(X | X = 0\right) {\rm Pr}\left(X = 0\right) = {\rm E} \left(X | X > 0\right)
{\rm Pr}\left(X > 0\right)$, and therefore $\left( {\rm E} \left(X | X > 0\right) \right)^2 = 
\left({\rm E} \left(X \right) / {\rm Pr}\left(X > 0\right) \right)^2$.

\noindent {\bf Martingales}

\noindent Something that goes oddly unmentioned, but is used, is that
${\rm E} Z_n = {\rm E} Z_0$ for any martingale.  For example, it gets used in the stopping rule
discussion.  {\it Proof}: We have 
$$
\eqalign{ 
{\rm E}\, Z_n 
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} {\rm E} \left(Z_n | Z_{n-1} \ldots, Z_0 \right)
                {\rm Pr}\left(Z_{n-1}, \ldots, Z_{0} \right)\cr 
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} z_{n-1} {\rm Pr}\left(Z_{n-1}  \ldots, Z_{0} \right) \cr
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} z_{n-1} {\rm Pr}\left(Z_{n-1} | Z_{n-2}, \ldots, Z_0\right)
                 {\rm Pr} \left(Z_{n-2}, \ldots, Z_0 \right) \cr
 &= {\rm E} Z_{n-1}.\cr
}
$$
And we can keep doing this all the way down to ${\rm E}\, Z_0$.

\sec {Basic Backtracking}

\noindent The way that backtracking is implemented does not really square with
modern programming pracitce.  Generally, we would probably want to implement
backtracking as some form of iterator.  Furthermore, the use of gotos would
be frowned upon.  In most cases in Knuth, it's relatively easy to replace gotos
with if statements or for loops, but for Backtracking it is a bit more complicated
because the different paths are not nested, but instead interlock in an interesting
way.

After much trial and error, I hit on the following approach.  The iterator has
three states: New, Ready, and Done.  New and Done are self-describing, Ready
means that the iterator is halted right after visiting a solution, and our goal
is to continue to the next solution.

This is a bit simpler in Walker's version, where we keep track of the available
elements at each level in $S_l$.  When calling next on the iterator, we proceed as follows:

\vskip 0.1in
\noindent {\bf WI1} If the state is New, set $l \leftarrow 0$ and compute $S_0$.
If it is ready, backtrack by setting $l \leftarrow l-1$ and $S_l \leftarrow S_l \ x_l$.
If it is done, return the appropriate value (e.g., None in Rust).
\vskip 0.05in
\noindent {\bf WI2} Loop indefinitely on the below items.
\vskip 0.05in
\noindent {\bf WI3} If $S_l = \emptyset$ then there are no more values to try on
this level. If $l = 0$ then set the state to Done and return the no-more-iterations
value.  Otherwise backtrack by setting $l \leftarrow l-1$ and $S_l \leftarrow S_l \ x_l$
and go back to {\bf WI2}.
\vskip 0.05in
\noindent {\bf WI4} Set $x_l \leftarrow {\rm min} S_l$, $l \leftarrow l + 1$.
\vskip 0.05in
\noindent {\bf WI5} If $l = n$, set the state to Ready and return $x_0 \ldots x_{n-1}$.
\vskip 0.05in
\noindent {\bf WI6} Compute $S_l$ and go back to {\bf WI2}.
\vskip 0.1in

Unfortunately, this isn't as easy to do in the non-Walker approach.  In some
cases this can be avoided, but the general approach is to define some special
value $m_l$ at each level which is not part of ${\cal D}_l$, which is less
than ${\rm min} {\cal D}_l$, and whose successor is the minimum value.
It is not valid in the sense that any evaluation of $P\left(\ldots, m_l, \ldots\right)$ 
is false.  Upon calling next on the iterator, we proceed as follows:

\vskip 0.1in
\noindent {\bf BI1} If the state is New, set $l \leftarrow 0$, $x_0 \leftarrow m_0$.
If the stead is Ready, backtrack by setting $l \leftarrow l - 1$ and downdating
the datastructures.  If the state is Done, return whatever your iterator should
return when it is done (e.g., None in Rust).
\vskip 0.05in
\noindent {\bf BI2} Now repeat the below steps indefinitely.
\noindent {\bf BI3} If $x_l = {\rm max} {\cal D}_l$ then there are no more values
  to try on this level.  If $l = 0$, then set the state to Done and return the
  iteration-is-over value.  Otherwise, backtrack by setting $l \leftarrow l - 1$
  and downdating the datastructures.  Return to {\bf BI2}.
\vskip 0.05in
\noindent {\bf BI4} If $P\left(x_0, \ldots, x_l\right)$ is false, then go to {\bf I6}
  Otherwise set $l \leftarrow l + 1$.  If $l = n$, then set the state to Ready 
  and return $x_0, \ldots, x_{n-1}$.
\vskip 0.05in
\noindent {\bf BI5} Update the data structures for $l + 1$, set $x_l \leftarrow m_l$,
  and go back to {\bf L2}.
\vskip 0.05in
\noindent {\bf BI6} Set $x_l$ to the next element of ${\cal D}_l$ and go back to $I2$.
\vskip 0.1in

\noindent I implemented the N-Queens algorithms, including the optimizations
discussed in the problems, in c++ and timed their performance using
Google Benchmark.  The four methods were a  unoptimized
version, the array version of the basic algorithm~B, the
same version modified to use bit twiddling instead of arrays,
and Walkers version.  The execution time runs in the ratio
22:6.3:6.6:1.  So bit twiddling is not helpful, but, unsurprisingly,
Walkers method is by far the fastest.

\vskip 0.1in
\noindent {\bf Algorithm L}

\noindent In this algorithm, $1 \le l \le 2n$, and $l$ simply indicates the position
we are trying to set.  $k$ is the value we are trying, and $k = p_j$.
$y_l$ is the $j$ that we chose.  $x_i = 0$ means the value is unset,
and $x_i < 0$ means that value was already used earlier.

\vskip 0.1in
\noindent {\bf Optimized Algorithm L}

\noindent It's useful to have the fully optimized Algorithm~L in one place, 
incorporating the improvements from exercises (20) and (21).  

\noindent This uses auxillary arrays $p_0 p_1 \ldots p_n$, $y_1 \ldots y_{2n}$, 
and $a_1 \ldots a_n$.

\noindent {\bf L1.} [Initialize.] Set $x_1 \ldots x_{2 n} \leftarrow 0 
\ldots 0$, $p_k \leftarrow k + 1$ for $0 \leq k < n$, $p_n \leftarrow 0$, 
$a_{1} \ldots a_{n} \leftarrow 0 \ldots 0$, $l \leftarrow 1$,
$n^{\prime} = n - [n \rm{\, is\, even}].$
\vskip 0.05in
\noindent {\bf L2.} [Enter level $l$.] Set $k \gets p_0$.  If $k = 0$, visit
$x_1 \ldots x_{2n}$, optionally visit $x_{2n} \ldots x_1$, and 
go to {\bf L5}.  Otherwise, set $j \leftarrow 0$, and
while $x_l < 0$, go to {\bf L5} if ($l = \lfloor n / 2 \rfloor$ and 
$a_{n^{\prime}} = 0$) or ($l \ge n - 1$ and $a_{2n - l - 1} = 0$), otherwise
set $l \leftarrow l + 1$.
\vskip 0.05in
\noindent {\bf L3.} [Try $x_l = k$.] (At this point we have $k = p_j$).  
If $l + k + 1 > 2n$ goto {\bf L5}.  If $l = \lfloor n / 2 \rfloor$ and 
$a_{n^{\prime}} = 0$, while $k \ne n^{\prime}$ set $j \leftarrow k$, 
$k \leftarrow p_k$.  If $l \ge n - 1$ and $a_{2n - l - 1} = 0$, while 
$l + k + 1 \ne 2 n $ set $j \leftarrow k$, $k \leftarrow p_k$.  If 
$x_{l + k + 1} = 0$, set $x_l \leftarrow k$, $x_{l + k + 1} \leftarrow - k$,
$a_k \leftarrow 1$, $y_l \leftarrow j$, $p_j \leftarrow p_k$, 
$l \leftarrow l + 1$, and return to {\bf L2}.
\vskip 0.05in
\noindent {\bf L4.} [Try again.] (We've found all solutions that begin with 
$x_1 \ldots x_{l-1} k$ or something smaller.) Set $j \leftarrow k$ and 
$k \leftarrow p_j$, then go to {\bf L3} if $k \ne 0$.
\vskip 0.05in
\noindent {\bf L5.} [Backtrack.] Set $l \leftarrow l - 1$.   If $l = 0$ then 
terminate the algorithm. Otherwise do the following: While $x_l < 0$, set 
$l \leftarrow l - 1$.  Then set $k \leftarrow x_l$, $x_l \leftarrow 0$, 
$x_{l + k + 1} \leftarrow 0$, $a_k \leftarrow 0$, $j \leftarrow y_l$, 
$p_j \leftarrow k$.  If $l = \lfloor n / 2 \rfloor$ and $k = n^{\prime}$
goto {\bf L5}, otherwise goto {\bf L4}.

When implemented in c++, the relative timings are 16 $\mu$s and 7.3 $\mu$s 
for $n = 7$ for the raw and optimized Algorithm~L (both visiting both the 
forward and reversed solutions), and 60 ms vs.\ 18 ms for $n = 11$.

\bye

\bye