\def\newstep#1{\smallskip \noindent {\bf #1}}
\def\newitem#1{\vskip 0.06in \noindent [p #1]}
\def\subsec#1{\vskip 0.15in \noindent {\bf #1}}
\def\sec#1 {\vfil\break \centerline{\tt #1} \vskip 0.2in}
\def\CC{\hbox{C++}}

\topglue 0.5in
\centerline{Notes on Knuth Mathematical Preliminaries Redux}
\vskip 0.3in

\noindent [p 3] Forumula (12): Expectation of expectations: show that
$E\left( E\left(X | Y\right)\right) = E\left(X\right)$. \hfil\break 
The more fundamental relation here is the definition of conditional expectation for
any event, (10).  The formulation in terms of random variables is really just a shorthand --
that is $E\left(X | Y=y\right)$ is just shorthand for $E\left(X | A\right)$ where $A$ is the
event that the random variable $Y$ is taking the value $y$.  In general $A$ may therefore
be composed of many (possibly all) atomic events.

As explained in the text, $E\left(X|Y\right)$
is a random function -- that is, a map from events $\omega \in \Omega$ to real numbers.   For an 
event $\omega$ it takes the value (from 13):
$$
  E\left(X|Y\right)\left(\omega\right) =
    \sum_{\omega^{\prime} \in \Omega} X\left(\omega^{\prime}\right)
      \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
      {Pr\left(\omega^{\prime}\right) \over Pr \left(Y = Y\left(\omega\right)\right)}.
$$
where
$$
  Pr\left(Y = Y\left(\omega\right)\right) = \sum_{\omega^{\prime} \in  \Omega}
    Pr \left(\omega^{\prime}\right) \left[ Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
$$
and $[]$ is the indicator function.  For example, if $\Omega = {-2, -1, 0, 1, 2}$, $Pr\left(\omega\right)
= 1/5$, and $Y\left(\omega\right) = \omega^2$, then 
$Pr \left(Y = 1\right) = Pr\left(1\right) + Pr\left(-1\right) = 2/5$.

So, continuing:
$$
\eqalign{
  E\left(E\left(X|Y\right)\right) =&\sum_{\omega \in \Omega} \sum_{\omega^{\prime} \in \Omega}
  X\left(\omega^{\prime}\right)
  \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
  {Pr\left(\omega^{\prime}\right) Pr\left(\omega\right) \over Pr \left(Y = Y\left(\omega\right)\right)} \cr
   =& \sum_{\omega^{\prime} \in \Omega} \left(
     X\left(\omega^{\prime}\right) Pr\left(\omega^{\prime}\right) 
      \sum_{\omega \in \Omega} {\left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
       Pr\left(\omega\right) \over Pr \left(Y = Y\left(\omega\right)\right)} \right).
}
$$
Now, from above, for the inner term we have
$$
\eqalign {
  \sum_{\omega \in \Omega} {
    \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
    Pr \left(\omega\right) \over Pr\left(Y = Y\left(\omega\right)\right)
  } 
  =& 
 \sum_{\omega \in \Omega} {
   \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right] Pr \left(\omega\right) 
   \over 
   \sum_{\omega^{\prime\prime} \in \Omega}
     \left[Y\left(\omega\right) = Y\left(\omega^{\prime\prime}\right)\right]
      Pr\left(\omega^{\prime\prime}\right) 
  } \cr
  =& \, 1.
}
$$
Substituting, we get $E\left(E\left(X|Y\right)\right) = \sum_\omega X\left(\omega\right)
Pr \left(\omega\right) = E\left(X\right)$, as claimed.

\vskip 0.1in
\noindent {\bf Inequalities}
\vskip 0.05in

\noindent [p 4] Jensen's Inequality Examples: note that the example $\ln E\, X \ge E \ln X$
has a $\ge$ because $\ln$ is concave rather than convex.

\newitem {5} Formula (23): derivation of the second moment principle.  The
final step again takes advantage of ${\rm E}\left(X | X > 0\right) = {\rm E} \left(X | X > 0\right)
{\rm Pr}\left(X > 0\right) + {\rm E} \left(X | X = 0\right) {\rm Pr}\left(X = 0\right) = {\rm E} \left(X | X > 0\right)
{\rm Pr}\left(X > 0\right)$, and therefore $\left( {\rm E} \left(X | X > 0\right) \right)^2 = 
\left({\rm E} \left(X \right) / {\rm Pr}\left(X > 0\right) \right)^2$.

\vskip 0.1in
\noindent {\bf Martingales}
\vskip 0.05in

\noindent Something that goes oddly unmentioned, but is used, is that
${\rm E} Z_n = {\rm E} Z_0$ for any martingale.  For example, it gets used in the stopping rule
discussion.  {\it Proof}: We have 
$$
\eqalign{ 
{\rm E}\, Z_n 
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} {\rm E} \left(Z_n | Z_{n-1} \ldots, Z_0 \right)
                {\rm Pr}\left(Z_{n-1}, \ldots, Z_{0} \right)\cr 
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} z_{n-1} {\rm Pr}\left(Z_{n-1}  \ldots, Z_{0} \right) \cr
 &= \sum_{Z_{n-1}, \ldots, Z_{0}} z_{n-1} {\rm Pr}\left(Z_{n-1} | Z_{n-2}, \ldots, Z_0\right)
                 {\rm Pr} \left(Z_{n-2}, \ldots, Z_0 \right) \cr
 &= {\rm E} Z_{n-1}.\cr
}
$$
And we can keep doing this all the way down to ${\rm E}\, Z_0$.

\newitem {9} Special Cases of the Maximmal Inequality \hfil\break
If $Z_n$ is a martingale, then Jensen's inequality 
($f\left(E \, X\right) \le E \, f\left(X\right)$) implies that $\left| Z_n \right|$ and
$Z_n^2$ are non-negative sub-martingales, and so (34) and (35) follow immediately.

\newitem {9} Hoeffding-Azuma Inequality: symmetric bound (38)\hfil\break
We have:
$$
 \eqalign{
  {\rm Pr}\left(Y_1 + \ldots + Y_n \ge x \right) &\le e^{-2x^2 / \left(\left(b_1-a_1\right)^2 
    + \ldots + \left(b_n-a_n\right)^2\right)} \cr
  {\rm Pr}\left(Y_1 + \ldots + Y_n \le -x \right) &\le e^{-2x^2 / \left(\left(b_1-a_1\right)^2 
    + \ldots + \left(b_n-a_n\right)^2\right)}. \cr
 }
$$
and by the Union Bound ${\rm Pr}\left(\left| Y_1 \right| + \ldots + \left| Y_n \right|
\right) \le {\rm Pr}\left(Y_1 + \ldots + Y_n \ge x \right) + 
{\rm Pr}\left(Y_1 + \ldots + Y_n \le -x \right)$, from which (38) follows.

\sec {7.2.2: Basic Backtracking}

\noindent The way that backtracking is implemented does not really square with
modern programming pracitce.  Generally, we would probably want to implement
backtracking as some form of iterator.  Furthermore, the use of gotos is generally
frowned upon.  In most cases in Knuth, it's relatively easy to replace gotos
with if statements or for loops, but for Backtracking it is a bit more complicated
because the different paths are not nested, but instead interlock in an interesting
way.

This is easiest in Walker's approach, where we maintain the available values at
each level.  The iterator has three states: NEW, READY, and DONE.  A new iterator
starts in the NEW state.  The interesting stuff happens when we call Next.
Here I assume something like the Rust model, where we return None if there are no
more solutions, and Some(vector) for a solution.

\vskip 0.1in
\noindent {\bf WI1}\hfil\break 
\phantom{1}\hskip1em If the iterator state is DONE, return None.\hfil\break
\phantom{2}\hskip1em If the iterator state is READY, set $l \leftarrow l - 1.$\hfil\break
\phantom{3}\hskip1em If the iterator state is NEW, set $l \leftarrow 0$, compute $S_0$, and set
the state to READY.\hfil\break
\noindent {\bf WI2} Repeat this step while $S_l = \emptyset$: if $l = 0$ then set 
the state to DONE and return None.  Otherwise, $l \leftarrow l - 1$.
\vskip 0.05in
\noindent {\bf WI3} Set $x_l \leftarrow {\rm min}\, S_l$, $S_l \leftarrow S_l / x_l$, and 
$l \leftarrow l + 1$.
\vskip 0.05in
\noindent {\bf WI4} If $l = n$, then return ${\rm Some}\left(x_0, \ldots, x_{n-1}\right)$.
\vskip 0.05in
\noindent {\bf WI5} Compute $S_l$ and return to {\bf WI2}.
\vskip 0.1in

Unfortunately, this isn't as easy to do in the non-Walker approach.  In some
cases this can be avoided, but the general approach is to define some special
sentinel value $s_l$ at each level which is not part of ${\cal D}_l$, and which is
more than any valid value and which is the sucessor of the maximum valid ${\cal D}_l$.
For example, for Sudoku we could use 10 as a maximum sentinel, for Langford pairs
or for $n$-queens we could use $n+1$, etc.  Rather than using a maximum sentinel value, 
one can use a minimum instead.

A new iterator is created in the NEW state.

\vskip 0.1in
\noindent {\bf BI1}\hfil\break 
\phantom{1}\hskip1em If the iterator state is DONE, return None.\hfil\break
\phantom{2}\hskip1em If the iterator state is READY, set $l \leftarrow l - 1.$\hfil\break
\phantom{3}\hskip1em If the iterator state is NEW, set $l \leftarrow 0$, compute $S_0$, and set
the state to READY.\hfil\break
\noindent {\bf BI2} If $x_l = s_l$ then: if $l == 0$, set the state to DONE and return None.
  Otherwise, backtrack by setting $l \leftarrow l - 1$, downdate the datastructures appropriately,
  set $x_l \leftarrow {\rm next} {\cal D}_l$, and repeat this step.
\vskip 0.05in
\noindent {\bf BI3} If $P\left(x_0, \ldots, x_l\right)$ then set $l \leftarrow l + 1$.  If
  $l = n$ than return ${\rm Some}\left(x_0, \ldots, x_{n-1}\right)$.  Otherwise
  update the datastructures for the new level, set $x_l \leftarrow {\rm min} {\cal D}_l$,
  and return to {\bf BI2}.
\noindent {\bf BI4} Set $x_l \leftarrow {\rm next} {\cal D}_l$ and return to {\bf BI2}.
\vskip 0.1in

\noindent I implemented the N-Queens algorithms, including the optimizations
discussed in the problems, in c++ and timed their performance using
Google Benchmark.  The four methods were a  unoptimized
version, the array version of the basic algorithm~B, the
same version modified to use bit twiddling instead of arrays,
and Walkers version.  The execution time runs in the ratio
22:6.3:6.6:1.  So bit twiddling is not helpful, but, unsurprisingly,
Walkers method is by far the fastest.

\subsec {Algorithm L}

\noindent In this algorithm, $1 \le l \le 2n$, and $l$ simply indicates the position
we are trying to set.  $k$ is the value we are trying, and $k = p_j$.
$y_l$ is the $j$ that we chose.  $x_i = 0$ means the value is unset,
and $x_i < 0$ means that value was already used earlier. 

\subsec { Optimized Algorithm L}

\noindent It's useful to have the fully optimized Algorithm~L in one place, 
incorporating the improvements from exercises (20) and (21).   However, this
is not the Iterator based approach as discussed above.

\noindent This uses auxillary arrays $p_0 p_1 \ldots p_n$, $y_1 \ldots y_{2n}$, 
and $a_1 \ldots a_n$.

\noindent {\bf L1.} [Initialize.] Set $x_1 \ldots x_{2 n} \leftarrow 0 
\ldots 0$, $p_k \leftarrow k + 1$ for $0 \leq k < n$, $p_n \leftarrow 0$, 
$a_{1} \ldots a_{n} \leftarrow 0 \ldots 0$, $l \leftarrow 1$,
$n^{\prime} = n - [n \rm{\, is\, even}].$
\vskip 0.05in
\noindent {\bf L2.} [Enter level $l$.] Set $k \gets p_0$.  If $k = 0$, visit
$x_1 \ldots x_{2n}$, optionally visit $x_{2n} \ldots x_1$, and 
go to {\bf L5}.  Otherwise, set $j \leftarrow 0$, and
while $x_l < 0$, go to {\bf L5} if ($l = \lfloor n / 2 \rfloor$ and 
$a_{n^{\prime}} = 0$) or ($l \ge n - 1$ and $a_{2n - l - 1} = 0$), otherwise
set $l \leftarrow l + 1$.
\vskip 0.05in
\noindent {\bf L3.} [Try $x_l = k$.] (At this point we have $k = p_j$).  
If $l + k + 1 > 2n$ goto {\bf L5}.  If $l = \lfloor n / 2 \rfloor$ and 
$a_{n^{\prime}} = 0$, while $k \ne n^{\prime}$ set $j \leftarrow k$, 
$k \leftarrow p_k$.  If $l \ge n - 1$ and $a_{2n - l - 1} = 0$, while 
$l + k + 1 \ne 2 n $ set $j \leftarrow k$, $k \leftarrow p_k$.  If 
$x_{l + k + 1} = 0$, set $x_l \leftarrow k$, $x_{l + k + 1} \leftarrow - k$,
$a_k \leftarrow 1$, $y_l \leftarrow j$, $p_j \leftarrow p_k$, 
$l \leftarrow l + 1$, and return to {\bf L2}.
\vskip 0.05in
\noindent {\bf L4.} [Try again.] (We've found all solutions that begin with 
$x_1 \ldots x_{l-1} k$ or something smaller.) Set $j \leftarrow k$ and 
$k \leftarrow p_j$, then go to {\bf L3} if $k \ne 0$.
\vskip 0.05in
\noindent {\bf L5.} [Backtrack.] Set $l \leftarrow l - 1$.   If $l = 0$ then 
terminate the algorithm. Otherwise do the following: While $x_l < 0$, set 
$l \leftarrow l - 1$.  Then set $k \leftarrow x_l$, $x_l \leftarrow 0$, 
$x_{l + k + 1} \leftarrow 0$, $a_k \leftarrow 0$, $j \leftarrow y_l$, 
$p_j \leftarrow k$.  If $l = \lfloor n / 2 \rfloor$ and $k = n^{\prime}$
goto {\bf L5}, otherwise goto {\bf L4}.
\vskip 0.1in

\noindent When implemented in c++, the relative timings are 16 $\mu$s and 7.3 $\mu$s 
for $n = 7$ for the raw and optimized Algorithm~L (both visiting both the 
forward and reversed solutions), and 60 ms vs.\ 18 ms for $n = 11$.


\sec{Section 7.2.2.2: Satisfiability}

\newitem {185} Equation~2 is shorter, at 5 clauses, than the `shortest
interesting unsatisfiable formula in 3CNF', 7.1.1-(32), simply because it isn't
in 3CNF -- three of the clauses are in 2CNF.  If we rewrite those clauses in 
the standard fashion with auxillary variables (i.e., $\left(x_1 \vee {\bar x}_2 \right) =
\left(x_1 \vee {\bar x}_2 \vee x_4\right) \wedge \left(x_1 \vee {\bar x}_2 \vee {\bar x}_4\right))$,
then it also has 8 clauses.

\subsec{A simple example}

\newitem {188} Manual solution: note that Knuth is proceeding lexicographically,
which is why he starts with 00, and then extends 001 to 0010011 -- this
is the lexicographically least extension from 001 that doesn't violate
the conditions.  This is because 001000 ends with three equally spaced 0s,
and 0010010 also has three equally spaced 0s.

\newitem {188} $W\left(1, k\right)$ and $W\left(2, k\right)$: $W\left(1,k\right)$
can't contain any 0s, so must consist entirely of 1s.  Thus, the smallest
string that contains $k$ equally spaced 1s is of length $k$.  
Similarly, $W\left(2, k\right)$ clearly can only contain one 0 (because
if it had 2 they would be equally spaced by definition).  If $k$
is odd, the longest possible string without $k$ equally spaced 1s 
is $k-1$ 1s, then a 0, then $k-1$ 1s.  For example, for
$W\left(2, 3\right)$ the longest sequence is 11011, so
$W\left(2, 3\right) = 6$.  But for $k$ even, this doesn't
work, since $1110111$ {\it does} have 4 equally spaced 1s.
So we have to pull one off when it's even, and
$W\left(2, k\right) = 2 k - \left[ k\, {\rm even} \right]$.

\subsec{Exact Covering}

\newitem {189} Langford pairs: recall that the way the exact cover
formulation works is that there is one row for each allowable subset
(so: 1 in positions 1 and 3, 1 in positions 2 and 4, etc.), and one
column for each element.  Each element is 1 if that element
is included in the subset, and 0 otherwise.  So:
$$
 \bordermatrix{
  ~                   & d_1 & d_2 & d_3 & s_1 & s_2 & s_3 & s_4 & s_5 & s_6 \cr
   d_1 s_1 s_3 & 1    & 0     & 0     & 1     & 0     & 1     & 0     & 0     & 0    \cr
   d_1 s_2 s_4 & 1    & 0     & 0     & 0     & 1     & 0     & 1     & 0     & 0    \cr
   d_1 s_3 s_5 & 1    & 0     & 0     & 0     & 0     & 1     & 0     & 1     & 0    \cr
   d_1 s_4 s_6 & 1    & 0     & 0     & 0     & 0     & 0     & 1     & 0     & 1    \cr
   d_2 s_1 s_4 & 0    & 1     & 0     & 1     & 0     & 0     & 1     & 0     & 0    \cr
   d_2 s_2 s_5 & 0    & 1     & 0     & 0     & 1     & 0     & 0     & 1     & 0    \cr
   d_2 s_3 s_6 & 0    & 1     & 0     & 0     & 0     & 1     & 0     & 0     & 1    \cr
   d_3 s_1 s_5 & 0    & 0     & 1     & 1     & 0     & 0     & 0     & 1     & 0    \cr
 } 
$$

\subsec{Backtracking for SAT}

\newitem{212} {\bf Algorithm A}\hfil\break
The use of the doubly linked list is
natural in light of the discussion of Dancing links associated with 
\S 7.2.2.1.  Recall that shrinking any individual
clause to size 0 means the expression is false, while eliminating
all clauses means success.  There are therefore two techniques involved
in setting a literal $l$: first, $\bar l$ is removed from all clauses, and
second all the other variables in a clause containing $l$ are removed
from the linked lists.  The first is because there is no way to satisfy
$\bar l$ once $l$ is set, the second corresponds to removing that clause.

Exploring the linked representation of $R^{\prime}$ given as an example in the book,
1 appears in the clauses 1 and 3, and corresponds to $l = 2$.  Thus, 
${\tt C}\left(2\right) = 2$
(there are two clauses), and ${\tt F}\left(2\right)$ points to elements 30
and then 24, both of which have ${\tt L} = 2$ (meaning the represent
1), and which have ${\tt L} = 1, 3$ for the first and third clause, respectively.
The literals are written in reverse order in the memory block in order to
make removing $\bar l$ easy, which depends on the fact that we iterate
through the literals in order (1, then 2, then 3, etc.).  When we set $l$,
we must have that $\bar l$ is at the front of every clause it appears in, so
all we have to do is reduce the size of the clause by one.  Thus, it's
important that the literals appear in sorted order in each clause.  Row $2 n + 2$
is the boundary between the initial variable entries and the representation
of the clauses.

Note that positions 0 and 1 are not used at all -- so things could be
simplified by starting with $x_0$ rather than $x_1$.

In any case, it's useful to try a simple case -- simpler than the 8 block
unsatisfiable version discussed in the text.  However, we still want something
complicated enough that the algorithm will have to backtrack.
Here's one:  $\{\bar 1 2, \bar 1 \bar 2, 1 \bar 2 3, 123 \}$, which has
the solution 011.  Writing in the same manner:
$$
\matrix{
p =       & 2  &  3 &  4 &  5 &  6  & 7 &  8 &  9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 \cr
{\tt L} = & -  &  - &  - &  - &  -  & - &  6 &  4 &  2 &  6 &  5 &  2 &  5 &  3 &  4 &  3 \cr
{\tt F} = & 13 & 17 & 16 & 14 & 11  & 7 &  6 &  4 &  2 &  8 &  5 & 10 & 12 &  3 &  9 & 15 \cr
{\tt B} = & 10 & 15 &  9 & 12 &  8  & 7 & 11 & 16 & 13 &  6 & 14 &  2 &  5 & 17 &  4 &  3 \cr
{\tt C} = &  2 &  2 &  2 &  2 &  2  & 0 &  4 &  4 &  4 &  3 &  3 &  3 &  2 &  2 &  1 &  1 \cr
}
$$
with ${\tt SIZE} = 2, 2, 3, 3$ and ${\tt START} = 8, 11, 14, 16$.

Here we use the books solution to exercise 121 (which fills out the link
manipulation).
\newstep {A1}: $a \gets 4$ (4 active clauses), $d \gets 1$ (variables up to $x_0$ are set --
that is, none).
\newstep {A2}: $l \gets 2$, and ${\tt C}\left(2\right) = {\tt C}\left(3\right)$ so it is not
  set to 3. (We start by setting $x_1 = 1$).
\newstep {A3}: Remove $\bar 1$ from all clauses.  $p \gets {\tt F}\left(l \oplus 1\right) =
{\tt F}\left(3\right) = 17$ ($p$ points to the first clause containing $\bar 1$).
$j \gets {\tt C}\left(p\right) = {\tt C}\left(17\right) = 1$, $i \gets {\tt SIZE}
\left(1\right) = 2$, which we now reduce by one ({\tt SIZE} = 1, 2, 3, 3).
This removes $\bar 1$ from $C_1$.  Next $p \gets {\tt F}\left(p\right) = 15$,
which is the next clause containing $\bar 1$ ($C_2$).  Repeating, we end up
with ${\tt SIZE} = 1, 1, 3, 3$.
\newstep {A4}: Remove all clauses containing 1 by unlinking all other variables
in those clauses. $p \gets {\tt F}\left(l\right) = {\tt F}\left(2\right) = 13$ ($p$ is
first appearance of $x_1$), $j \gets 3$ (which is in $C_1$), $i \gets
11$ (which starts in position 16), $p \gets {\tt F}\left(p\right) = {\tt F}
\left(13\right) = 10$ (and now $p$
points at the {\it next} appearance, which is in $C_4$). 
Now, for
$11 \le s < 11 + 3 - 1$, or $s = 11, 12$ (we will remove the other variables
$\bar 2,3$ from $C_3$ from all the linked lists), $q \gets {\tt F}\left(11\right) = 8$,
$r \gets 6$, ${\tt B}\left(q\right) = {\tt B}\left(8\right) \gets r = 6$,
${\tt F}\left(r\right) = {\tt F}\left(6\right) \gets q = 8$ (so $3$ is unlinked
for $C_3$).  Repeating for $s = 12$, which is $\bar 2$, we end up
with $q \gets 5$, $r \gets 14$, ${\tt B}\left(5\right) \gets 14$, 
${\tt F}\left(14\right) \gets 5$.  Also we change ${\tt C}\left(6\right) \gets 1$,
and ${\tt C}\left(5\right) \gets 1$.     

Then we have to repeat the same process for $C_4$ starting from $p = 10$.
$j \gets 4$, $i \gets 8$, $p \gets 2$ (which will terminate this process on the
next iteration, since we've done all clauses -- p now points to the literal
part of the array), and $8 \le s < 10$, so $s = 8, 9$ (which are literals $3, 2$).
For $s=8$, $q \gets 6$, $r \gets 6$ (since 3 was already removed from $C_3$),
${\tt B}\left(6\right) \gets 6$, ${\tt F}\left(6\right) \gets 6$, and 
${\tt C}\left(6\right) \gets 0$ (3 has been removed from all clauses).
Then for $s=9$, $q \gets 4$, $r \gets 16$, ${\tt B}\left(4\right) \gets 16$,
${\tt F}\left(16\right) \gets 4$, and ${\tt C}\left(4\right) \gets 1$.

This leaves us in the state:
$$
\matrix{
p =       & 2  &  3 &  4 &  5 &  6  & 7 &  8 &  9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 \cr
{\tt L} = & -  &  - &  - &  - &  -  & - &  6 &  4 &  2 &  6 &  5 &  2 &  5 &  3 &  4 &  3 \cr
{\tt F} = & 13 & 17 & 16 & 14 &  6  & 7 &  6 &  4 &  2 &  8 &  5 & 10 &  5 &  3 &  4 & 15 \cr
{\tt B} = & 10 & 15 &  16 & 14 &  6  & 7 &  6 & 16 & 13 &  6 & 14 &  2 &  5 & 17 &  4 &  3 \cr
{\tt C} = &  2 &   2 &  1 &  1 &  0  & 0 &  4 &  4 &  4 &  3 &  3 &  3 &  2 &  2 &  1 &  1 \cr
}
$$
with ${\tt SIZE} = 1, 1, 3, 3$.  Finally, $a \gets a - {\tt C}\left(2\right) = 2$ and $d \gets
d + 1 = 2$ (we have now set $x_1$).  The remaining equation is 
$\{2, \bar 2\}$, which we can easily see is not satisfiable! But the algorithm
has to discover that for itself.
\newstep {A2}: $l \gets 4$, and ${\tt C}\left(4\right) = {\tt C}\left(5\right)$ so
we leave it there -- we are trying $2$ -- and $m_2 \gets 2$.
\newstep {A3}: Deactivate $\bar 2$ -- which will result in an empty clause.
$p \gets {\tt F}\left(\bar l\right) = 14$, $j \gets 2$ (this is the $\bar 2$ in $C_2$),
$i \gets 1$.  This triggers the abort, which sets $p \gets {\tt B}\left(14\right) = 5$,
but that's less than 8 so we immediately jump to
\newstep {A5}: Try $\bar 2$ instead.  $m_2 \gets 3$ (trying $x_2 = 0$ after $x_2 = 1$
failed), $l \gets 5$.
\newstep {A3}: Deactivate 2, which again results in an empty clause, which again
triggers the quick exit.
\newstep {A5}: $m_2 > 2$, so no action.
\newstep {A6}: Backtrack. $d \gets 1$, and $l \gets 2$ (fixing 1).
\newstep {A7}: Reactivate $1$.  $a \gets 4$, $p \gets {\tt B}\left(l\right) = 10$,
$j \gets {\tt C}\left(p\right) = 4$, $i \gets 8$. $p \gets {\tt B}\left(p\right) = 13$,
and $8 \le s < 10$ (reactivating 2, 3).  For $s = 8$, $q \gets 6$, $r \gets 6$,
${\tt B}\left(q\right) \gets {\tt F}\left(r\right) \gets 8$, or ${\tt F}\left(6\right) \gets 
{\tt F}\left(6\right) \gets 8$, and ${\tt C}\left(6\right) \gets 1$
(the 3 has been reactivated from $C_4$).  Then for $s = 9$,
$q \gets 4$, $r \gets 16$, ${\tt B}\left(4\right) \gets {\tt F}\left(16\right) \gets 9$,
and ${\tt C}\left(4\right) \gets 2$.
\newstep {A8}: Unremove $\bar 1$. $p \gets {\tt F}\left(3\right) = 17$. $j \gets 
{\tt C}\left(17\right) = 1$ ($C_1$), ${\tt SIZE}\left(1\right) \gets 2$, $p \gets 15$.
Then ${\tt SIZE}\left(2\right) \gets 2$.  So we have restored the initial state
at the top of the page.
\newstep {A5}: Now set $m_1 \gets 3$ (trying $\bar 1$ after 1 failed), $l \gets 3$.
\newstep {A3}: Remove $1$ from all clauses ($C_3$ and $C_4$) by setting
${\tt SIZE} = 2, 2, 2, 2$.
\newstep {A4}: Deactivate $\bar 1$s clauses through link manipulation.  That means
removing the links to $2$ from $C_1$ and $\bar 2$ from $C_2$.
The net result is:
$$
\matrix{
p =       & 2  &  3 &  4 &  5 &  6  & 7 &  8 &  9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 \cr
{\tt L} = & -  &  - &  - &  - &  -  & - &  6 &  4 &  2 &  6 &  5 &  2 &  5 &  3 &  4 &  3 \cr
{\tt F} = & 13 & 17 & 9 & 12 & 11  & 7 &  6 &  4 &  2 &  8 &  5 & 10 & 12 &  3 &  9 & 15 \cr
{\tt B} = & 10 & 15 &  9 & 12 &  8  & 7 & 11 & 16 & 13 &  6 & 14 &  2 &  5 & 17 &  4 &  3 \cr
{\tt C} = &  2 &  2 &  1 &  1 &  2  & 0 &  4 &  4 &  4 &  3 &  3 &  3 &  2 &  2 &  1 &  1 \cr
}
$$
and $a \gets 2$, which is ${\bar 2 3, 2 3}$, $d \gets 2$.
\newstep {A2}: $l \gets 4$, $m_2 \gets 0$
(trying 2).
\newstep {A3}: Remove $\bar 2$ from all clauses ($C_3$), which results in
${\tt SIZE} = 2, 2, 1, 2$.
\newstep {A4}: Deactivate the $3$ in $C_4$, and set $a \gets 1$, $d \gets 3$.
The state is now:
$$
\matrix{
p =       & 2  &  3 &  4 &  5 &  6  & 7 &  8 &  9 & 10 & 11 & 12 & 13 & 14 & 15 & 16 & 17 \cr
{\tt L} = & -  &  - &  - &  - &  -  & - &  6 &  4 &  2 &  6 &  5 &  2 &  5 &  3 &  4 &  3 \cr
{\tt F} = & 13 & 17 & 9 & 12 & 11  & 7 &  6 &  4 &  2 &  8 &  5 & 10 & 12 &  3 &  9 & 15 \cr
{\tt B} = & 10 & 15 &  9 & 12 & 11  & 7 & 11 & 16 & 13 &  6 & 14 &  2 &  5 & 17 &  4 &  3 \cr
{\tt C} = &  2 &  2 &  1 &  1 &  1  & 0 &  4 &  4 &  4 &  3 &  3 &  3 &  2 &  2 &  1 &  1 \cr
}
$$
representing $3$.
\newstep {A2}: $l \gets 6$, $m_3 \gets 0$.  And now ${\tt C}\left(6\right) = a$ (the
only active clause has a 3, which is what we are trying).  So we exit with
$m = 300$.  Using $x_j \gets 1 \oplus \left(m_j \& 1\right)$
gives $x = 011$.

\smallskip
\noindent The thing we didn't explore was unit clauses $m = 4, 5$.

\subsec{Lazy Data Structures}

\newitem {215} {\bf Algorithm B} \hfil\break

\noindent The idea with watching is that we are trying
to monitor when a clause becomes unsatisfiable.  So we keep
tabs on one literal, and when it breaks we see if there's another
one we could be watching instead.  If not, then the clause has
become unsatisfiable and we have to backtrack.

Again, it's helpful to explore the data structures
using the same formula: $\{\bar 1 2, \bar 1 \bar 2, 1 \bar 2 3, 123\}$.
The clause setup is very simple -- just list the clauses and literals
in reversed order under the same scheme (where $\bar 2$ is 5, etc.)
$$
\matrix{
p =       & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \cr
{\tt L} = & 6 & 4 & 2 & 6 & 5 & 2 & 5 & 3 & 4 & 3 \cr
}
$$
with ${\tt START} = \{ 10, 8, 6, 3, 0 \}$ (note that 
{\tt START} starts at index 0) so that the last literal
of, say, $C_3 = {\tt START}\left(3-1\right) - 1 = 5$.  
The watched literals are the first in each clause in memory,
so the last one when written out in the original representation:
$2, \bar 2, 3, 3$ respectively for $C_1, C_2, C_3, C_4$.
Thus, in the symbolism we've been using,
$W_4 = 1$, $W_5 = 2$, $W_6 = 4$, with the rest 0.
Furthermore, ${\tt LINK} = \{0, 0, 4, 0\}$ -- note that 
we store only the head of each list $W$, with
the information needed is in {\tt LINK}.
Unlike Algorithm~A, we don't need the literals written in any order
within each clause, although that would change which literals
we watched.

\newstep {B1} $d \gets 1$
\newstep {B2} $m_1 \gets 0$, $l \gets 2$ (trying 1).
\newstep {B3} No literal is watching $2 \oplus 1 = 3$.
\newstep {B4} $W_3 \gets 0$ (which it already was, since $\bar 1$
can't be satisfied), $d \gets 2$.
\newstep {B2} $W_4 = 1$ and $W_5 = 2$, so $m_2 = 1$ (trying $\bar 2$).
  $l \gets 5$.
\newstep {B3} Change the watched literal in any clause watching 2
(which can no longer be satisfied since we are trying $x_2 = 0$).
This will fail on the first clause, which is unsatisfiable with $x=10\star$.
$j \gets W_4 = 1$ ($C_1$ was watching 2, so we need to try to 
see if we can figure out another literal to watch). $i \gets 
{\tt START}\left(1\right) = 8$, $i^{\prime} \gets {\tt START}\left(0\right) = 10$,
$j^{\prime} \gets {\tt LINK}\left(j\right) = 0$ (no other clauses watching 2),
$k \gets 9$ (trying the next element in $C_1$).  We have $k < i^{\prime}$,
so $l^{\prime} \gets {\tt L}\left(k\right) = 3$ (trying the $\bar 1$ in $C_1$).  This
can't be satisfied, since it has been set ($\left| l^{\prime} \right| = 1 < d$)
and $l^{\prime} + m_{\left| l^{\prime} \right|} = 3 + 0$ is odd, so we increment $k$.
But now $k$ has reached $i^{\prime}$, so we have to backtrack by
doing $W_{\bar l} = W_4 \gets j = 1$ (no change -- $C_1$ is still being
watched by 2).
\newstep {B5}: $m_2 = 1 < 2$, so $m_2 \gets 2$ (trying $x_2 = 1$ after
$x_2 = 0$ has failed), $l \gets 4$.
\newstep {B3}: We need to change the watched literal in any clause watching $\bar 2$,
since that can no longer be satisfied.  $j \gets W_5 = 2$ ($C_2$ was watched
by $\bar 2$), $i \gets 6$, $i^{\prime} \gets 8$, $j^{\prime} \gets 0$ (no other
clauses watching $\bar 2$), $k \gets 7$ (trying the next element in $C_2$),
$l^{\prime} \gets {\tt L}\left(7\right) = 3$ (trying the $\bar 1$).  This
is also unsatisfiable, so we do $W_{\bar l} = W_5 \gets j = 2$ (no change).
\newstep {B5}: Now $m_2 = 2$.
\newstep {B6}: (Backtrack) $d \gets 1$ (we are all the way back to $x_1$).
\newstep {B5}: $m_1 = 0 < 2$, so $m_1 \gets 3$, $l \gets 3$, (trying $\bar 1$ after $1$ failed).
\newstep {B3}: Now we need to remove all clauses watching $1$ -- of which there
 are none.
\newstep {B4}: $W_2 \gets 0$ (nobody can watch $\bar 1$ -- but this was already true),
 $d \gets 2$.
\newstep {B2}: $m_2 \gets \left[ W_4 = 0 | W_5 \ne 0 \right] = 1$ (trying $\bar 2$),
$l \gets 5$.
\newstep {B3}: Again, change the watched literal in any clause watching 2.
This time it will work, however. $j \gets W_4 = 1$ ($C_1$ is watching 2).
${\tt START}\left(1\right) = 8$, $i^{\prime} \gets {\tt START}\left(0\right) = 10$,
$j^{\prime} \gets {\tt LINK}\left(j\right) = 0$ (no other clauses watching 2),
$k \gets 9$ (trying the next element in $C_1$).  We have $k < i^{\prime}$,
so $l^{\prime} \gets {\tt L}\left(k\right) = 3$ (trying the $\bar 1$).
But this time $l^{\prime} + m_{\left| l^{\prime} \right|} = l^{\prime} + m_1 = 3 + 3$
is even, and so
${\tt L}\left(i\right) = {\tt L}\left(8\right) \gets l^{\prime} = 3$,
${\tt L}\left(k\right) = {\tt L}\left(9\right) \gets \bar l = 4$ (we've moved the
new watched clause to the end), ${\tt LINK}\left(j\right) = {\tt LINK}\left(1\right)
\gets W_{l^\prime} = W_2 = 0$ (this is the only clause watching 1), 
$W_{l^{\prime}} = W_2 \gets j = 1$ (if there were other clauses watching 1,
these moves would have moved this one to the front but linked to the old
head using {\tt LINK}).  And finally $j \gets j^{\prime} = 0$.  We exit the $k$ loop,
but $j=0$ so we are done (no other clause was watching 2).  The link state is
$$
\matrix{
p =       & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \cr
{\tt L} = & 6 & 4 & 2 & 6 & 5 & 2 & 5 & 3 & 3 & 4 \cr
}
$$
with $W_3 = 1$, $W_5 = 2$, $W_6 = 3$ (and the rest 0), 
${\tt LINK} = \{0, 0, 4, 0 \}$, and $\{ m =  3, 1 \}$ corresponding to
$x = 00$.
\newstep {B4}: Set $W_4 \gets 0$ (no change), $d \gets 3$.
\newstep {B2}: $m_3 \gets \left[W_6 = 0 | W_7 \ne 0\right] = 0$, so $l \gets 6$
 (trying 3).
\newstep {B3}: Nothing is watching $\bar 3$.
\newstep {B4}: $d \gets 4$, $W_7 \gets 0$.
\newstep {B2}: And, success, with $m = 3, 1, 0$, or $x = 001$.

\smallskip \noindent Note this is a different solution than Algorithm~A found, 
but it does work.

\subsec{Forced moves from Unit Clauses}

\newitem {33} {\bf Algorithm~D}\hfil\break
This is only modestly modified from Algorithm~B, although there is quite
a bit more link manipulation required.  There are a few details worth mentioning.
First, The active ring is used to look for unit clauses by storing watched variables
that aren't yet seen.  Why only watched ones?  Well, anything in a unit clause
must be watched, since we watch a literal in every clause. Second
the move codes $m$ are really the same as in Algorithms~A and B.
The only differences are that $m_1$ refers to the variable in $h_1$
rather than always $x_1$, and codes 4 and 5 refer to unit clauses rather
than pure literals. The implementation is lazy in the sense that, for 
example, the way unit clauses are found is to iterate through all the variables in 
each clause looking for unit clauses.  Finally, note that the backtrack step
will unset any unit clause forced literals when it backtracks, unlike the one
in Algorithm~A.

I won't repeat the previous example, since there's a step-through in 
Problem~128 of a different problem, and it's rather tedious.

\subsec{Random Satisfiability}

\newitem {231} $q_m$: The critical point is that the $q_m$ are defined
in relation to distinct clauses where no literal is allowed to repeat.  So
both $11$ and $1 \bar 1$ are disallowed.

\newitem {232} $T_m$.  $T_0$ is 32 for five variables because, of course,
any of the $2^5$ settings will satisfy the case with n clauses.  $T_1 = 28$
because having one clause of 3 variables specifies 3 variables explicitly,
but the other two can take $2^2 = 4$ values, so there are $2^5 - 2^2$
satisfying settings.  For example, $123$ is not satisfied by $00000$,
$00001$, $00010$, and $00011$.  $T_2$ takes a bit more work, but
can be counted directly without too much effort.

\newitem {232} ${\rm E}\,P$.  As noted, this is by definition $\sum m p_m$,
but we also have $p_m = q_{m-1} - q_m$, so
$$
 {\rm E}\,P = \sum m p_m = \sum m q_{m-1} - \sum m q_m =
  \sum q_m + \sum \left(m - 1\right) q_{m - 1} - \sum m q_m = \sum q_m .
$$

\subsec{Resolution}

\newitem {238}: Resolution DAG \hfil\break
The resolution DAG sounds a lot more complicated than it is.
You just start with your clauses as axioms, then combine them with resolution
to form new clauses.  Since each new clause is formed by combining two
other clauses, every non-axiom vertex has in-degree~2.  An important piece is
that the vertex labels are unique.  Note that you aren't required to form all combinations.
What is meant by a `resolution proof of A' is that if the set of clauses is true then A must
be true.

\newitem {239} Exercise~224 is worth doing, but it's also helpful to discuss it
less formally.  Another way to say this is that if $F | \bar{x}$ is false (so we
can prove $\epsilon$) then we must be able to form a resolution proof of $x$ without
using resolution to eliminate it -- meaning that if $F$ is true, $x$ must be true.

\subsec{SAT solving via resolution}

\newitem {244} The key to understanding why the resolution process works
is that {\it all} the products are formed between clauses containing $x$ and $\bar x$.
This is what forces the statement that the setting of the other variables must
satisfy either all the $A$ or all the $A^{\prime}$ (and can satisfy both, of course).
Consider what would happen if this weren't true -- then there would be at least
one $A$ and one $A^{\prime}$ that weren't satisfied.  Call them $A_j$ and $A^{\prime}_k$.
Then the final set of clauses would contain 
$\left(x \vee A_j\right) \diamond \left(\bar x \vee A^{\prime}_k\right) = A_j \vee A^{\prime}_k$,
which is false.

\bye
