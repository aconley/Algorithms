\def\newstep#1{\smallskip \noindent {\bf #1}}
\def\newprob#1{\vskip 0.12in \noindent {\bf #1}}

\topglue 0.5in
\centerline{\tt Mathematical Preliminaries Redux}
\vskip 0.3in


\noindent {\bf Problem 1} {\it Non-transitive dice}\hfil\break
(a) ${\rm Pr}\left(A > B\right) = 2 / 3$ (chance of $A$ rolling a 5) $\times 5 / 6$
(chance that a 5 beats $B$) = 5 / 9.  And ${\rm Pr}\left(B > C\right) = 2 / 6 \times 2 / 6$
($B = 3$) $ + 3 / 6 \times 4 / 6$ ($B = 4$) $+ 1 / 6 \times 4 / 6$ ($B = 6$)
$ = 1 / 9 + 1 / 3 + 1 / 9 = 5 / 9$, ${\rm Pr}\left(C > A\right) = 2 / 6 \times 2 / 6 +
2 / 6 \times 2 / 6 + 2 / 6 = 1 / 9 + 1 / 9 + 1 / 3 = 5 / 9$.\hfil\break
(b) The die should not have any common faces between them or we could make a better
die by making use of any unused ones.  So we want to partition {1, 2, 3, 4, 5, 6} somehow between
three die, also specifying how many times each one appears.  Because we are only trying find
{\it a} solution, not prove it is optimal, we don't need to be that rigorous.

Start by considering the case where one die is all of one value.  It obviously can't be 6, or
that die would alway win.  Say it has value $a \times 6$.  Now, say, that die B also all one
face $b \times 6$.  We must have $a > b$ (and then $P\left(A > B\right) = 1$.   Clearly
die C can't also be all one value, since then we couldn't have B beating C anc C beating A.
So, say that c has two values, with $c_2 > c_1$.  Then we must have $c_2 > a$ and $c_1 < a$.
We must also have $b > c_1$ so that B can beat C.  But then we can't have both 
$P\left(C > A\right) > 1/2$ and $P \left(B > C\right) > 1/2$ because we run out of faces.
Adding a third value to C does not help.

So, if A is pure, B can't be.  So now we consider only one pure die -- say B also has two values
$b_2 > b_1$.  We must have $a > b_2$ so that $P\left(A > B\right) > 0$.  If we also had 
$a > b_1$ then A would also beat B.  We must have $c_1 > a$ so that C can beat B,
but we can't have $c_2 > a$ or else B would lose to C.  So we must have $c_1 > a > c_2$ and $a > b_2$.
If we then have $a > b_1$ then $P\left(C > B\right) \leq N_{c_2}$, $P \left(A > C\right) = N_{c_1}$,
so this doesn't work.  We conclude that having A be pure doesn't seem t owork.

So, try two values per die, with $a_1 > a_2, b_1 > b_2, c_1 > c_2$.  Try A is $a_1, 5 \times a_2$
and C is $k_c c_1, \left(6-k_c\right) \times c_2$, and we have
$$
 P\left(C > A\right) = {k_c \over 36} \left[c_1 > a_1\right] + {5 k_c \over 36} \left[c_1 > a_2\right]
  + {\left(6 - k_c\right) 5 \over 36} \left[c_1 > a_2\right],
$$
so we must have $c_1 > a_2$ to obtain the desired possibilities, and $P\left(C > A\right) = k_c / 6$,
and we need $k_c \geq 5$.  So: A is $1 \times a_1, 5 \times a_2$ and C is $5 \times c_1, 1 \times c_2$.

Now B is $k_b \times b_1, \left(6 - k_b\right) \times b_2$.  We must have $a_1 > b_1$, so 
$$
P\left(A > B\right) = {1 \over 6} + {5 k_b \over 36} \left[a_2 > b_2\right] + {5 \left(6-k_b\right) \over 36}
 \left[a_2 > b_2\right] \geq {1 \over 6} + {5 \left(6 - k_b\right) \over 36} \left[a_2 > b_2\right].
$$
This means that we must have $k_b \leq 3$.

And
$$
 P\left(B > C\right) = {k_b \over 6} \left[b_1 > c_1\right] + {6 - k_b \over 36} \left[b_2 > c_2\right]
  + {\left(6 - k_b\right) 5 \over 36} \left[b_2 > c_1\right].
$$
Since $k_b \leq 3$, we must have $b_2 > c_2$ and $b_1 > c_1$.  This still gives us a lower limit of 
$k_b \geq 3$, so together we have $k_b = 3$.

Putting this all together: A is $1 \times a_1, 5 \times a_2$, B is $3 \times b_1, 3 \times b_2$,
C is $5 \times c_1, 1 \times c_2$ with $a_1 > c_1 > a_2 > c_2$ and $b_1 > c_1$ and $b_2 > c_2$.
This can be done with $A = 6 3 3 3 3 3$, $B=5 5 5 2 2 2$, $C = 4 4 4 4 4 1$.\hfil\break
(c) Clearly we want to try splitting each die into $F_m = F_{m-1} + F_{m-2}$.  Try making
one of the die all one value -- say A is $F_m \times a$, B is $F_{m-2} \times b_1, F_{m-1} \times b_2$.
If $b_1 > a > b_2$, then $P\left(A > B\right) = F_{m-1} / F_m$.  If then C is $F_{m-1} \times c_1,
F_{m-2} \times c_2$ with $c_1 > a > c_2$ then $P\left(C > A\right) = F_{m-1} / F_m$, and
$P\left(B > C\right)$ is complex.

So, this seems to work, but B should be the singleton.  That is: $A = F_{m-1} \times a_1, F_{m-2} \times a_2$,
$B = F_m \times b$, $C = F_{m-1} \times c_1, F_{m-2} \times c_2$, and we can get two of the
probabilities if $c_2 > b > c_1$ and $a_1 > b > a_2$, with 
$P\left(C > A\right) = \left(F_{m-1} F_{m-2} + F_{m-2} F_m\right) / F^2_m$ 
if $c_1 > a_2$ and $c_2 > a_1$.  That is: $c_2 > a_1 > b > c_1 > a_2$, which we can get by 
using $A = F_{m-1} \times 4, F_{m-2} \times 1$, $B = F_m \times 3$, 
$C = F_{m-1} \times 2, F_{m-2} \times 5$.

But does this work? $P\left(C > A\right)$ is complex enough to suggest that it might work,
but needs to be simplified.  Start with the Cassini identity: 
$$
 \eqalign{
    F_{n+1} F_{n-1} - F_n^2 =& \left(-1\right)^n \cr
   \left(F_{n+2} - F_{n}\right)  F_{n-1} - F_n^2 =& \left(-1\right)^n \cr
   \left(F_{n+2} - F_{n}\right)  \left(F_n - F_{n-2}\right) - F_n^2 =& \left(-1\right)^n \cr
   F_{n+2} F_n - F_n^2 - F_{n+2} F_{n-2} + F_n F_{n-2} - F_n^2 =& \left(-1\right)^n \cr
   F_{n+2} \left(F_n - F_{n-2}\right) - 2 F_n^2 + F_n F_{n-2} =& \left(-1\right)^n \cr
   F_{n+2} F_{n-1} - F_n \left(2 F_n - F_{n-2}\right) =& \left(-1\right)^n \cr
   F_{n+2} F_{n-1} - F_n \left(F_n + F_{n-1}\right) =&  \left(-1\right)^n \cr
   F_{n+2} F_{n-1} - F_n F_{n+1} =&  \left(-1\right)^n \cr
   F_{n-1} F_{n+2} =& F_n F_{n+1} + \left(-1\right)^n \cr
   F_{m-2} F_{m+1} =& F_{m-1} F_m - \left(-1\right)^{m-1}
 }
 $$
 which, when substituted into $P\left(C > A\right)$ gives the desired result.

\newprob{Problem 6} {\it Pairwise independence does not imply $k$-wise independence}\hfil\break
Note that these can't be independent, or there would be no way to have a vector
with two set values have non-zero probability but 1 or 3 have zero probability.
Also note that there are $n \choose 2$ ways to have $x_1 + \ldots + x_n = 2$,
so the chance that 2 are set is ${n \choose 2} \times {1 \over \left(n - 1\right)^2} =
{n \left(n - 1\right) \over 2} \times {1 \over \left(n - 1\right)^2} = {n \over 2 \left(n - 1\right)}$.
Combined with the chance that zero are set ($\left(n - 2\right) \over \left(2 n - 2\right)$),
the total probability is one.

In any case, we want to compute the probability that $X_i = 1$.  Given $X_i = 1$, there are $n - 1$
ways to set $X_j = 1$ for $i \neq j$, all of equal probability, and all other settings have zero probability.
Therefore, the probability that $X_i = 1$ must be $1 / \left(n - 1\right)$ so that the sum of
all those arrangements adds up to the right value.  What about $\left(X_i, X_j\right) = \left(0, 1\right)$ 
for $i \neq j$? Well, there are $n-2$ other choices for $X_k = 1$, so the probability
must be $n - 2 \over \left(n - 1\right)^2$, and we must have the same probability
for $\left(X_i, X_j\right) = \left(1, 0\right)$.  We already know that the chances of
$\left(X_i, X_j\right) = \left(1, 1\right) = {1 \over \left(n - 1\right)^2}$ for $i \neq j$,
so by subtraction we must have the chances of them both being 0 of
$\left(n - 2\right)^2 \over \left(n - 1\right)^2$.

Now note that these can be written as the probability that ${\rm Pr} \left(X_i, X_j\right) = 
\left(0, 0\right), \left(0, 1\right), \left(1, 0\right), \left(1, 1\right) =
p_0^2, p_0 p_1, p_1 p_0, p_1^2$ for $p_0 = {\left( n - 2\right) \over \left(n - 1\right)}$,
$p_1 = {1 \over n - 1}$, which constitutes 2-wise independence.
But we can't have $\left(X_i, X_j, X_k\right) = \left(0, 0, 0\right)$ for any distinct
$i, j, k$, and the same is true for any combination of 4 or more.  So they are 2-wise
independent, but no more.

\newprob {Problem 10} {\it k-wise independence modulo $p$}\hfil\break
Note a similar (but simpler) version is discussed as example~15.1.1 of Mitzenmacher and Upfal.
The basic idea is to show that $P\left(X_1 = a_1, \dots, X_k = a_k\right) = 1 / p^k$, and if it turns
out that there is a unique map from the $Y$s to the $X$s, this will be true because the $Y$s are
independent.

First, note that if $n > p$ then $X_{p+1} = \left( \left(p+1\right)^m + Y_1 \left(p+1\right)^{m-1}
 + \ldots + Y_m\right) \bmod p =X_1$, so they are definitely not unique.  Why?  Well, because
 $\left(p + 1\right)^k \bmod p = \left[ \sum_q {m \choose q} p^k  \right] = {m \choose 0} \bmod p = 
 1 \bmod p$.  So therefore there is no independnece unless $n \leq p$.  Now, the 1 terms (that don't
 involve the $Y$s) are just constants we can ignore.

 As long as the vectors $1, j, \ldots, j^{m-1}$ are independent, then the solution is unique, otherwise
 there would be multiple ways to formulate the solution.  Note, however, that the vectors don't have
 to span the space, so for a given set of values of the $Y$s not all $X$s are possible!  And, indeed,
 these vectors are unique modulo $p$ as long as there are less than $m$ of them, so this formulation
 gives $m-1$-wise independence.

\newprob {Problem 13} {\it Does $P\left(A|B\right) > P\left(A\right)$,
$P\left(B | C\right) > P\left(B\right)$ imply $P\left(A|C\right) > P\left(A\right)$?}\hfil\break
Writing it out in terms of the definition does not suggest any way of demonstrating
this, but is there a counterexample?  Yes, a particularly simple one is 
$\Omega = {1, 2, 3, 4, 5}$, $P\left(i\right) = 1/5$ for $i \in \Omega$, and
$A = {1, 2, 3}$, $B = {2, 3, 4}$, $C = {3, 4, 5}$.  For then
$ P\left(A\right) = P\left(B\right) = P\left(C\right) = 3 / 5$
and
$$
 \eqalign{
  P\left(A | B\right) = {P\left({2, 3}\right) \over P\left(B\right)} = {2 \over 3} >& P\left(B\right) \cr
  P\left(B | C\right) = {P\left({3, 4}\right) \over P\left(C\right)} = {2 \over 3} >& P\left(C\right) \cr
 P\left(A | C\right) = {P\left({3}\right) \over P\left(C\right)} = {1 \over 3} <& P\left(C\right) \cr
 }.
$$

\newprob {Problem 14} {\it Prove the chain rule for conditional probability.}\hfil\break
That is:
$$
  P\left(A_1 \cap \ldots \cap A_n\right) = P\left(A_1\right) P\left(A_2 | A_1\right)
   \ldots P\left(A_n | A_1 \ldots A_{n-1} \right).
$$

Proceed via induction.  First, the base case 
$P\left(A_1 \cap A_2\right) = P\left(A_2 | A_1\right) P\left(A_1\right)$
is true by the definition of conditional probability (MPR.4).  So now assume we have
the above relation and want to demonstrate it for $n+1$.  If we denote $A_1 \cap A_n = An$,
then this is simply $P\left(An\right) = P\left(A_1\right) \ldots P\left(A_n | A_1 \ldots A_{n-1}\right)$,
and we have (from the definition) that 
$P\left(A_{n+1} \cap An\right) = P\left(A_{n+1} | An\right) P\left(An\right)$
which is just the desired formula.

\newprob {Problem 15} {\it Is $P\left(A | B \cap C\right) P\left(B | C\right) = P\left(A \cap B | C\right)$?}\hfil\break
Writing this out:
$$
P\left(A | B \cap C\right) P\left(B | C\right)  = {P\left(A \cap B \cap C\right) \over P\left(B \cap C\right)}
   {P\left(B \cap C\right) \over P\left(C\right)} = {P\left(A \cap B \cap C\right) \over P\left(C\right)} =
    P\left(A \cap B | C\right).
$$
So this is true {\it as long} as we avoid the special case $P\left(C\right) = 0$ 
(and then it's true if $A$ and $B$ are independent).

\newprob {Problem 17} {\it Evaluate $P\left(\hbox{\rm T is ace} | \hbox{\rm B is Q of spades}\right)$}. 
\hfil \break
Four of the remaining 51 cards are aces, so this is just $4/51$.

\newprob {Problem 18} {\it Prove that $\hbox{\rm Var}\left[x\right] 
\leq \left(M - E\,X\right) \left(E\,X - m\right)$}.\hfil\break
Here $M$ is the maximum value and $m$ the minimum.  Denoting the quantity
as $Q$, we are trying to prove $\hbox{\rm Var}\left[x\right] \leq Q$, which is 
equivalent to  $Q - \hbox{\rm Var}\left[x\right] \geq 0$.
$$
\eqalign{
  \hbox{\rm Var}\left[x\right] =& E\left(X^2\right) - \left(E\,X\right)^2 \cr
  Q =& \left(M + m\right) E\,X - \left(E\,X\right)^2 - m\,M \cr
  Q - \hbox{\rm Var}\left[x\right] =& \left(M+m\right) E\,X - E\left(X^2\right) -m\,M .
}
$$
Is the latter quantity positive?  Well, $\left(M - X\right) \left(X - m\right) \geq 0$, so
it's expectation must also be positive.
$$
\eqalign{
  E\left(M - X\right)\left(X - m\right) & \geq \cr
  E\left(\left(M + m\right) X - X^2 -m\,M\right) & \geq 0 \cr
  \left(M + m\right) E\, X - E\left(X^2\right) \geq 0 
}
$$
which is the desired proof.

\newprob {Problem 20} {\it Prove the Union rule: $P\left(A_1 
\cup \ldots \cup A_n\right) \leq P\left(A_1\right) + \ldots P\left(A_n\right).$}\hfil\break
There are multiple ways to show this, but doing it with equation~8:
$$
\eqalign{
  P\left(A_1 \cup \ldots \cup A_n \right) &= 
           E\left( \left[A_1 \cup \ldots \cup A_n \right] \right) \cr 
    &= E\, \hbox{\rm max}\left( \left[A_1\right], \ldots, \left[A_n \right] \right) \cr
    &\leq E\left(\left[A_1\right] + \ldots + \left[A_n \right]\right) \cr
    &= E\left[A_1\right] + \ldots + E\left[A_n\right] \cr
    &= P\left(A_1\right) + \ldots + P\left(A_n\right)
}
$$
where the last step follows by linearity of expectations.

\newprob {Problem 35} {\it True or false for conditional expectations.}\hfil\break
(a) $E\left( E\left(X | Y\right) | Y \right) = E\left(X | Y\right)$.  First, notice that
$E\left(X|Y\right)$ is just a function of $Y$ -- that is, a random variable which
takes the value $\sum_x \, x Pr\left(X=x| Y = y\right)$ when $Y=y$.  Or to write
it out more fully
$$
E\left(X|Y\right)\left(\omega\right) =
    \sum_{\omega^{\prime} \in \Omega} X\left(\omega^{\prime}\right)
      \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
      {Pr\left(\omega^{\prime}\right) \over Pr \left(Y = Y\left(\omega\right)\right)}.
$$
Thus
$$
\eqalign{
   E\left( E\left(X | Y\right) | Y \right) \left(\omega\right)  &=
     \sum_{\omega^{\prime}} \sum_{\omega^{\prime \prime}}
       X\left(\omega^{\prime\prime}\right)
       \left[Y\left(\omega^{\prime\prime}\right) = Y\left(\omega^{\prime}\right)\right]
       {Pr\left(\omega^{\prime\prime}\right) \over Pr\left(Y = Y\left(\omega^{\prime}\right)\right)}
       \left[Y\left(\omega^{\prime}\right) = Y\left(\omega\right)\right]
       {Pr\left(\omega^{\prime}\right) \over Pr\left(Y = Y\left(\omega\right)\right)} \cr
   &=  \sum_{\omega^{\prime \prime}}
     \left(
     X\left(\omega^{\prime\prime}\right)
     \left[Y\left(\omega^{\prime\prime}\right) = Y\left(\omega\right) \right]
     {Pr\left(\omega^{\prime\prime}\right) \over Pr\left(Y=Y\left(\omega\right)\right)}
     \sum_{\omega^{\prime}} 
      { \left[Y\left(\omega\right) = Y\left(\omega^{\prime}\right)\right]
         Pr\left(\omega^{\prime}\right) 
         \over Pr \left(Y = Y\left(\omega^{\prime}\right)\right) }.
     \right)
}
$$
where we have swapped the equalities in the indicator functions because they must all be equal for a term to count.
The last sum is 1 by the definition of $Pr\left(Y = Y\left(\omega\right)\right)$ (see the Notes for this portion),
which leaves us with $E\left(X | Y\right) \left(\omega\right)$, as claimed.\hfil\break
(b) $E\left(E\left(X|Y\right)|Z\right) = E\left(X|Z\right)$.  This one is not true.  We now have
$E\left(E\left(X|Y\right)|Z\right) = \sum_y f\left(Y, Z\right) Pr\left(Z=z\right)$, but that isn't
going to be $\sum_x x Pr\left(Z=z\right)$ in general.  In contrast, consider
$$
\eqalign{
  E\left(E\left(X|Y, Z\right)|Z\right) &= 
    \sum_{y^{\prime}, x} Pr\left(X=x|Y=y^{\prime}, Z=z\right) Pr\left(Y=y^{\prime}| Z = z\right) \cr
    &= \sum_x Pr\left(X=x|Z=z\right) \cr 
    &= E\left(X|Z\right)
}.
$$

\newprob{Problem 36} {\it Simplify conditional expectations}\hfil\break
(a) Simplify $E\left(f\left(X\right) | X\right).$  This is simply $f\left(X\right)$\hfil\break
(b) Simplify $E \left( f\left(Y\right) E\left(g\left(X\right) | Y\right) \right)$.  Recall that
the conditional expectation is just a random variable.
$$
\eqalign{
E \left( f\left(Y\right) E\left(g\left(X\right) | Y\right) \right)
 &= \sum_y f\left(y\right) E\left(g\left(X\right) | Y\right) Pr\left(Y=y\right)\cr
 &= \sum_{x, y}f\left(y\right) g\left(x\right) Pr\left(X=x | Y=y\right) Pr\left(Y=y\right) \cr
 &= \sum_{x, y} f\left(y\right) g\left(x\right) 
   {Pr\left(X=x , Y=y\right) \over Pr\left(Y=y\right)} Pr\left(Y=y\right) \cr
  &= \sum_{x, y} f\left(y\right) g\left(x\right) Pr\left(X=x, Y=y\right) \cr
  &= E\left( f\left(Y\right) g\left(X\right)\right)
}
$$

\newprob{Problem 46} {\it Explain why ${\rm E}\left(X^2 | X > 0\right) \geq 
\left({\rm E} \left(X | X > 0\right)\right)^2$}\hfil\break
There are (at least) two ways to look at this, and neither of them depends on the $X > 0$
part.  So I'll drop that.  First, there's the usual ${\rm E}\left( \left(X - {\rm E} \, X\right)^2 \right) =
{\rm E} \, X^2 - \left({\rm E}\, X\right)^2$ where the square is manifestly non-negative.
Second, $X^2$ is convex so we can simply use Jensen's inequality.

\topglue 0.5in
\centerline{\tt Knuth 7.2: Basic Backtracking}
\vskip 0.3in

\noindent {\bf Problem~1} {\it Generating Combinatoric Quantities using 
Backtracking}\hfil\break
(a) $n$-tuples: $D_k$ is whatever the domain is and $P_l$ is always 
true.\hfil\break 
(b) Permutations: $D_k = \{ 1, \ldots, n \}$ and $P_l$ is that all the elements 
are distinct.\hfil\break
(c) Combinations: $D_k = \{1, \ldots, N + 1 - k \}$ and 
$P_l = x_1 < \ldots < x_l$.\hfil\break

\newprob {Problem~2} {\it Making $P_1$ always true}\hfil\break
Simply discard any elements of $D_1$ that do not satisfy $P_1$.

\newprob {Problem~3} {\it Saving half the work for $n$-queens}\hfil\break
Restrict $D_1$ to be only the first half of the range, and then add the 
reflected solution $n + 1 - x_1, \ldots, n + 1 - x_n$.  For example, if $n = 8$
then $D_1 = \{1, 2, 3, 4\}$.

\newprob {Problem~4} {\it Recursive backtracking}\hfil\break
{\bf I1.} [Initialize] Set $l \leftarrow 0$ and initialize other data 
structures.\hfil\break
{\bf I2.} [Test $P_l$] If $P_l\left(x_1 \ldots x_l\right)$ set 
 $l \leftarrow l + 1$ and $x_l \leftarrow {\rm min}\,D_l$,
  otherwise goto {\bf I4}.\hfil\break 
{\bf I3.} [Visit] If $l > n$ visit $x_1 \ldots x_n$, set $l \leftarrow l-1$.
\hfil\break
{\bf I4.} [Iterate] Set $x_l$ to the next larger element in $D_l$ and goto 
 {\bf I2}. If there is no such element set $l \leftarrow l - 1$.\hfil\break
{\bf I5.} [Done] If $l=0$ halt, otherwise goto {\bf I2}.
\hfil\break

So, why not use something like this?  Really, the answer is because Don Knuth
is an assembly programmer, and that's the way he thinks.  One could argue his
algorithm is faster, which could be critical in large problems, but tests show
the difference is small when coded in a higher level language -- in fact this
way is sometimes faster.

\newprob {Problem~5} {\it NQueens skipping one row.}\hfil\break
For $n=8$, skipping row $0, 1, \ldots, 7$ gives ${312, 396,
430, 458, 458, 430, 396, 312}$ solutions.

\newprob {Problem~9} {\it Bitwise N-Queens}\hfil\break
Obviously we will represent the current state of the $A_l, B_l, C_l$ 
as integers $a_l, b_l, c_l$, one for each level, and where, for example, 
$a_l\left[i\right] = 1$ if and only if $x_{i-1}$ is present in $A_l$.  
Similarly, $S_l$ will be represented by integers $s_l$.  Let 
$m = \left(1 \ll {n-1}\right)$, which is a bitmask on the available elements
for $x$.

Then in {\bf W2}, to compute the available values,
$s_l \leftarrow m \,\&\, {\bar a_{l-1}} \,\&\, {\bar b_{l-1}} \,\&\, 
{\bar c_{l-1}}$. We compute the bitwise representation of the current choice 
for $x$ to chose as $x \leftarrow s_l \& \left(- s_l\right)$, then 
$a_l \leftarrow a_{l-1} + t$, $b_l \leftarrow \left(b_{l-1} + t\right) \gg 1$, 
and  $c_l \leftarrow \left( {c_{l-1} + t} \ll 1 \right) \,\&\, m$.  The shifts 
are necessary for $b, c$ because $l$ has increased.  In {\bf W4} we
remove that choice by doing $s_l \leftarrow s_l - t$, although since we keep
all the $s_l$, we can do it in W2 if desired.

\newprob {Problem 15} {\it Hexwise N-Queens}\hfil\break
This is like N-Queens except we only need to check one diagonal.  This turns out
to be like ignoring $b$ from Problem~9.

Then we have $n=1: 1, 2: 1, 3: 3, 4: 7, 5: 23, 6: 83, 7: 405, 8: 2113, 9: 12657,
10: 82297$.

\newprob {Problem 20} {\it Forced move in Langford.}\hfil\break
We need a new array $a$ where $a_i$ means that $i$ has already appeared.
We initially set it to all 0s, and when we chose the value in {\bf L3}
we set $a_k \leftarrow 1$.  Similarly, when we undo in $L_5$ we set
$a_k \leftarrow 0$.

The more interesting changes have to do with forcing a value.  First,
we don't need to do any checks if $l < n - 1$ since nothing can be forced
before that.  We force moves in {\bf L3}, after we determine we are not
off the end (that is, after the $l + k + 1 \le 2 n$ check), we check to see 
if the forced value has already been used.  If it has, continue, if not
then we have to force it by walking the list forward until it is selected.
So: if $l \ge n - 1$ and $a_{2n - l - 1} = 0$, while $l + k + 1 \ne 2 n$
set $j \leftarrow k$, $k \leftarrow p_k$.  We are guaranteed this will
terminate because we know it is available by checking $a$.

But we can also modify $l$ by walking over used values in {\bf L2}, which
may lead us to realize we are out of room.  So, in {\bf L2} while looping
forward over negative values, we immediately backtrack (jump to {\bf L5}).
So: while $x_l < 0$, if $l \ge n - 1$ and $a_{2 n - l - 1} = 0$ goto {\bf L5},
otherwise set $l \leftarrow l + 1$.


\topglue 0.5in
\centerline{\tt Knuth 7.2.1: Dancing Links}
\vskip 0.3in

\noindent {\bf Problem 3} {\it Solving exact cover problems using
linear algebra}\hfil\break
(a) Together $x_2 + x_4 = 1$ and $x_2 + x_4 + x_6 = 1$
imply that $x_6 = 0$.  Combined that with $x_1 + x_6 = 1$
gives $x_1 = 1$.  In turn with $x_1 + x_3 = 1$ this implies
$x_3 = 0$, then with $x_3 + x_5 = 1$ we have $x_5 = 1$.
Then $x_3 + x_4 = 1 \rightarrow x_4 = 1$ and finally
$x_2 + x_4 = 1 \rightarrow x_2 = 0$.  Altogether
$\vec{x} = [1, 0, 0, 1, 1, 0]$.\hfil\break
(b) Because in general there will be many, many more
rows than columns (e.g., $m \gg n$), and so we the solution
will not be uniquely determined.

\newprob {Problem~4} {\it Exact cover on graph.}\hfil\break
You are chosing edges such that each vertex has exactly one incident edge.
This has various interpretations depending on the graph.  If it is bipartite, for
example, each solution choses the vertices of one part.

\newprob {Problem~6} {\it Size of data structures in Algorithm~D}\hfil\break
The items occupy $N+1$ locaions of size 2 + the space to represent the names.
Then we have $N$ header nodes plus $M+1$ spacers plus the $L$ options,
each taking 3 elements, so $3\left(L + N + M + 1\right)$ locations.

\newprob {Problem~7} {\it Explain specific values in Table~1}\hfil\break
(a) {\tt TOP(23) = -4} because it is a spacer following option 4 (this is not
really part of the algorithm; all that matters is that it is negative.\hfil\break
(b) {\tt DLINK(23) = 25} because that is the last element of the next option
(option 5).

\newprob {Problem~18} {\it MRV Heuristic}\hfil\break
Simply walk the list of active items and keep track of the minimum encountered
so far.\hfil\break
{\bf M1.} [Initialize] If $RLINK\left(0\right) = 0$, terminate unsuccessfully (there are no
 active items).  Otherwise, set $m \leftarrow \infty$, $p \leftarrow 0$ ($m$ will hold
 the current minimum).\hfil\break
{\bf M2.} [Walk forward] Set $p \leftarrow RLINK\left(p\right)$.\hfil\break
{\bf M3.} [Done?] If $p = 0$ goto {\bf M6}.\hfil\break
{\bf M4.} [Compare] Set $l \leftarrow LEN\left(p\right)$.  If $l < m$ set $m \leftarrow l$,
 $i \leftarrow p$.\hfil\break
{\bf M5.} [Early terminate] If $m \ne 0$, goto {\bf M2}.\hfil\break
{\bf M6.} [Exit] Return $i$.\hfil\break

\bye