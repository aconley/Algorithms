\topglue 0.5in
\centerline{\tt Knuth 7}
\vskip 0.3in

\noindent
{\bf Problem 2} We can start with any solution to the normal problem (where we count
from 1 to $n$ instead of 0 to $n-1$ and simply
append 00 on the front.  Therefore, we must have $n-1 = 4m - 1$ or $n-1 = 4m$ --
that is, $n \bmod 4$ is 0 or 1.

\vskip 0.1in
\noindent
{\bf Problem 5} There are $2 n \choose 2$ pairs of positions (since we don't care about 
the ordering of each pair.  Of those, $2 n - k - 1$ satisfy the condition (the first member of the 
pair can be in position 1 up to $2 n - k - 1$ before the second one runs off the end).
A guess as to the number would be to falsely assume the probabilities are independent --
which is clearly nonsense.  The total number of arrangements is $2 n \choose 2, \ldots, 2$,
so the product  ${2 n \choose 2, \ldots, 2} \prod_{j=1}^n \left(2 n - k - 1\right) / {2 n \choose n}$
is an estimate.

\vskip 0.1in
\noindent
{\bf Problem 7} The number of uncompleted pairs $u_i$ must start at 0 at the left of
the sequence (before processing the first number) and end at 0 when we finish.
After each number it can either increase by one or decrease by one.  The longest
sequence we can imagine where we never exceed, 6 is
$\{0, 1, \ldots, 5, 6, 5, 6, \ldots, 2, 1, 0\}$.

How can we make use of this information?  Well, one thing we know about Langford
pairs is that each term $k$ contributes a count of 1 to $u$ for $k$ spaces -- that
is, after we process, say, 4, we know that $u$ will be increased by 1 for 4 more spaces
until we close out the pair -- that is, it increases the sum over $u$ by $k+1$.
Thus, $\sum_{i=1}^{2 n} u_i = \sum_{i=1}^{k} k + 1 = {n+1 \choose 2} + n$.  The
other sum, from the above sequence, is, for $u_i$ never exceeding 6,
$11 n - 30$.  So we need $11  n - 30 \ge {n + 1 \choose 2} + n$, or $n \le 15$,
and it doesn't work.

\vskip 0.1in
\noindent
{\bf Problem 39} $G \setminus e$ is always spanning (includes all the vertices of $G$),
but not induced (contains all the edges).

\vskip 0.1in
\noindent
{\bf Problem 40} (a) A spanning subgraph has all $n$ vertices.  So the only
question is which subset of the $e$ edges are present.  There are $2^e$ 
possibilities. (b) Now the question is only which subset of the vertices
are included; there are $2^n$ possibilities (assuming we allow the empty
subgraph).

\vskip 0.1in
\noindent
{\bf Problem 41} (a) 1 and 2. (b) 0 and 3.

\vskip 0.1in
\noindent
{\bf Problem 55} Two different ways to write this: $n_1 \times n_2 +
n_1 \times n_3 + \ldots + n_1 \times n_k + n_2 \times n_3 + \ldots + 
n_{k-1} \times n_k$.  Or, alternatively, start with the complete graph
and subtract all the edges between vertices in the same components:
${N \choose 2} - {n_1 \choose 2} - {n_2 \choose 2} - \ldots - {n_k \choose 2}$
(where $N = \sum_i n_i$).

\vskip 0.1in
\noindent
{\bf Problem 56} True.  Simple means that there are not multiple arcs between
the same vertices in the digraph.  That rules out self edges, since converting
a multigraph to a digraph would add two arcs for each self loop in the multigraph.
Therefore, the multigraph can't have any self loops, and is a graph.

\vskip 0.1in
\noindent
{\bf Problem 57} This would be true if the question asked about strongly
connected components, but it doesn't.  So false.  A trivial example is to
have two of three vertices point into the third vertex.

\topglue 0.5in
\centerline{\tt Knuth 7.1.1}
\vskip 0.3in

\noindent
{\bf Problem 3} (a) max corresponds to logical or: $\vee$ (b) min corresponds to 
logical and: $\wedge$ (c) $-x$ is negation or right complementation: $\overline{x}$
(d) $x \cdot y$ is equivalence: $x \equiv y$.

\vskip 0.1in
\noindent
{\bf Problem 4} (a) This is straightforward but extremely tedious -- that is,
it would be a good thing to write a program to do.  In any case, the idea
is to start with the truth tables of the left and right projection operators
$x$ and $y$ (0011 and 0101, respectively), and NAND $\bar \land$ (1110)
and try successively combining them.  For example, $\bar L = L \bar \land L = x \bar
\land x$, $\bar R = R \bar \land R = y \bar \land y$.  Then try combinations:
$L \bar \land \bar L = 0011 \bar \land 1100 = 1111 = T = x \bar \land x \bar \land x$.
Continuing, $\vee = \bar L \bar \land \bar R = 1100 \bar \land 1010 = 0111 =
\left(x \bar \land x\right) \bar \land \left(y \bar \land y\right)$,
$\land = \bar \land \bar \land \bar \land = 1110 \bar \land 1110 = 0001 = 
\left(x \bar \land y \right) \bar \land \left( x \bar \land y \right)$,
$\supset = x \bar \land \bar \land = 0011 \bar \land 1110 = 1101 = 
x \bar \land \left(x \bar \land y\right)$, $\subset = \bar \land \bar \land =
y \bar \land \left(x \bar \land y\right)$.
The toughest ones are $\oplus = \supset \bar \land \subset = 1011 \bar \land 1101 =
0110 = y \bar \land \left(x \bar \land x\right) \bar \land \left(x \bar \land
\left(x \bar \land y \right)\right)$, $\perp$, and $\bar \subset, \bar \supset$.
The latter two are just $\subset \bar \land T$ and $\supset \bar \land T$,
and $\perp = T \bar \land T$.

\vskip 0.1in \noindent {\bf Problem 12} ({\it multilinear representation}).\hfil\break
a) Recall that (22) has the truth table $1100\,1001\,0000\,1111$, and
the expansion using Boole's law is given by (23).  This can rather easily
be turned into a polynomial by leaving any term $t$ as $t$, and
$\bar t \mapsto t$.  Then $\land \mapsto \times$, but what about $\vee$?
Well, $x \vee y = x + y - x y$.  Furthermore, since each variable is either
0 or 1, $t^n = t$ for any $n$.  So this turns into a giant algebraic expansion,
which can be fed to something like Mathematica to get $1 - y - x z + 2 x y z - w +
wy + wx + wxz - 2 w x y z$.

\vskip 0.05in
\noindent b) Use the Boolean decomposition directly.  A boolean function of $n$ 
variables can be written 
$$f\left(x_1, \ldots, x_n\right) = \left(f_1 \left(x_1, \ldots, x_{n-1}\right)
\land {\bar x_n}\right) \vee \left(f_2 \left(x_1, \ldots, x_{n-1}\right)
\land x_n\right)
$$ 
where $f_1\left(x_1, \ldots, x_{n-1}\right) =
f\left(x_1, \ldots, x_{n-1}, 0\right)$ and $f_2\left(x_1, \ldots, x_{n-1}\right) =
f\left(x_1, \ldots, x_{n-1}, 1\right)$.  Converting to the integral form,
this can be expanded to form $f = \left(f_1 \left(1 - x_n\right)\right) \vee
\left(f_2 x_n\right) = f_1 + \left(f_2 - f_1\right) x_n$.  Now,
$f_1$ and $f_2$ are both drawn from the space of functions of $n-1$ variables.
If the maximum possible (in an absolute value sense) coefficient 
for such a function is $\alpha_{n-1}$, then we can use the previous relation
to compute the limit on $\alpha_n$.  There are two cases -- we are
considering a term that does {\it not} include $x_n$, which is the first
term ($f_1$), or we are considering a term that does include $x_n$ which is
the $\left(f_2 - f_1\right) x_n$ term.  In the first case, the maximum coefficient
hasn't changed.  In the second, the maximum absolute value of $\alpha_n$
is $2 \alpha_{n-1}$ (if they occur with the opposite sign in $f_1$ and $f_2$).
Combine this with the fact that the maximum coefficient for a function of $x_1$
only is 1 ($1 - x_1$ or $x_1$), and the maximum absolute value of any coefficient 
is $2^{n-1}$.  Note the above also implies that the maximum coefficient for
a term containing $m$ of the $x_i$ is $2^{m-1}$.

\vskip 0.05in
\noindent c) Rather than multiplying out all the terms as in a), consider
that each function is comprised of a product of $x_j$ and $1 - x_k$s --
e.g., $\left(1 - x_3\right) x_4 \left(1 - x_8\right)$.  Therefore, if each $x_i$
is between 0 and 1, then each term is between 0 and 1.  When we or
them together, the result is $a \vee b = a + b - a b$, which, for
$a, b$ between 0 and 1, has a minimum at $a = b = 0$ of 0 and a maximum
when $a = b = 1$ of 1.  So therefore we must have $0 \leq f \leq 1$.

\vskip 0.05in
\noindent d) Note that the final expression if we multiply everything out
must be linear in each variable (i.e., with all other variables fixed).  We
know from the previous item that the maxima must occur at some set
of 0, 1 assignments.  Since the functions are linear, and $g \geq f$
at those endpoints, it must also be greater everywhere in between.

\vskip 0.05in
\noindent e) Since $f$ is monotone, it must be possible to write
$\partial f / \partial x_i = h\left(x\right) - g\left(x\right)$ for some
$h$ and $g$ such that $h \geq g$ for all $x$, and where we use d)
to convert this from a function defined only on 0, 1 to one defined on
real arguments.

\vskip 0.1in \noindent {\bf Problem 13} This is just the multilinear representation
from the previous problem, since the expectation value of any product
of $x_1 \cdot x_2 \cdot \ldots \cdot x_n$ is just $p_1 \cdot p_2 \cdot \ldots \cdot p_n$.

\vskip 0.1in \noindent {\bf Problem 18} Try setting $u_i = 1$ for all $i$ and
$v_j = 0$ for all $j$.  Then $f$ would be true in the DNF, but $f$ would
be false from the CNF.

\vskip 0.1in \noindent {\bf Problem 19} For the CNF we are looking for 
the points where (22) is {\it false}, not true, since each clause represents
such a string.  The strings $wxyz$ where (22) is false are
0010, 0011, 0101, 0110, 1000, 1001, 1010, 1011.  The maximal
subcubes are therefore 10**, 0*10, *01*, and 0101.  Each one is a barred
literal, each 0 an unbarred one (since we are looking for falsehood, so this
corresponds to $\left(\bar w \vee x\right) \land \left(w \vee \bar y \vee z\right)
\land \left(x \vee \bar y\right) \land \left(w \vee \bar x \vee y \vee \bar z\right)$.

\vskip 0.1in \noindent {\bf Problem 38} No, it's easy to test satisfiability
for a function in disjunctive normal form.  In some sense it's the opposite
of SAT -- if there are any implicants at all, one can just read off the values
that make it true, and the function is satisfiable.  The hard part, by symmetry,
is figuring out if the function is ever {\it false}.

\vskip 0.1in \noindent {\bf Problem 41} State the pigeonhole principle in
conjunctive normal form.  Well, the first task is to make clauses saying
that each of the $m$ pigeons is in at least one hole.  If we let the
variable $x_{jk}$ mean that pigeon $j$ is in hole $k$, then for each
$1 \leq j \leq m$ we must have $\left(x_{j1} \vee \cdots \vee x_{jn}\right)$.
Next we have to add clauses saying two pigeons can't be in the {\it same}
hole -- to wit $\left(\bar x_{ik} \vee \bar x_{jk}\right)$ for $i$ and $j$ ranging
over the distinct $m$ pigeons and $k$ the $n$ holes.  Actually, that's slightly redundant;
we can cut out about half the clauses by having $1 \leq i \le j \leq m$ and
$1 \leq k \leq n$ for these terms.

\vskip 0.1in \noindent {\bf Problem 48} The problem is really asking if we
can convert indefinite Horn clauses into definite ones -- which means can
we convert a clause with no unbarred literals (indefinite) into one or more with
exactly one (definite), since clearly we don't need to do anything to the
ones that already are definite.  So, we have a clause like $\bar x_1 \vee
\ldots \vee \bar x_n$.  As seems to be the standard approach here, make
some new variable $y$.  Now or it into each clause: $\bar x_1 \vee \ldots
\bar x_n \vee y$.  Now solve that problem using, e.g., {\bf Algorithm C},
noting that our augmented function must be satisfiable since it is definite.
Now, if $y$ shows up in the core, then the original formula was unsatisfiable.
In practice, we would stop as soon as {\bf C5} was about to set $y$ to true.

\vskip 0.1in \noindent {\bf Problem 56} The formula is satisfied by 010, 011, 111.
So therefore $\exists \exists \exists, \exists \exists \forall, \forall \exists \exists$
are satisfied.

\topglue 0.5in
\centerline{\tt Knuth 7.1.2}
\vskip 0.3in

\noindent {\bf Problem 1} As it happens, problem 7 provides exactly such
a formula: $\left(x_1 \bar \vee x_4\right) \vee \bar x_2 \equiv 
x_1 \bar \vee x_3$.

\vskip 0.1in \noindent {\bf Problem 6} If $f$ is normal, and $y = 0$ then we
have $f\left(x_1, \ldots, x_n\right) \land y = 0$ on the left and
$f\left(x_1, \ldots, x_n \land 0\right) = f\left(x_1, \ldots, 0\right) = 0$
on the right, so they are equal.  If $y=1$, then these are 
$f\left(x_1, \ldots, x_n\right)$ and $f\left(x_1, \ldots, x_n\right)$, which
are obviously equal.  Now, assume the equality holds and show
$f$ must be normal.  First, if $y=0$ then the left is 0 and the right is
$f\left(x_1, \ldots, 0\right)$, showing normality. 

\vskip 0.1in \noindent {\bf Problem 7} One approach (and maybe there
are others -- I don't know) is to try computing 
the negated truth table whenever non-normal operators are used, 
until the last step.  So, instead of $x_1 \bar \vee x_4$,
compute $x_1 \vee x_4$, instead of $\bar x_2 \vee x_5$ compute
$x_2 \land \bar x_5 = x_2 \land \hat x_5$, etc.  Putting it together:
$\hat x_5 = x_1 \vee x_4$, $\hat x_6 = x_2 \land \hat x_5$,
$\hat x_7 = x_1 \vee x_3$, $\hat x_8 = \hat x_6 \oplus \hat x_7$.

\vskip 0.1in \noindent {\bf Problem 8} Explain equation (11).\hfil\break
Equation (11) is 
$$
 x_k = \left(2^{2^n} - 1\right) / \left(2^{2^{n-k}} + 1\right).
$$
First, try some examples.  Say $n = 3$ and $k=1$, then this is
$255 / 17$, or, with the appropriate padding, $0\,0\,0\,0\,1\,1\,1\,1$.
For $k=2$ we get $255 / 5$ or $0\,0\,1\,1\,0\,0\,1\,1$.

Now try to think about this generally; the desired truth table is $2^{n-k}$
0s alternating with the same number of 1s.  Multiply that by the denominator
of (11), which is $1$, $101$, $1001$, etc., to get 
$x_k + \left(x_k \ll 2^{n-k}\right)$, which is simply a block of 1s of the
appropriate length, which the numerator generates.  Note that we don't have
to worry about things overflowing the gaps.

\vskip 0.1in \noindent {\bf Problem 10} Modify Algorithm L to compute $D$
instead of $L$.\hfil\break
All we have to do is change the looping ranges so that we are computing up
to the specified {\it depth} rather than length.  In particular, step 
{\bf L3} is limiting our length, instead we want depth, so we just change
the limit to $r - 1$ from $r - j - 1$.

\vskip 0.1in \noindent {\bf Problem 12} $x_4 = x_1 \oplus x_2$,
$x_5 = x_3 \land x_4$, $x_6 = x_2 \land \bar x_4$, $x_7 = x_5 \vee x_6$.

\vskip 0.1in \noindent {\bf Problem 17} One can do this by taking
advantage of CNF.  Actually, that suffices to do it in $n+2$ registers;
one calculates each miniterm in $x_{n+1}$ and stores the running
conjunction in $x_0$.  To get that down to $n+1$, take advantage of
7.1.1.16: $f\left(x_1, \ldots, x_n\right) = g\left(x_1, \ldots, x_{n-1}\right)
\oplus h\left(x_1, \ldots x_{n-1}\right) \land x_n$ by, say, accumulating
each miniterm of $h$ in $x_0$, anding that into $x_n$, then doing
something similar with $g$ applying $\oplus$ into $x_n$ on each step.

\vskip 0.1in \noindent {\bf Problem 51} When $x_5 x_6 = 00$
the numbers are never prime (since, after all, they are a multiple
of 4), nor whenever $x_6$ is 0 (multiple of 2).  Anyways:
$$
\eqalign{
 f\left(x_1, \ldots, x_6 \right) = \enspace
   & \left( \left[ x_5 x_6 \in \{ 01 \} \right] \land \left[ x_1 x_2 x_3 x_4 \in 
   \{0001, 0011, 0100, 0111, 1001, 1010, 1101, 1111 \} \right] \right) \cr
   \vee & \left(\left[x_5 x_6 \in \{10, 11\} \right] \land \left[ x_1 x_2 x_3 x_4 \in \{0000\}
      \right] \right)\cr
   \vee & \left(\left[x_5 x_6 \in \{11\} \right] \land \left[x_1 x_2 x_3 x_4 \in
     \{0001, 0010, 0100, 0101, 0111, 1010, 1011, 1110\} \right] \right)\cr
}
$$

\vskip 0.1in \noindent {\bf Problem 56} Imagine a truth table with only
two don't cares.  It's either a constant (0 or 1), or else we can match it
to one of the variables so that it is either $x_i$ or $\bar x_i$ for some
$i$.

\vskip 0.1in \noindent {\bf Problem 57}
$$
\matrix{
 x_1                              & 1 & 1 & 1 & 1 & 1 & 1 \cr
 x_2                              & 0 & 0 & 1 & 1 & 1 & 1 \cr
 x_3                              & 1 & 1 & 0 & 0 & 1 & 1 \cr
 x_4                              & 0 & 1 & 0 & 1 & 0 & 1 \cr
 x_5 = x_1 \oplus x_2   & 1 & 1 & 0 & 0 & 0 & 0 \cr
 x_6 = x_3 \land \bar x_4 & 1 & 0 & 0 & 0 & 1 & 0 \cr
 x_7 = x_1 \oplus x_3   & 0 & 0 & 1 & 1 & 0 & 0 \cr
 x_8 = x_2 \land \bar x_6 & 0 & 0 & 1 & 1 & 0 & 1 \cr
 x_9 = x_3 \wedge x_4 & 1 & 1 & 0 & 1 & 1 & 1 \cr
 \bar e = x_10 = x_4 \wedge x_8 & 0 & 1 & 1 & 1 & 0 & 1 \cr
 g = x_11 = x_7 \oplus x_8 & 0 & 0 & 0 & 0 & 0 & 1 \cr
 x_12 = x_4 \oplus x_11 & 0 & 1 & 0 & 1 & 0 & 0 \cr
 \bar d = x_13 = x_10 \land x_12 &  0 & 1 & 0 & 1 & 0 & 0 \cr
 \bar a = x_14 = \bar x_3 \land x_13 & 0 & 0 & 0 & 1 & 0 & 0 \cr
 \bar b = x_15 = x_2 \land \bar x_13 & 0 & 0 & 1 & 0 & 1 & 1 \cr
 \bar c = x_16 = \bar x_2 \land x_6 & 1 & 0 & 0 & 0 & 0 & 0 \cr
 \bar f = x_17 = \bar x_5 \land x_9 & 0 & 0 & 0 & 1 & 1 & 1 \cr
}
$$
or, debarring,
$$
\matrix{
\left(x_4 x_3 x_2 x_1\right)_2 & 10 & 11 & 12 & 13 & 14 & 15 \cr
 a & 1 & 1 & 1 & 0 & 1 & 1 \cr
 b & 1 & 1 & 0 & 1 & 0 & 0 \cr
 c & 0 & 1 & 1 & 1 & 1 & 1 \cr
 d & 1 & 0 & 1 & 0 & 1 & 1 \cr
 e & 1 & 0 & 0 & 0 & 1 & 0 \cr
 f & 1 & 1 & 1 & 0 & 0 & 0 \cr
 g & 0 & 0 & 0 & 0 & 0 & 1 \cr
}
$$
These aren't really numbers as such, so I'll leave it to the true experts
to typeset them == but 11 is 7 and 13 is 1.

\topglue 0.5in
\centerline{\tt Knuth 7.1.3}
\vskip 0.3in

\noindent {\bf Problem 1} Start with the simpler $m=-1$ case.  Then this
simply swaps $x$ and $y$.  More generally, wherever $m$ has a 1 set,
then this swaps that bit of $x$ and $y$, while wherever $m$ is 0 this
becomes $x \gets x \, \oplus \, y \, \oplus \, y = x \, \oplus \, x = x$,
and $y \gets y$.  So this swaps the bits of $x$ and $y$ where $m$ is
set without using another register.

\vskip 0.08in \noindent {\bf Problem 2} They don't
work in general for negative numbers.  As specific
examples: for (i) $x = -2, y =-4$ gives $x \oplus y = 2$
and $x\, |\, y = -2$; for (ii) $x=-2, y =4$ gives $x \, \& \, y = 4$
and $x\, |\, y = -2$; for (iii) the same values give $| x - y | = 6$
and $x \oplus y = -2$.  They are true for positive integers though.

\vskip 0.08in \noindent {\bf Problem 4} Recall (from 16) that
$-x = \bar x + 1$.  So $\bar x = -x - 1$, and hence $x^{CN} = x + 1 = x^S$.
By (17), we have $x^{NC} = \overline{-x} = x - 1 = x^P$.

\vskip 0.08in \noindent {\bf Problem 8} First, denote the
target set $Z = \left(S \, \oplus \, y\right) \, \cup \, \left(x \, \oplus \, T\right)$.
We first need to show that $x \oplus y \notin Z$.  Assume
that $x \oplus y = s \oplus y$ for some $s \in S$.  This implies that $s = x$
(apply $\oplus y$ to both sides), and $x \notin S$.  The same argument
applies to the other subset, so $x \oplus y \notin Z$.

Now, consider the case when $x = y$, so that $x \, \oplus \, y = 0$.  This is clearly 
${\rm mex}\left(Z\right)$,
since it is in both components of the union and is clearly the smallest element.

Since we already excluded the $x = y$ case,
we know that $x \, \oplus \, y$ must have at least one 1 in it, so
we can write $x \, \oplus \, y = \alpha 1 \beta$.  Therefore, there must be
some integer $k < x \, \oplus \, y$ which we can write as
$k = \alpha 0 \gamma$, where $| \gamma | = | \beta |$.  Furthermore,
we can write $x = \delta 1 \zeta$, $y = \eta 0 \theta$ (or vice versa),
where the cardinalities match ($|\delta| = | \eta | = | \alpha |$
and  $|\zeta| = |\theta| = |\beta|$).  Now, this implies that $\delta \, \oplus \, \eta = \alpha$
and $\zeta \oplus \theta = \beta$, and $k \, \oplus \, y = \left(\alpha \, \oplus \, \eta\right)
0 \left(\gamma \, \oplus \, \theta\right) = \left(\delta \, \oplus \, \eta \, \oplus \, \eta\right)
0 \left(\gamma \, \oplus \, \theta\right)$.  This must be less than $x$ as well by
comparison -- it agrees until it has a 0 and $x$ has a 1.  

Since, by definition,
$x$ is the smallest non-negative integer not in S, we must have $k \, \oplus \, y \in S$,
and so $k  = \left(k \, \oplus \, y\right) \, \oplus \, y \in S \, \oplus \, y$.
Since the above definition of $k$ was general, this means that any $k < x \, \oplus \, y$
must be in $S \oplus y$.  Swapping the 0/1 in $x$ and $y$ would give the same
thing with $x \oplus T$, and therefore we conclude that $x \, \oplus \, y$
must be the smallest thing not in their union, which is the desired result.

\vskip 0.08in {\bf Problem 9} ({\it Nim}) \hfil\break
There is more than one way to analyze this.  I find direct
analysis to be much easier to understand than the Sprague-Grundy
theorem approach from the previous problem, so lets start with that.
Let $s$ be the nim sum of the state before the player moves
and $t$ the state after.  $t$ can differ in the contents of at most
one stack.  So we can write 
$s = a_0 \oplus \ldots \oplus a_j \oplus \ldots \oplus a_k$ and 
$t = a_0 \oplus \ldots a^{\prime}_j \oplus \ldots \oplus a_k$ 
where $a^{\prime}_j < a_j$.  How are $s$ and $t$ related?

$t = s \oplus s \oplus t$, but $s \oplus t = a_j \oplus a^{prime}_j$
(since all the other terms are equal), so
$t = s \oplus a_j \oplus a^{prime}_j$.  Since $a_j \ne a^{\prime}_j$,
$a_j \oplus a^{\prime}_j \ne 0$.

This leads to an important conclusion: if $s = 0$, then $t \ne 0$.
Since the winning move will have $t = 0$, this is interesting -- 
clearly if $s=0$ we can't force a win with the current move, and it's
possible that our opponent might be able to.  

But what about the other way -- if $s \ne 0$ can we force $t = 0$ with our 
move?  The answer is yes.  Since $s \ne 0$ it must have a leading 1
somewhere in it's binary representation.  We chose the leftmost one in
position $d$.Moreover, we must be able to
find at least one pile $a_m$ that also has a 1 at $d$ (or $s$
would be 0 there).  Now form $a^{prime}_m = s \oplus a_m$.  We must
have $a^{\prime} < a_m$ -- all bits to the left of $d$ must
be the same (since $s$ is 0 to the left of $d$ by definition).
But $a^{\prime}_m \left(d \right) = 0$ and $a_m\left(d\right) = 1$.
So, the player takes $a_m - a^{\prime}_m$ sticks from the $m$th pile,
and we have $t = s \oplus a_m \oplus a^{\prime}_m = s \oplus a_m 
\oplus s \oplus a_m = 0$.

So, if $s \ne 0$, you can force $t = 0$ with your move.  If that 
empties all the stacks, you win.  If not, the other player is forced
to make a move that changes $t$ back to 1.  Since that can't be a win,
and the piles are always getting smaller (and assuming finiteness),
you can force a win.

How does this relate to the Sprague-Grundy theorem?  It is, in some
sense, a generalization of the above that can be applied to other games.
That is, this is an example of Knuth killing a fly with a jackhammer.
The fundamental idea is to show how to move from a single pile game
to a multiple pile game. So, consider a single pile game of Nim with
$x$ sticks.  Let $S$ be the set of all pile states that can be reached
with one move -- so, if $x \ne 0$, $S = \{0, 1, \ldots, x-1\}$.
Now ${\rm mex}\left(S\right) = x$, and so ${\rm mex}\left(S\right) = 0$
corresponds to $S = \emptyset$ -- in which case the player loses.

Now consider two piles of size $x$ and $y$ with corresponding state
sets $S$ and $T$.  The states that can be reached by the player are 
the union of one element of $S$ and any element of $T$ and 
any elements of $S$ and one element of $T$ if the player takes from
the first or second pile, respectively.  This maps onto the structure
of the Sprague-Gundy theorem via $\oplus$, although it isn't exactly
obvious why that is interesting.  The point of the theorem is essentially
to establish a connection between mex and $\oplus$.

To establish this, an immediate consequence of Sprague-Grundy is that
there exists a $0 \le k < x \oplus y$ if and only if there exists
either a $i < x$ such that $i \oplus y < x \oplus y$ or
a $j < y$ such that $x \oplus j < x \oplus y$ pretty much entirely from
the definition of mex.  Now, if there is no such $k$, that implies the
player has no legal move from the position $x, y$, and so loses.
But if there is, then the players winning strategy is to reduce pile $x$ to
size $i$ or $y$ to $j$ (depending which one exists).  Thus, if the game state is $x \, \oplus \, y = 0$, it is not possible to reduce that
value by making any move (since the new game state will either be 
$i \oplus y$ or $x \oplus j$), and so they must increase it.  But if it
is, then they can reduce $x \oplus y$, and in fact by the construction
discussed earlier they can set it to zero.  This therefore becomes the
same argument as before, although in a rather indirect fashion.
The point is then that a game with two piles of size $x$ and $y$ is
the same as a game with one pile of size $x \oplus y$ -- if it's zero
you lose, if it isn't you can win.  And you can repeat this by stacking
on extra piles.

\vskip 0.08in \noindent {\bf Problem 20} ({\it Gosper's hack})\hfil\break
$x \, \& \, -x$ turns all other bits except the rightmost one off: for
$x = \alpha 0 1^a 1 0^b$, $u \gets x \, \& \, -x = 0^{\infty} 0^a 1 0^b$.
Then $v \gets x + u = \alpha 1 0^a 0 0^b$, replaces the rightmost
run of 1s by zeros but puts a 1 before them.  The last bit is complicated, so
piece by piece, $v \oplus x = 0^{\infty} 1 1^a 1 0$,
$v \oplus x / u = 0^{\infty} 1 1^a 1$ -- it's the rightmost run of 1s with a 1 appended,
then shifted all the way to the right.  Then two more shifts to the right removes
two of the ones: $\left(v \oplus x / u\right) \gg 2 = 0^{\infty} 1^a$.
Now, finally, $y \gets v + \left( \left(v \oplus x / u\right) \gg 2 \right)
= \alpha 1 0^{b} 0 1^a$.  So, what is this?  It's the next larger number than $x$
with the same number of 1s (that is $\nu x = \nu y$).

\vskip 0.08in \noindent {\bf Problem 21} Start with the case where 
there are trailing 0s (so $b > 0$ in 32).  Then what we want to
do is shift the rightmost 1 one to the right: $y = \alpha 0 1^a 1 0^b$,
$x = \alpha 0 1^1 0 1 0^{b-1}$, which means $x \gets y -  0^{\infty} 1 0^{b-1}$.
But that's easy enough: $\left(y \, \& \,y\right) \gg 1 = 0^{\infty} 1 0^{b-1}$,
or, putting it together: $x \gets y - \left( \left(y \,\& y\right)  \gg 1 \right)$.
Note that the $\gg 1$ can be replaced by $ / \,2$.

Now, if there are no trailing zeros, then $y = \alpha 0 1^a 1$, then
we start having to mess with $\alpha$.  Basically, we need to 
extract $\alpha$ and then figure out the right amount to subtract from it
to keep the number of 1s right.  The first bit can be accomplished by $u \gets y \, \&
\left(y + 1\right) = \alpha 0^{a+2}$.  But what's the
right amount to subtract?  Well, we're going to need the rightmost
1 bit in $\alpha$, which is $u \, \& \, -u$, then we need to shift it right by
$a + 2$.  So we need to construct $0^{\infty} 1 0^{a+2}$ and divide by it.
But this is $y \oplus \left(y + 1\right)$ -- so our recipe is
$u \gets y \, \& \left(y + 1\right)$, $v \gets \left(y \oplus \left(y + 1\right)\right) + 1$,
and $x \gets u - \left(u \, \& \, -u\right) / v$.

These can be combined by branching on whether or not $y$ is even, but
Knuth's gives a branch free version: $u \gets y \, \& \, \left(y + 1\right)$, 
$v \gets  y \, \oplus \, \left(y + 1\right)$,
$x \gets v - \left(u \, \& \, -u\right) / \left( v + 1 \right)$.

\vskip 0.08in\noindent {\bf Problem 23} ({\it Parenthesis Trace})\hfil\break
a) The smallest trace is $(( \ldots (()) \ldots )) $, which is $m$ 0s followed
by $m$ 1s, otherwise known as $2^m-1$.  The largest is $()()\ldots()$ which 
is $m$ $01$s, which is $\left(2^{2m}-1\right)/3$.

\noindent b) The general rule for finding the successor of any lexicographic 
pattern $a_1 \ldots a_n$ is to find the largest $j$ such that $a_j$ can be increased, increase
$a_j$ by the smallest possible amount, and then complete the pattern
in the lexicographically least way possible (\S7.2.1.2).  So -- how do we do that?

The key here is that $\mu_0 =  -1/3 = \ldots010101$ is the largest possible value.  
So we want to find the rightmost element that is out of position with respect to that, since
we can increase anything less.  All elements that differ from $\mu_0$
can be located by calculating $z \gets \mu_0 \, \oplus \, x$ and then selecting
that position by either $z \, \& \, -z$ or by extracting and smearing right with
$z \oplus \left(z - 1\right)$.

However, we can't just add 1 at that position (which selects 
the element just before the last out of place element) because there might be 1s 
before that.  For example, $\ldots))))$ would give $z = \ldots 0011$,
and if we just added one to that we would be ignoring the )s before
that point.  So, really, we want the run of 1s containing the rightmost out 
of place value.  Note, incidentally, that the rightmost
out of place element will always be a 1. If it were a zero, there
would have to be another 1 to its right to get a valid pattern.
Anyways, we can both increment by the least possible amount
{\it and} preserve the pattern before that by doing 
$u = z \oplus \left(z - 1\right)$, $v = x\, |\, u$, $w = v + 1$.
So, this is the increasing the pattern by the smallest amount.
As an example, for $x = ()(())() = 01001101$, $u = 00001111$,
$v = 01001111$, $w = 01010000$.

So now we need to complete the pattern in the least way possible --
which basically means 0s followed by 1s.  What we more or less want
is to subtract the number of 1s from half the number of bits, and then
make a pattern with that many 1s on the end and 0s elsewhere,
then add that to $w$.  We can do this by selecting all the trailing 0s
in $w$ using $w \& \left(w - 1\right) = v \& \bar w$ and then shifting
the right number off, \'a la Gosper's hack.  So the the question
becomes how many 0s we need in the bits rightward of the rightmost
1 bit in $w$?  Well, we need the total number of either 0s or 1s
to be equal to $n/2$ (which is equal to $\nu x$), so if we
were working with a $d$ bit word, we could just compute
$-1 \gg \left(d - \nu x + \nu w \right)$ and add that to $w$.
That's actually not a bad way to do it if you precompute and save
$\nu x - 1$, but there's something unsatisfying about depending on $d$.
Knuth gives a different, more mathematical solution that doesn't
have this problem that took me some effort to understand.

The key is to write out the form that $x, v, w$ must have if $u$ has $k$ 1s:
$$
\eqalign{
 x =& \alpha 0 1^a 1 1 \overbrace{01 \ldots 01}^{k-2}\cr
 u =& 0^{\infty} 0 0^{a} 1^k\cr
 v =& \alpha 0 1^a 1^k \cr
 w =& \alpha 1 0^a 0^k \cr
 v \& \bar{w} =& 0^{\infty} 0 1^a 1^k\cr
}
$$
where $a \ge 0$.  Note that we can't have $x = \alpha 0 1^a 1 0 \ldots$
because we would be opening a brace we never closed;
that is $k must be even$. 
Now, our goal is to fill out the last $a + k$ positions in $w$, but we have to
preserve the same number of 0s and 1s in the final $1 + a + k$
positions as we had in those positions in $x$.  Simply counting
in $x$ gives $k / 2 + 1 + a$.  $w$ already has 1, so we need
$k / 2 + a$.  $v \& \bar{w}$ has $a + k$, so if we can shift off $k/2$
($v \& \bar{w} \gg k / 2$) that will give us what we want.

How do we best compute $k/2$?  There are a number of methods.
First, note that if $u$ has $k$ 1s, then $u = 2^k - 1$, so
left shifting by $k/2$ is the same as computing $v \& \bar{w} / \sqrt{u + 1}$.
However, this is a rather expensive expression to compute.
An alternative is therefore to compute either $\nu \left(u \& \mu_0\right)$
or $\nu \left(u \& \bar{\mu_0}\right)$ -- either gives the same answer,
since $k$ is always even.

So, putting it together: $z = x \oplus \mu_0$, $u = z \oplus \left(z - 1 \right)$,
$v = x\, |\, u$, $w = v + 1$, $y = w + \left(v \, \& \, {\bar w}\right) / \sqrt{u + 1}$.
To actually implement this, you wouldn't want to do the division, but rather
use one of the other shift forms.

So how does this work if we are trying to work with a limited number
of bits?  For example, if we want to work with 52 bits in a 64 bit word?
One easy solution is to build a mask of the upper 12 bits and as soon
as that anded with $y$ is non-zero we are done.  But what happens
in the formula?  Well, this causes $x \oplus \mu_0$ to have a 1 bit
in an odd position, which means that $u + 1$ is no longer a power of 4,
and so the division by $\sqrt{u+1}$ has strange results.  The 
$\nu\!\left(u \& \bar{\mu_0}\right)$ version produces an invalid pattern
with a one bit beyond the desired positions and more ones than desired.

\vskip 0.04in \noindent c) This is a straightforward translation of the $\nu\left(u \&
\bar{\mu_0}\right)$ version:
{\bf XOR z,x,mu0}; {\bf SUBU u,z,1}; {\bf XOR u,z,u}; {\bf OR v,x,u};
{\bf ADDU w,v,1}; {\bf ANDN z,v,w}; {\bf SADD y,u,mu0};
{\bf SRU y,z,y}; {\bf ADDU y,w,y}.  As mentioned above, if one is
working with fewer than the full 64 bits, just create a mask with 0s
in the allowed positions and 1s elsewhere ($-1 \ll n$ for $n$ desired bits),
AND it with the pattern, and stop via branch if that isn't 0:
{\tt SLU mask,-1,n}; {\tt AND done,y,mask}, then branch somewhere else if {\tt done} is
non-zero.

\vskip 0.08in \noindent {\bf Problem 27} a) $x + x \oplus \left(x - 1\right) = 
\left(\alpha 1 0^a 0 1^b\right)_2$. b) $\left(x + x \oplus \left(x - 1\right)\right) \, | \, 
\left(x \oplus \left(x + 1\right)\right) = \left(\alpha 1 0^a 1 1^b\right)_2$.
c) This is getting cumbersome; so let $y = \left(x - 1\right)$, $z = x \, \oplus \, 
\left(x - 1\right) = x \, \oplus \, y$.  Then $y \, \& \, \left(x + z\right) =
\left(\alpha 0 0^a 0 1^b\right)_2$. d) $y \, \oplus \, \left(x + z \right) =
\left(0^{\infty} 1 1^a 0 0^b\right)_2$. e) $y \, \& \, \overline{x + z}$.
f) $x \, \oplus \left(x + z\right)$.

\vskip 0.08in \noindent {\bf Problem 28} What is $\bar x \, \& \left(x + 1\right)$?
I did this in the notes: it sets the rightmost 0 to 1 and turns off all other bits.

\vskip 0.08in \noindent {\bf Problem 30} A very simple solution is to add 
{\tt CSZ rho,x,64} to set it to 64 in that
case.  Knuth gives a more sophisticated solution.  For the table look-up
version, one simply has to put the right value (8, as it happens) 
in the 0 position in the table.

\vskip 0.08in \noindent {\bf Problem 31} Most importantly, this will loop
forever for $x=0$.  Also, the random model for $x$ is unlikely to
be a good representation of the numbers people actually want to
compute $\rho$ for.

\vskip 0.08in \noindent {\bf Problem 38} The MMIX instructions are:
{\tt SRU t,x,1}; {\tt OR y,x,t}; {\tt SRU t,y,2}; {\tt OR, y,y,t}; {\tt SRU t,y,4};
{\tt OR, y,y,t}; {\tt SRU t,y,8}; {\tt OR y,y,t}; \dots, {\tt SRU t,y,32};
{\tt OR y,y,t}; {\tt SRU t,y,1}; {\tt SUBU y,y,t}, which takes 14$\upsilon$.

However, there's a potentially faster solution not given by Knuth, which
is to reverse using (66), extract using (37), and then reverse again.
To wit: {\tt rev GREG \#0102040810204080}, {\tt MOR y,x,rev},
{\tt MOR y,rev,y}, {\tt NEGU t,y}, {\tt AND t,y,t}, {\tt MOR y,t,rev},
{\tt MOR y,rev,y}.  This takes 6$\upsilon$, plus an additional
$\upsilon + \mu$ if {\tt rev} has to be loaded from memory.

\vskip 0.08in \noindent {\bf Problem 39} Knuth gives a non-MMIX
general solution, but on MMIX one can just reverse as in the
previous problem, apply (57), and reverse again.  This is again
potentially faster, depending on whether one needs to load {\tt rev}.

\vskip 0.08in \noindent {\bf Problem 40} First, suppose that 
$\lambda x = \lambda y = k$ for $x$ and $y$ not equal to zero.
Then $x = \left(1 \alpha \right)_2$
and $y = \left(1 \beta \right)_2$ for some $| \alpha | = | \beta | = x$.
Well, $x \oplus y < 2^k < x \& y$.  Now suppose that $x < y$.
Then we have $x \oplus y \geq 2^k \ge x \& y$.  We can just swap
the arguments to prove the same thing for $x > y$.
That only leaves $x = y = 0$, in which case $x \oplus y = x \& y$.
Now, as for the quick test, we can only have $\lambda x < \lambda y$
if $x < \bar{x} \& y$, since that will always have a 1 somewhere
left of $\lambda x$.

\vskip 0.08in \noindent {\bf Problem 43} Start by implementing (63): 
{\bf SET  nu,0}; {\bf SET  y,0}; {\bf BZ  y,Done}; {\bf 1B ADDU  nu,nu,1};
{\bf SUBU  t,y,1}; {\bf AND  y,y,t}; {\bf PBNZ  y,1B} followed by {\bf Done}.
The loop will be executed $\nu   x$ times, so the total cost is 
$\left(5 + 4 \nu   x \right) \upsilon$.

Next, implement (62): {\bf SRU  t,x,1}; {\bf AND  t,t,mu0}, {\bf SUBU  y,x,t};
{\bf SRU  t,y,2}; {\bf AND  t,t,mu1}; {\bf AND  y,y,mu1}; {\bf ADDU  y,y,t};
{\bf SRU  t,y,4}; {\bf ADDU  y,y,t}; {\bf AND  y,y,mu2}; {\bf MULU  y,a,y};
{\bf SRU  nu,y,56}.  Assuming we don't have to load any of the constants,
this takes $21\upsilon$.  So (62) ties for $\nu   x = 4$, and wins for all
smaller values.

Knuth points out an improved solution that replaces that last two steps of (62)
with {\bf SRU  t,y,8}; {\bf ADDU  y,y,t}; {\bf SRU  t,y,16}; {\bf ADDU  y,y,t};
{\bf SRU  t,y,32}; {\bf ADDU  y,y,t}; {\bf AND y,y,\#ff} which reduces the cost
to $18\upsilon$, which only eliminates the tie case.

\vskip 0.08in \noindent {\bf Problem 47} Yes, and rather straightforwardly:
$y \gets \left(x \gg \delta\right) \& \,\theta$, $z \gets \left(x\, \& \,\theta\right) \ll \delta$,
$x \gets \left(x\, \& \, m\right)\,|\,y\,|\,z$, where ${\bar m} = \theta \, | \,
\left(\theta \ll \delta\right)$.

\vskip 0.08in \noindent {\bf Problem 79} Consider $x - 1$. 
If $x$ ends in a 1, this will just turn that bit off, which is still
a subset of $\chi$.  If $x$ doesn't end in a 1, this will
simply propagate up $x$ flipping 0s to 1s until it encounters it's first 1, which
it will flip to 0.  The resulting number probably won't be a subset of $\chi$,
so we just have to mask off the parts we don't want: $x_{\prime} = 
\left(x - 1\right) \, \& \, \chi$.

\vskip 0.08in \noindent {\bf Problem 80} We want to just take turn flipping off
bits of $\chi$.  So we want to identify the rightmost bit in $\chi$ we haven't
yet already tried flipping off.  We'll represent all the unvisited bits of $\chi$
as $x$.  Thus: start with $x \gets \chi$ (none visited), $t \gets x \, \& \, -x$,
(identify rightmost bit) visit $\chi - t$, then $x \gets x - t$.  Stop when $x = 0$.

\vskip 0.08in \noindent {\bf Problem 93} $x - y = \left(x \oplus y\right) -
\left(\left(\bar{x} \, \& \, y\right) \gg 1\right)$; where they are both 1 shut
off the resulting bit.  Where $x$ is 1 and $y$ is 0, leave it alone,
but where $x$ is 0 and $y$ is 1, subtract that from the bit one more to the right.

\vskip 0.08in \noindent {\bf Problem 143} ({\it Knights moves})\hfil\break
This is actually simpler than it sounds.  Consider moving two steps up
and one left.  That's the same as left shifting by 17 positions.
However, we have to exclude all pieces that are in the left most column,
or the top two rows.  We don't have to worry about the top two rows --
anything there just gets shifted off.  So therefore, the reachable positions
for this particular type of move are $\left(x \,\&\,k_1\right) \ll 17$,
where $k_1 = {\bf \#7f7f7f7f7f7f7f7f}$.  Similarly, down 2 and to the right is
$\left(x \ll 17\right) \, \& \, k_1$.  Up one and left 2 is a 10 shift:
$\left(x \ll 10\right) \, \& \, k_2$ where $k_2 = {\bf \#3f3f3f3f3f3f3f3f}$,
and the same mirror.  Up one and left right is a 6 shift:
$\left(x \ll 6\right) \, \& \, k_3$ where $k_3 = {\bf \#fcfcfcfcfcfcfcfc}$,
and finally up two and right 1 is a 15 shift with a constant $k_4 = {\bf \#fefefefefefefefe}$.
Putting it all together is a bit tedious: $f\left(x\right) = \left(\left(x\&k_1\right) \ll 17\right) |
 \left(\left(x \gg 17\right)\&k_1\right) |\allowbreak \left(\left(x\&k_2\right)\ll 10\right) | \allowbreak
\left(\left(x\gg10\right)\&k_2\right) | \left(\left(x\&k_3\right)\ll6\right) |
\left(\left(x\gg6\right) \& k_3\right) | \left(\left(x\&k_4\right)\ll15\right) |
\left(\left(x\gg15\right)\&k_4\right)$.
\end